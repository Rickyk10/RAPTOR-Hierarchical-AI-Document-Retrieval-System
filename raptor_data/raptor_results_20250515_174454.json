{
    "1": {
        "clusters": [
            {
                "text": "\n\nThe Ultimate Guide to Fine-Tuning LLMs from\nBasics to Breakthroughs: An Exhaustive Review of\nTechnologies, Research, Best Practices, Applied\nResearch Challenges and Opportunities\n(Version 1.1)\n\n\fChapter 1\n\nIntroduction\n1.1\n\nBackground of Large Language Models (LLMs)\n\nLarge Language Models (LLMs) represent a significant leap in computational systems capable of understanding and generating human language. Building on traditional language models (LMs) like N-gram\nmodels [1], LLMs address limitations such as rare word handling, overfitting, and capturing complex\nlinguistic patterns. Notable examples, such as GPT-3 and GPT-4 [2], leverage the self-attention mechanism within Transformer architectures to efficiently manage sequential data and understand long-range\ndependencies. Key advancements include in-context learning for generating coherent text from prompts\nand Reinforcement Learning from Human Feedback (RLHF) [3] for refining models using human responses. Techniques like prompt engineering, question-answering, and conversational interactions have\nsignificantly advanced the field of natural language processing (NLP) [4].\n\n1.2\n\nHistorical Development and Key Milestones\n\nLanguage models are fundamental to natural language processing (NLP), leveraging mathematical techniques to generalise linguistic rules and knowledge for tasks involving prediction and generation. Over\nseveral decades, language modelling has evolved from early statistical language models (SLMs) to today\u2019s advanced large language models (LLMs). This rapid advancement has enabled LLMs to process,\ncomprehend, and generate text at a level comparable to human capabilities [5, 6].\nFigure 1.1 shows the evolution of large language models from early statistical approaches to current\nadvanced models.\n\n1.3\n\nEvolution from Traditional NLP Models to State-of-the-Art\nLLMs\n\nUnderstanding LLMs requires tracing the development of language models through stages such as Statistical Language Models (SLMs), Neural Language Models (NLMs), Pre-trained Language Models (PLMs),\nand LLMs.\n\n1.3.1\n\nStatistical Language Models (SLMs)\n\nEmerging in the 1990s, SLMs analyse natural language using probabilistic methods to determine the\nlikelihood of sentences within texts. For instance, the probability P (S) of the sentence \u201cI am very\nhappy\u201d is given by:\nP (S) = P (\u03c91 , \u03c92 , \u03c93 , \u03c94 ) = P (I, am, very, happy)\n\n(1.1)\n\nThis probability can be calculated using conditional probabilities:\nP (I, am, very, happy) = P (I) \u00b7 P (am | I) \u00b7 P (very | I, am) \u00b7 P (happy | I, am, very)\nConditional probabilities are estimated using Maximum Likelihood Estimation (MLE):\n\n6\n\n(1.2)\n\n\fFigure 1.1: A chronological timeline showcasing the evolution of Large Language Models (LLMs) from\n1990 to 2023. This progression begins with early statistical models such as N-grams, transitions through\nneural language models like Word2Vec and RNN/LSTM, and advances into the era of pre-trained models with the introduction of transformers and attention mechanisms. The figure highlights significant\nmilestones, including the development of BERT, GPT series, and recent innovations such as GPT-4 and\nChatGPT, demonstrating the rapid advancements in LLM technology over time. (adapted from [6])\n\nP (\u03c9i | \u03c91 \u03c92 \u00b7 \u00b7 \u00b7 \u03c9i\u22121 ) =\n\n1.3.2\n\nC(\u03c91 \u03c92 \u00b7 \u00b7 \u00b7 \u03c9i )\nC(\u03c91 \u03c92 \u00b7 \u00b7 \u00b7 \u03c9i\u22121 )\n\n(1.3)\n\nNeural Language Models (NLMs)\n\nNLMs leverage neural networks to predict word sequences, overcoming SLM limitations. Word vectors\nenable computers to understand word meanings. Tools like Word2Vec [7] represent words in a vector\nspace where semantic relationships are reflected in vector angles. NLMs consist of interconnected neurons\norganised into layers, resembling the human brain\u2019s structure. The input layer concatenates word vectors,\nthe hidden layer applies a non-linear activation function, and the output layer predicts subsequent words\nusing the Softmax function to transform values into a probability distribution.\nFigure 1.2 illustrates the structure of Neural Language Models, highlighting the layers and connections\nused to predict subsequent words.\n\n1.3.3\n\nPre-trained Language Models (PLMs)\n\nPLMs are initially trained on extensive volumes of unlabelled text to understand fundamental language\nstructures (pre-training). They are then fine-tuned on a smaller, task-specific dataset. This \u201dpre-training\nand fine-tuning\u201d paradigm, exemplified by GPT-2 [8] and BERT [9], has led to diverse and effective model\narchitectures.\n\n1.3.4\n\nLarge Language Models (LLMs)\n\nLLMs like GPT-3, GPT-4, PaLM [10], and LLaMA [11] are trained on massive text corpora with tens of\nbillions of parameters. LLMs undergo a two-stage process: initial pre-training on a vast corpus followed\n\n7\n\n\fFigure 1.2: A schematic representation of Neural Language Models, showcasing the layered architecture\nwhere the input layer processes sequential data, the hidden layer captures dependencies, and the output\nlayer generates predictions. The figure emphasises the flow of information through concatenation and\nmatrix multiplications, culminating in a probability distribution via the softmax function. (adopted from\n[6])\nby alignment with human values. This approach enables LLMs to understand human commands and\nvalues better.\n\n1.4\n\nOverview of Current Leading LLMs\n\nLLMs are powerful tools in NLP, capable of performing tasks such as translation, summarisation, and\nconversational interaction. Advances in transformer architectures, computational power, and extensive\ndatasets have driven their success. These models approximate human-level performance, making them\ninvaluable for research and practical implementations. LLMs\u2019 rapid development has spurred research\ninto architectural innovations, training strategies, extending context lengths, fine-tuning techniques, and\nintegrating multi-modal data. Their applications extend beyond NLP, aiding in human-robot interactions\nand creating intuitive AI systems. This highlights the importance of comprehensive reviews consolidating\nthe latest developments [12].\nFigure 1.3 provides an overview of current leading LLMs, highlighting their capabilities and applications.\n\n1.5\n\nWhat is Fine-Tuning?\n\nFine-tuning uses a pre-trained model, such as OpenAI\u2019s GPT series, as a foundation. The process\ninvolves further training on a smaller, domain-specific dataset. This approach builds upon the model\u2019s\npre-existing knowledge, enhancing performance on specific tasks with reduced data and computational\nrequirements.\nFine-tuning transfers the pre-trained model\u2019s learned patterns and features to new tasks, improving\nperformance and reducing training data needs. It has become popular in NLP for tasks like text classification, sentiment analysis, and question-answering.\n\n8\n\n\fFigure 1.3: Mind map depicting various dimensions of Large Language Models (LLMs), covering aspects\nfrom pre-training and fine-tuning methodologies to efficiency, evaluation, inference, and application domains. Each dimension is linked to specific techniques, challenges, and examples of models that exemplify\nthe discussed characteristics. This diagram serves as an overview of the multifaceted considerations in\nthe development and deployment of LLMs. (adapted from [13])\n\n1.6\n\nTypes of LLM Fine-Tuning\n\n1.6.1\n\nUnsupervised Fine-Tuning\n\nThis method does not require labelled data. Instead, the LLM is exposed to a large corpus of unlabelled text from the target domain, refining its understanding of language. This approach is useful for\nnew domains like legal or medical fields but is less precise for specific tasks such as classification or\nsummarisation.\n\n1.6.2\n\nSupervised Fine-Tuning (SFT)\n\nSFT involves providing the LLM with labelled data tailored to the target task. For example, fine-tuning\nan LLM for text classification in a business context uses a dataset of text snippets with class labels.\nWhile effective, this method requires substantial labelled data, which can be costly and time-consuming\nto obtain.\n\n9\n\n\f1.6.3\n\nInstruction Fine-Tuning via Prompt Engineering\n\nThis method relies on providing the LLM with natural language instructions, useful for creating specialised assistants. It reduces the need for vast amounts of labelled data but depends heavily on the\nquality of the prompts.\n\n1.7\n\nPre-training vs Fine-tuning\n\nTable 1.1 provides a comparison between pre-training and fine-tuning, highlighting their respective characteristics and processes.\nAspect\nDefinition\nData Requirement\nObjective\nProcess\n\nModel Modification\nComputational Cost\nTraining Duration\nPurpose\nExamples\n\nPre-training\nTraining on a vast amount of\nunlabelled text data\nExtensive and diverse unlabelled text data\nBuild general linguistic knowledge\nData collection, training on\nlarge dataset, predict next\nword/sequence\nEntire model trained\nHigh (large dataset, complex\nmodel)\nWeeks to months\nGeneral language understanding\nGPT, LLaMA 3\n\nFine-tuning\nAdapting a pre-trained model to\nspecific tasks\nSmaller, task-specific labelled\ndata\nSpecialise model for specific\ntasks\nTask-specific data collection,\nmodify last layer for task, train\non new dataset, generate output\nbased on tasks\nLast layers adapted for new task\nLower (smaller dataset, finetuning layers)\nDays to weeks\nTask-specific performance improvement\nFine-tuning LLaMA 3 for summarisation\n\nTable 1.1: A Comparative Overview of Pre-training and Fine-tuning in Large Language Models (LLMs).\nThe table outlines key differences between the pre-training and fine-tuning phases across various aspects\nsuch as definition, data requirements, objectives, processes, model modification, computational costs,\ntraining duration, and their respective purposes, with examples highlighting specific models and tasks.\nPre-training involves extensive training on vast amounts of unlabelled data to build general linguistic\nknowledge, while fine-tuning adapts the pre-trained models to specialised tasks using smaller, labelled\ndatasets, focusing on task-specific performance improvements.\n\n1.8\n\nImportance of Fine-Tuning LLMs\n\n1. Transfer Learning: Fine-tuning leverages the knowledge acquired during pre-training, adapting\nit to specific tasks with reduced computation time and resources.\n2. Reduced Data Requirements: Fine-tuning requires less labelled data, focusing on tailoring\npre-trained features to the target task.\n3. Improved Generalisation: Fine-tuning enhances the model\u2019s ability to generalise to specific\ntasks or domains, capturing general language features and customising them.\n4. Efficient Model Deployment: Fine-tuned models are more efficient for real-world applications,\nbeing computationally efficient and well-suited for specific tasks.\n5. Adaptability to Various Tasks: Fine-tuned LLMs can adapt to a broad range of tasks, performing well across various applications without task-specific architectures.\n6. Domain-Specific Performance: Fine-tuning allows models to excel in domain-specific tasks by\nadjusting to the nuances and vocabulary of the target domain.\n\n10\n\n\f7. Faster Convergence: Fine-tuning usually achieves faster convergence, starting with weights that\nalready capture general language features.\n\n1.9\n\nRetrieval Augmented Generation (RAG)\n\nA popular method to utilise your own data is by incorporating it into the prompt when querying the LLM\nmodel. This approach, known as Retrieval-Augmented Generation (RAG), involves retrieving relevant\ndata and using it as additional context for the LLM. Instead of depending solely on knowledge from the\ntraining data, a RAG workflow pulls pertinent information, connecting static LLMs with real-time data\nretrieval. With RAG architecture, organisations can deploy any LLM model and enhance it to return\nrelevant results by providing a small amount of their own data (see Figure1.4 for visual workflow). This\nprocess avoids the costs and time associated with fine-tuning or pre-training the model.\n\nFigure 1.4: An illustration of the Traditional Retrieval-Augmented Generation (RAG) pipeline steps,\ndepicting the sequential process from client query to response generation. The pipeline starts with\nthe client\u2019s question, followed by semantic search in a vector database, contextually enriching the data\nbefore generating a prompt for the large language model (LLM). The final response is post-processed\nand returned to the client.\n\n1.9.1\n\nTraditional RAG Pipeline and Steps\n\n1. Data Indexing: Organise data efficiently for quick retrieval. This involves processing, chunking,\nand storing data in a vector database using indexing strategies like search indexing, vector indexing,\nand hybrid indexing.\n2. Input Query Processing: Refine user queries to improve compatibility with indexed data. This\ncan include simplification or vector transformation of queries for enhanced search efficiency.\n3. Searching and Ranking: Retrieve and rank data based on relevance using search algorithms\nsuch as TF-IDF, BM25, and deep learning models like BERT to interpret the query\u2019s intent and\ncontext.\n4. Prompt Augmentation: Incorporate relevant information from the search results into the original query to provide the LLM with additional context, enhancing response accuracy and relevance.\n11\n\n\f5. Response Generation: Use the augmented prompt to generate responses that combine the LLM\u2019s\nknowledge with current, specific data, ensuring high-quality, contextually grounded answers.\n\n1.9.2\n\nBenefits of Using RAG\n\n\u2022 Up-to-Date and Accurate Responses: Enhances the LLM\u2019s responses with current external\ndata, improving accuracy and relevance.\n\u2022 Reducing Inaccurate Responses: Grounds the LLM\u2019s output in relevant knowledge, reducing\nthe risk of generating incorrect information.\n\u2022 Domain-Specific Responses: Delivers contextually relevant responses tailored to an organisation\u2019s proprietary data.\n\u2022 Efficiency and Cost-Effectiveness: Offers a cost-effective method for customising LLMs without\nextensive model fine-tuning.\n\n1.9.3\n\nChallenges and Considerations in Serving RAG\n\n1. User Experience: Ensuring rapid response times suitable for real-time applications.\n2. Cost Efficiency: Managing the costs associated with serving millions of responses.\n3. Accuracy: Ensuring outputs are accurate to avoid misinformation.\n4. Recency and Relevance: Keeping responses and content current with the latest data.\n5. Business Context Awareness: Aligning LLM responses with specific business contexts.\n6. Service Scalability: Managing increased capacity while controlling costs.\n7. Security and Governance: Implementing protocols for data security, privacy, and governance.\n\n1.9.4\n\nUse Cases and Examples\n\n1. Question and Answer Chatbots: Integrate LLMs with chatbots to generate accurate answers\nfrom company documents, enhancing customer support.\n2. Search Augmentation: Enhance search engines with LLM-generated answers for more accurate\ninformational queries.\n3. Knowledge Engine: Use LLMs to answer questions related to internal functions, such as HR\nand compliance, using company data.\n\n1.9.5\n\nConsiderations for Choosing Between RAG and Fine-Tuning\n\nWhen considering external data access, RAG is likely a superior option for applications needing to access\nexternal data sources. Fine-tuning, on the other hand, is more suitable if you require the model to adjust its behaviour, and writing style, or incorporate domain-specific knowledge. In terms of suppressing\nhallucinations and ensuring accuracy, RAG systems tend to perform better as they are less prone to generating incorrect information. If you have ample domain-specific, labelled training data, fine-tuning can\nresult in a more tailored model behaviour, whereas RAG systems are robust alternatives when such data\nis scarce. RAG systems provide an advantage with dynamic data retrieval capabilities for environments\nwhere data frequently updates or changes. Additionally, it is crucial to ensure the transparency and\ninterpret ability of the model\u2019s decision-making process. In that case, RAG systems offer insight that is\ntypically not available in models that are solely fine-tuned. Figure1.5 illustrates the visual representation\nalongside example use cases.\n\n12\n\n\fFigure 1.5: Graph comparing the model adaptation required versus the level of external knowledge needed\nacross different scenarios, highlighting the roles of Retrieval-Augmented Generation (RAG), Fine-Tuning,\nand their hybrid applications in various contexts such as Q&A systems, customer support automation,\nand summarisation tasks. (adapted from [14])\n\n1.10\n\nObjectives of the Report\n\n1.10.1\n\nGoals and Scope\n\nThe primary goal of this report is to conduct a comprehensive analysis of fine-tuning techniques for LLMs.\nThis involves exploring theoretical foundations, practical implementation strategies, and challenges. The\nreport examines various fine-tuning methodologies, their applications, and recent advancements.\n\n1.10.2\n\nKey Questions and Issues Addressed\n\nThis report addresses critical questions surrounding fine-tuning LLMs, starting with foundational insights into LLMs, their evolution, and significance in NLP. It defines fine-tuning, distinguishes it from\npre-training, and emphasises its role in adapting models for specific tasks. Key objectives include enhancing model performance for targeted applications and domains.\nThe report outlines a structured fine-tuning process, featuring a high-level pipeline with visual representations and detailed stage explanations. It covers practical implementation strategies, including\nmodel initialisation, hyperparameter definition, and fine-tuning techniques such as Parameter-Efficient\nFine-Tuning (PEFT) and Retrieval-Augmented Generation (RAG). Industry applications, evaluation\nmethods, deployment challenges, and recent advancements are also explored.\n\n1.10.3\n\nOverview of the Report Structure\n\nThe rest of the report provides a comprehensive understanding of fine-tuning LLMs. The main chapters\ninclude an in-depth look at the fine-tuning pipeline, practical applications, model alignment, evaluation\nmetrics, and challenges. The concluding sections discuss the evolution of fine-tuning techniques, highlight\nongoing research challenges, and provide insights for researchers and practitioners.\n\n13\n\n\fChapter 2\n\nSeven Stage Fine-Tuning Pipeline\nfor LLM\nFine-tuning a Large Language Model (LLM) is a comprehensive process divided into seven distinct\nstages, each essential for adapting the pre-trained model to specific tasks and ensuring optimal performance. These stages encompass everything from initial dataset preparation to the final deployment\nand maintenance of the fine-tuned model. By following these stages systematically, the model is refined\nand tailored to meet precise requirements, ultimately enhancing its ability to generate accurate and\ncontextually appropriate responses. The seven stages include Dataset Preparation, Model Initialisation,\nTraining Environment Setup, Fine-Tuning, Evaluation and Validation, Deployment, and Monitoring and\nMaintenance.\nFigure 2.1 illustrates the comprehensive pipeline for fine-tuning LLMs, encompassing all necessary stages\nfrom dataset preparation to monitoring and maintenance.\n\n2.1\n\nStage 1: Dataset Preparation\n\nFine-tuning a Large Language Model (LLM) starts with adapting the pre-trained model for specific tasks\nby updating its parameters using a new dataset. This involves cleaning and formatting the dataset to\nmatch the target task, such as instruction tuning, sentiment analysis, or topic mapping. The dataset is\ncomposed of < input, output > pairs, demonstrating the desired behaviour for the model.\nFor example, in instruction tuning, the dataset may look like:\n###Human: $<Input Query>$\n###Assistant: $<Generated Output>$\nHere, the \u2019Input Query\u2019 is what the user asks, and the \u2019Generated Output\u2019 is the model\u2019s response. The\nstructure and style of these pairs can be adjusted based on the specific needs of the task.\n\n2.2\n\nStage 2: Model Initialisation\n\nModel initialisation is the process of setting up the initial parameters and configurations of the LLM\nbefore training or deploying it. This step is crucial for ensuring the model performs optimally, trains\nefficiently, and avoids issues such as vanishing or exploding gradients.\n\n2.3\n\nStage 3: Training Environment Setup\n\nSetting up the training environment for LLM fine-tuning involves configuring the necessary infrastructure\nto adapt a pre-existing model for specific tasks. This includes selecting relevant training data, defining the\nmodel\u2019s architecture and hyperparameters, and running training iterations to adjust the model\u2019s weights\nand biases. The aim is to enhance the LLM\u2019s performance in generating accurate and contextually\nappropriate outputs tailored to specific applications, like content creation, translation, or sentiment\nanalysis. Successful fine-tuning relies on careful preparation and rigorous experimentation.\n\n14\n\n\fFigure 2.1: A comprehensive pipeline for fine-tuning Large Language Models (LLMs), illustrating the\nseven essential stages: Dataset Preparation, Model Initialisation, Training Environment Setup, FineTuning, Evaluation and Validation, Deployment, and Monitoring and Maintenance. Each stage plays\na crucial role in adapting the pre-trained model to specific tasks and ensuring optimal performance\nthroughout its lifecycle.\n\n2.4\n\nStage 4: Partial or Full Fine-Tuning\n\nThis stage involves updating the parameters of the LLM using a task-specific dataset. Full fine-tuning updates all parameters of the model, ensuring comprehensive adaptation to the new task. Alternatively, Half\nfine-tuning (HFT) [15] or Parameter-Efficient Fine-Tuning (PEFT) approaches, such as using adapter\nlayers, can be employed to partially fine-tune the model. This method attaches additional layers to the\npre-trained model, allowing for efficient fine-tuning with fewer parameters, which can address challenges\nrelated to computational efficiency, overfitting, and optimisation.\n\n2.5\n\nStage 5: Evaluation and Validation\n\nEvaluation and validation involve assessing the fine-tuned LLM\u2019s performance on unseen data to ensure\nit generalises well and meets the desired objectives. Evaluation metrics, such as cross-entropy, measure\nprediction errors, while validation monitors loss curves and other performance indicators to detect issues\nlike overfitting or underfitting. This stage helps guide further fine-tuning to achieve optimal model\nperformance.\n\n15\n\n\f2.6\n\nStage 6: Deployment\n\nDeploying an LLM means making it operational and accessible for specific applications. This involves\nconfiguring the model to run efficiently on designated hardware or software platforms, ensuring it can\nhandle tasks like natural language processing, text generation, or user query understanding. Deployment\nalso includes setting up integration, security measures, and monitoring systems to ensure reliable and\nsecure performance in real-world applications.\n\n2.7\n\nStage 7: Monitoring and Maintenance\n\nMonitoring and maintaining an LLM after deployment is crucial to ensure ongoing performance and\nreliability. This involves continuously tracking the model\u2019s performance, addressing any issues that\narise, and updating the model as needed to adapt to new data or changing requirements. Effective\nmonitoring and maintenance help sustain the model\u2019s accuracy and effectiveness over time.\n\n16\n\n\fChapter 3\n\nStage 1: Data Preparation\n3.1\n\nSteps Involved in Data Preparation\n\n3.1.1\n\nData Collection\n\nThe first step in data preparation is to collect data from various sources. These sources can be in any\nformat such as CSV, web pages, SQL databases, S3 storage, etc. Python provides several libraries to\ngather the data efficiently and accurately. Table 3.1 presents a selection of commonly used data formats\nalong with the corresponding Python libraries used for data collection.\n\n3.1.2\n\nData Preprocessing and Formatting\n\nData preprocessing and formatting are crucial for ensuring high-quality data for fine-tuning. This step\ninvolves tasks such as cleaning the data, handling missing values, and formatting the data to match the\nspecific requirements of the task. Several libraries assist with text data processing and Table 3.2 contains\nsome of the most commonly used data preprocessing libraries in python.\n\n3.1.3\n\nHandling Data Imbalance\n\nHandling imbalanced datasets is crucial for ensuring balanced performance across all classes. Several\ntechniques and strategies are employed:\n1. Over-sampling and Under-sampling: Techniques like SMOTE (Synthetic Minority Oversampling Technique) generate synthetic examples to achieve balance.\nPython Library: imbalanced-learn\nDescription: imbalanced-learn provides various methods to deal with imbalanced datasets, including oversampling techniques like SMOTE.\n2. Adjusting Loss Function: Modify the loss function to give more weight to the minority class,\nsetting class weights inversely proportional to the class frequencies.\n3. Focal Loss: A variant of cross-entropy loss that adds a factor to down-weight easy examples and\nfocus training on hard negatives.\nPython Library: focal loss\nDescription: The focal loss package provides robust implementations of various focal loss functions, including BinaryFocalLoss and SparseCategoricalFocalLoss.\n4. Cost-sensitive Learning: Incorporating the cost of misclassifications directly into the learning\nalgorithm, assigning a higher cost to misclassifying minority class samples.\n5. Ensemble Methods: Using techniques like bagging and boosting to combine multiple models\nand handle class imbalance.\nPython Library: sklearn.ensemble\nDescription: scikit-learn provides robust implementations of various ensemble methods, including\nbagging and boosting.\n\n17\n\n\fData Format\nCSV Files\n\nPython\nbrary\npandas\n\nLi-\n\nWeb Pages\n\nBeautifulSoup\nand requests\n\nSQL Databases\n\nSQLAlchemy\n\nS3 Storage\n\nboto3\n\nData\ntion\n\nRapidMiner\n\nIntegra-\n\nData Cleaning\n\nTrifacta Wrangler\n\nDescription\n\nLibrary Link\n\npandas is a powerful library for data manipulation and analysis. It provides the\nread csv function for easy and efficient\nreading of CSV files into DataFrame objects. It also supports reading data in\nExcel, JSON, and more.\nBeautifulSoup is a library for parsing\nHTML and XML documents. Combined\nwith requests for sending HTTP requests, it enables data extraction from\nweb pages, essential for web scraping\ntasks.\nSQLAlchemy is a SQL toolkit and\nObject-Relational Mapping (ORM) library for Python, providing a full suite\nof enterprise-level persistence patterns.\nboto3 is the Amazon Web Services\n(AWS) SDK for Python, allowing developers to use services like Amazon S3 and\nEC2. It enables interaction with AWS\nservices, including uploading, downloading, and managing S3 bucket files.\nRapidMiner is a comprehensive environment for data preparation, machine\nlearning, and predictive analytics, allowing efficient processing and transformation of raw data into actionable insights.\nTrifacta Wrangler focuses on simplifying and automating data wrangling processes, transforming raw data into clean\nand structured formats.\n\npandas documentation\n\nBeautifulSoup\ndocumentation,\nrequests documentation\n\nSQLAlchemy documentation\n\nboto3 documentation\n\nRapidMiner documentation\n\nTrifacta Wrangler\ndocumentation\n\nTable 3.1: Python libraries and tools for data collection and integration in various formats, providing\nan overview of commonly used libraries, their functions, and links to their official documentation for\nefficient data management and processing.\n6. Stratified Sampling: Ensuring that each mini-batch during training contains an equal or proportional representation of each class.\nPython Library: sklearn.model selection.StratifiedShuffleSplit\nDescription: scikit-learn offers tools for stratified sampling, ensuring balanced representation\nacross classes.\n7. Data Cleaning: Removing noisy and mislabelled data, which can disproportionately affect the\nminority class.\nPython Library: pandas.DataFrame.sample\nDescription: pandas provides methods for sampling data from DataFrames, useful for data cleaning and preprocessing.\n8. Using Appropriate Metrics: Metrics like Precision-Recall AUC, F1-score, and Cohen\u2019s Kappa\nare more informative than accuracy when dealing with imbalanced datasets.\nPython Library: sklearn.metrics\nDescription: scikit-learn offers a comprehensive set of tools for evaluating the performance of\nclassification models, particularly with imbalanced datasets.\n\n18\n\n\fLibrary Name\nspaCy\n\nNLTK\n\nHuggingFace\n\nKNIME\n\nData Preprocessing Options\nspaCy provides robust capabilities for text preprocessing, including tokenization, lemmatization, and\nefficient sentence boundary detection.\nNLTK offers a comprehensive set of tools for data\npreprocessing, such as tokenization, stemming, and\nstop word removal.\nHuggingFace provides extensive capabilities for\ntext preprocessing through its transformers library,\nincluding functionalities for tokenization and support for various pre-trained models.\nKNIME Analytics Platform allows visual workflow\ndesign for data integration, preprocessing, and advanced manipulations like text mining and image\nanalysis.\n\nLink\nspaCy documentation\n\nNLTK documentation\n\nHuggingFace documentation\n\nKNIME documentation\n\nTable 3.2: Outline of Python libraries commonly used for text data preprocessing, including spaCy,\nNLTK, HuggingFace, and KNIME. It details the specific preprocessing options offered by each library\nand provides links to their official documentation for users seeking more in-depth guidance on their use.\n\n3.1.4\n\nSplitting Dataset\n\nSplitting the dataset for fine-tuning involves dividing it into training and validation sets, typically using\nan 80:20 ratio. Different techniques include:\n1. Random Sampling: Selecting a subset of data randomly to create a representative sample.\nPython Library: sklearn.model selection.train test split\n2. Stratified Sampling: Dividing the dataset into subgroups and sampling from each to maintain\nclass balance.\nPython Library: sklearn.model selection.StratifiedShuffleSplit\n3. K-Fold Cross Validation: Splitting the dataset into K folds and performing training and validation K times.\nPython Library: sklearn.model selection.KFold\n4. Leave-One-Out Cross Validation: Using a single data point as the validation set and the rest\nfor training, repeated for each data point.\nPython Library: sklearn.model selection.LeaveOneOut\nFurther details can be found in scikit-learn\u2019s documentation on model selection.\n\n3.2\n\nExisting and Potential Research Methodologies\n\n3.2.1\n\nData Annotation\n\nData annotation involves labelling or tagging textual data with specific attributes relevant to the model\u2019s\ntraining objectives. This process is crucial for supervised learning tasks and greatly influences the\nperformance of the fine-tuned model. Recent research highlights various approaches to data annotation:\n\u2022 Human Annotation: Manual annotation by human experts remains a gold standard due to its\naccuracy and context understanding. However, it is time-consuming and costly for large datasets\n[16]. Tools like Excel, Prodigy1 , and Innodata2 facilitate this process.\n\u2022 Semi-automatic Annotation: Combining machine learning algorithms with human review to\ncreate labelled datasets more efficiently. This approach balances efficiency and accuracy. Tools\nlike Snorkel3 use weak supervision to generate initial labels, which are then refined by human\nannotators [17].\n1 https://prodi.gy\n2 https://innodata.com/\n3 https://snorkel.ai/\n\n19\n\n\f\u2022 Automatic Annotation: Fully automated annotation leverages machine learning algorithms to\nlabel data without human intervention, offering scalability and cost-effectiveness. Services like\nAmazon SageMaker Ground Truth4 utilise machine learning to automate data labelling, although the accuracy may vary depending on the complexity of the task [18].\n\n3.2.2\n\nData Augmentation\n\nData Augmentation (DA) techniques expand training datasets artificially to address data scarcity and\nimprove model performance. Advanced techniques often used in NLP include:\n\u2022 Word Embeddings: Using word embeddings like Word2Vec and GloVe to replace words with\ntheir semantic equivalents, thereby generating new data instances [19, 20].\n\u2022 Back Translation: Translating text to another language and then back to the original language\nto create paraphrased data. This technique helps in generating diverse training samples [21]. Tools\nlike Google Translate API5 are commonly used for this purpose.\n\u2022 Adversarial Attacks: Generating augmented data through adversarial examples that slightly\nmodify the original text to create new training samples while preserving the original meaning [22].\nLibraries like TextAttack6 provide frameworks for such augmentations.\n\u2022 NLP-AUG7 : This library offers a variety of augmenters for character, word, sentence, audio, and\nspectrogram augmentation, enhancing dataset diversity.\n\n3.2.3\n\nSynthetic Data Generation using LLMs\n\nLarge Language Models (LLMs) can generate synthetic data through innovative techniques such as:\n\u2022 Prompt Engineering: Crafting specific prompts to guide LLMs like GPT-3 in generating relevant\nand high-quality synthetic data [23].\n\u2022 Multi-Step Generation: Employing iterative generation processes where LLMs generate initial\ndata that is refined through subsequent steps [24]. This method can produce high-quality synthetic\ndata for various tasks, including summarising and bias detection.\nIt is crucial to verify the accuracy and relevance of synthetic data generated by LLMs before using\nthem for fine-tuning processes [25].\n\n3.3\n\nChallenges in Data Preparation for Fine-Tuning LLMs\n\nKey challenges in data preparation include:\n1. Domain Relevance: Ensuring that the data is relevant to the specific domain for accurate model\nperformance. Mismatched domain data can lead to poor generalisation and inaccurate outputs\n[26].\n2. Data Diversity: Including diverse and well-balanced data to prevent model biases and improve\ngeneralisation. A lack of diversity can cause the model to perform poorly on underrepresented\nscenarios [27].\n3. Data Size: Managing and processing large datasets, with at least 1000 samples recommended for\neffective fine-tuning. However, large datasets pose challenges in terms of storage, computational\nrequirements, and processing time.\n4. Data Cleaning and Preprocessing: Removing noise, errors, and inconsistencies are critical for\nproviding clean inputs to the model. Poorly preprocessed data can degrade model performance\nsignificantly.\n4 https://aws.amazon.com/sagemaker/groundtruth/\n5 https://translate.google.com/?sl=auto&tl=en&op=translate\n6 https://github.com/QData/TextAttack\n7 https://github.com/makcedward/nlpaug\n\n20\n\n\f5. Data Annotation: Ensuring precise and consistent labelling is essential for tasks requiring labelled data. Inconsistent annotation can lead to unreliable model predictions.\n6. Handling Rare Cases: Adequately representing rare but important instances in the dataset to\nensure the model can generalise to less frequent but critical scenarios.\n7. Ethical Considerations: Scrutinising data for harmful or biased content to prevent unintended\nconsequences. Ethical data handling includes removing biases and ensuring privacy [28].\n\n3.4\n\nAvailable LLM Fine-Tuning Datasets\n\nFor a comprehensive list of datasets suitable for fine-tuning LLMs, refer to resources like LLMXplorer,\nwhich provides domain and task-specific datasets.\n\n3.5\n\nBest Practices\n\n3.5.1\n\nHigh-Quality Data Collection\n\nEnsuring high-quality, diverse, and representative data is critical. Leveraging curated sources and ensuring comprehensive coverage across different scenarios enhances model robustness [29]. Tools like\nDataRobot Paxata8 and KNIME Analytics Platform9 offer robust data profiling and transformation capabilities.\n\n3.5.2\n\nEffective Data Preprocessing\n\nProper data preprocessing is essential for model performance. Utilising libraries like spaCy, NLTK, and\nHuggingFace Transformers can streamline preprocessing tasks. Platforms like Trifacta Wrangler\nand RapidMiner automate data cleaning tasks, improving efficiency and ensuring consistency [30].\n\n3.5.3\n\nManaging Data Imbalance\n\nAddressing data imbalance is crucial. Techniques like over-sampling, under-sampling, and SMOTE\nhelp balance datasets. Libraries like imbalanced-learn and ensemble methods in scikit-learn provide\nrobust tools for managing imbalanced datasets [31].\n\n3.5.4\n\nAugmenting and Annotating Data\n\nData augmentation and annotation improve model robustness. Tools like NLP-AUG, TextAttack,\nand Snorkel offer sophisticated capabilities for creating diverse and well-labelled datasets [32, 33].\n\n3.5.5\n\nEthical Data Handling\n\nEnsuring ethical data handling involves thorough scrutiny for biases and privacy concerns. Implementing privacy-preserving techniques and filtering harmful content is critical. Services like Amazon SageMaker Ground Truth ensure scalable and secure data annotation [34].\n\n3.5.6\n\nRegular Evaluation and Iteration\n\nContinuous evaluation and iteration of the data preparation pipeline help maintain data quality and\nrelevance. Leveraging feedback loops and performance metrics ensures ongoing improvements and adaptation to new data requirements.\nBy integrating these best practices, researchers and practitioners can enhance the effectiveness of LLM\nfine-tuning, ensuring robust and reliable model performance.\n\n8 https://www.datarobot.com/platform/preparation/\n9 https://www.knime.com/\n\n21\n\n\fChapter 4\n\nStage 2: Model Initialisation\n4.1\n\nSteps Involved in Model Initialisation\n\nFigure 4.1: Sequential steps involved in Initialising a Large Language Model (LLM), illustrating the\nprocess from setting up the environment to executing tasks. Each step is critical for ensuring that the\nLLM is correctly configured and ready for operation. This includes installing necessary dependencies,\nimporting libraries, selecting and downloading the appropriate language model from a repository, and\nfinally, loading the model to perform specific tasks.\n1. Set Up the Environment: Configure your environment, such as setting up GPU/TPU usage if\navailable, which can significantly speed up model loading and inference.\n2. Install the Dependencies: Ensure that all necessary software and libraries are installed. This\ntypically includes package managers like pip and frameworks like PyTorch or TensorFlow.\n\n22\n\n\f3. Import the Libraries: Import the required libraries in your script or notebook. Common libraries\ninclude transformers from Hugging Face, torch for PyTorch, and other utility libraries.\n4. Choose the Language Model: Select the appropriate pre-trained language model based on your\ntask requirements. This could be models like BERT, GPT-3, or others available on platforms like\nHugging Face\u2019s Model Hub.\n5. Download the Model from the Repository: Use the chosen framework\u2019s functions to download\nthe pre-trained model from an online repository. For instance, using transformers, you might use\nAutoModel.from pretrained(\u2019model name\u2019).\n6. Load the Model in the Memory: Load the model into memory, ready for inference or further\nfine-tuning. This step ensures the model weights are initialised and ready for use.\n7. Execute Tasks: Perform the desired tasks using the loaded model. This could involve making\npredictions, generating text, or fine-tuning the model on a new dataset.\n\n4.2\n\nTools and Libraries for Model Initialisation\n\nPython offers a wide range of libraries for Initialising large language models, providing access to both\nopen and closed-source models. Here are some notable libraries:\n1. Python Library: HuggingFace\nDescription: HuggingFace is renowned for its support of numerous pre-trained large language\nmodels, ranging from Phi-3 mini to Llama-3 70B. The transformers library, part of HuggingFace,\nenables users to access these models via classes such as AutoModelForCausalLM. This library\nsupports loading fine-tuned models as well as 4-bit quantised models. Additionally, the transformers\nlibrary includes the \u201dpipeline\u201d feature, making it easy to use pre-trained models for various tasks\n[35].\n2. Python Framework: PyTorch\nDescription: PyTorch offers comprehensive tools and libraries for Initialising and fine-tuning\nlarge language models. It provides a flexible and efficient platform for building and deploying deep\nlearning models. HuggingFace\u2019s transformers library bridges the gap between PyTorch and other\nframeworks, enhancing its usability for state-of-the-art language models [36].\n3. Python Framework: TensorFlow\nDescription: TensorFlow also provides extensive tools and libraries for Initialising and fine-tuning\nlarge language models. Similar to PyTorch, it benefits from the HuggingFace transformers library,\nwhich provides a versatile and user-friendly API and interface for working with the latest advancements in large language models [37].\n\n23\n\n\f4.3\n\nChallenges in Model Initialisation\n\nChallenge\nAlignment with the\nTarget Task\nUnderstanding the\nPre-trained Model\n\nAvailability and\nCompatibility\nModel Architecture\n\nResource Constraints\n\nPrivacy\n\nCost and Maintenance\n\nModel Size and\nQuantisation\n\nPre-training Datasets\n\nBias Awareness\n\nDescription\nIt\u2019s essential that the pre-trained model closely aligns with your specific\ntask or domain. This initial alignment serves as a solid foundation for\nfurther fine-tuning efforts, leading to improved efficiency and results [38].\nBefore making a selection, it\u2019s crucial to thoroughly comprehend the\narchitecture, capabilities, limitations, and the tasks the model was originally trained on. Without this understanding, fine-tuning efforts may\nnot yield the desired outcomes [23].\nCareful consideration of a model\u2019s documentation, license, maintenance,\nand update frequency is necessary to avoid potential issues and ensure\nsmooth integration into your application.\nNot all models excel at every task. Each model architecture has its\nstrengths and weaknesses, so selecting one aligned with your specific\ntask is essential for favourable outcomes [39].\nLoading pre-trained LLMs is resource-heavy and requires more computation. These models need high-performance CPUs and GPUs and a\nsignificant amount of disk space. For instance, the Llama 3 8B model\nrequires a minimum of 16GB of memory to load and run the inference.\nPrivacy and confidentiality are crucial factors when selecting a large language model (LLM). Many businesses prefer not to share their data\nwith external LLM providers. In such instances, hosting an LLM on\nlocal servers or using pre-trained LLMs available through private cloud\nproviders can be viable solutions. These approaches ensure that data\nremains within the company\u2019s premises, thereby preserving privacy and\nconfidentiality.\nHosting LLMs on local servers entails significant time and expense for\nsetup and ongoing maintenance. Conversely, utilising cloud vendors alleviates concerns about resource maintenance but incurs monthly billing\ncosts. These charges are typically based on factors such as model size\nand the volume of requests per minute.\nutilising a pre-trained model with high memory consumption can still be\nviable by employing its quantised version. Through quantisation, pretrained weights can be loaded with reduced precision, typically 4-bit or\n8-bit floating point, substantially diminishing parameter volume while\nmaintaining considerable accuracy [40].\nExamine the datasets used for pre-training to gauge the model\u2019s understanding of language. These are important as there are models available\nspecifically for performing code generation, and we do not want to use\nthose models for finance text classification [41].\nBe vigilant regarding potential biases in pre-trained models, especially if\nunbiased predictions are required. The bias awareness can be evaluated\nby testing different models and backtracking the datasets used for pretraining [42].\n\nTable 4.1: Comprehensive Overview of Challenges in Initialising a Large Language Model (LLM). This\ntable highlights critical considerations, such as the importance of aligning pre-trained models with specific\ntasks, understanding model architecture and compatibility, managing resource constraints, and ensuring\ndata privacy. Additionally, it discusses the challenges related to cost, maintenance, and the complexities\nof model size, quantisation, and bias awareness. Each challenge is associated with specific references to\nensure thorough understanding and proper model deployment.\n\n4.4\n\nTutorials\n\n1. Summarisation using Llama 3\n\n24\n\n\f2. HuggingFace tutorial for getting started with LLMs\n3. PyTorch tutorial for fine-tuning models\n4. TensorFlow tutorial for transformer models\n\n25\n\n\fChapter 5\n\nStage 3: Training Setup\n5.1\n\nSteps Involved in Training Setup\n\n1. Setting up the training environment: When setting up the environment for training an LLM,\nit is crucial to configure high-performance hardware, such as GPUs or TPUs, and ensure proper\ninstallation of necessary software components like CUDA, cuDNN, and deep learning frameworks\nsuch as PyTorch or TensorFlow. Verify hardware recognition and compatibility with the software to\nleverage computational power effectively, reducing training time and improving model performance.\n2. Defining the Hyper-parameters: When defining hyperparameters for fine-tuning an LLM, it is\nessential to carefully tune key parameters such as learning rate, batch size, and epochs to optimise\nthe model\u2019s performance.\n3. Initialising Optimisers and Loss Functions: When initialising optimisers and loss functions\nfor fine-tuning an LLM, it is crucial to select the appropriate optimiser to efficiently update the\nmodel\u2019s weights and the correct loss function to measure model performance [43].\n\n5.2\n\nSetting up Training Environment\n\nWhen fine-tuning a large language model (LLM), the computational environment plays a crucial role in\nensuring efficient training. To achieve optimal performance, it\u2019s essential to configure the environment\nwith high-performance hardware such as GPUs (Graphics Processing Units) or TPUs (Tensor Processing\nUnits). GPUs, such as the NVIDIA A100 or V100, are widely used for training deep learning models\ndue to their parallel processing capabilities. For larger-scale operations, TPUs offered by Google Cloud\ncan provide even greater acceleration [44].\nFirst, ensure that your system or cloud environment has the necessary hardware installed. For GPUs,\nthis involves setting up CUDA1 (Compute Unified Device Architecture) and cuDNN2 (CUDA Deep Neural Network library) from NVIDIA, which are essential for enabling GPU acceleration. For TPU usage,\nyou would typically set up a Google Cloud environment with TPU instances, which includes configuring\nthe TPU runtime in your training scripts.\nVerify that your hardware is correctly recognised and utilised by your deep learning frameworks. In\nPyTorch, for instance, you can check GPU availability with torch.cuda.is available(). Properly setting\nup and testing the hardware ensures that the training process can leverage the computational power\neffectively, reducing training time and improving model performance [36].\nWhen fine-tuning an LLM, both software and hardware considerations are paramount to ensure a smooth\nand efficient training process. On the software side, you need a compatible deep learning framework like\nPyTorch or TensorFlow. These frameworks have extensive support for LLMs and provide utilities for\nefficient model training and evaluation. Installing the latest versions of these frameworks, along with\nany necessary dependencies, is crucial for leveraging the latest features and performance improvements\n1 https://developer.nvidia.com/cuda-toolkit\n2 https://developer.nvidia.com/cudnn\n\n26\n\n\f[45].\nAdditionally, use libraries like Hugging Face\u2019s transformers to simplify the process of loading pre-trained\nmodels and tokenizers. This library is particularly well-suited for working with various LLMs and offers\na user-friendly interface for model fine-tuning. Ensure that all software components, including libraries\nand dependencies, are compatible with your chosen framework and hardware setup [35].\nOn the hardware side, consider the memory requirements of the model and your dataset. LLMs typically require substantial GPU memory, so opting for GPUs with higher VRAM (e.g., 16GB or more)\ncan be beneficial. If your model is exceptionally large or if you are training with very large datasets,\ndistributed training across multiple GPUs or TPUs might be necessary. This requires a careful setup of\ndata parallelism or model parallelism techniques to efficiently utilise the available hardware [46].\nLastly, ensure robust cooling and power supply for your hardware, as training LLMs can be resourceintensive, generating significant heat and requiring consistent power. Proper hardware setup not only\nenhances training performance but also prolongs the lifespan of your equipment [47].\n\n5.3\n\nDefining Hyperparameters\n\nKey hyperparameters like learning rate, batch size, epochs are crucial for enhancing the model\u2019s performance and obtaining superior outcomes. This process entails adjusting hyperparameters and training\nsettings to align with your particular use case. Below are the key hyperparameters:\n1. Learning Rate: Fine-tuning an LLM involves using optimisation algorithms like stochastic gradient descent (SGD). This technique estimates the error gradient for the model\u2019s current state using\nsamples from the training dataset and subsequently updates the model\u2019s weights via the backpropagation of errors algorithm. The learning rate dictates the speed at which the model adapts to the\nproblem. Smaller learning rates necessitate more training due to the minimal weight adjustments\nper update, while larger learning rates lead to quicker changes to weights [48].\n2. Batch Size: A batch refers to a subset of the training data used to update a model\u2019s weights\nduring the training process. Batch training involves dividing the entire training set into smaller\ngroups, updating the model after processing each batch. The batch size is a hyperparameter that\ndetermines the number of samples processed before the model parameters are updated.\n3. Epochs: Epoch refers to a full pass through the entire training dataset. This involves a complete\nforward and backward pass through the dataset. The dataset can be processed as a single batch\nor divided into multiple smaller batches. An epoch is considered complete once the model has\nprocessed all batches and updated its parameters based on the calculated loss.\n\n5.3.1\n\nMethods for Hyperparameter Tuning\n\nLLM hyperparameter tuning involves adjusting various hyperparameters during the training process\nto identify the optimal combination that yields the best output. This process often entails significant\ntrial and error, meticulously tracking each hyperparameter adjustment, and recording the resulting\nperformance. Conducting this manually can be highly time-consuming. To address this, automated\nhyperparameter tuning methods have been developed to streamline the process. The three most common\nmethods of automated hyperparameter tuning are random search, grid search, and Bayesian optimisation:\n1. Random Search: This method randomly selects and evaluates combinations of hyperparameters\nfrom a specified range. It is a straightforward and efficient approach capable of exploring a large\nparameter space. However, it may not always find the optimal combination of hyperparameters\nand can be computationally expensive [49].\n2. Grid Search: Unlike random search, grid search exhaustively evaluates every possible combination\nof hyperparameters from a given range. Although resource-intensive, this systematic approach\nensures that the optimal set of hyperparameters is found [50].\n\n27\n\n\f3. Bayesian Optimisation: This method uses a probabilistic model to predict the performance of\ndifferent hyperparameters and selects the best ones accordingly. It is an efficient method that can\nhandle large parameter spaces better and is less resource-intensive than grid search. However, it is\nmore complex to set up and may be less reliable in identifying the optimal set of hyperparameters\ncompared to grid search.\n4. Automated hyperparameter tuning: This facilitates the development of multiple language\nmodels, each with a unique combination of hyperparameters. By training these models on the same\ndataset, it becomes possible to compare their outputs and determine which configuration is best\nsuited for the desired use case. Additionally, models tuned with different sets of hyperparameters\ncan be tailored to various specific applications.\n\n5.4\n\nInitialising Optimisers and Loss Functions\n\nChoosing the right optimiser and loss function is crucial for training and fine-tuning LLMs. Below\nare descriptions of some commonly used optimisation algorithms, their advantages, disadvantages, and\nappropriate use cases:\n\n5.4.1\n\nGradient Descent\n\nGradient Descent is a fundamental optimisation algorithm used to minimise cost functions in machine\nlearning models. It aims to find the optimal parameters for a neural network.\nHow it Works: Gradient Descent iteratively updates model parameters in the direction of the\nnegative gradient of the cost function. It calculates gradients for each parameter and applies updates\nacross all data points until convergence. This method utilises the entire dataset to calculate gradients,\noften requiring a fixed learning rate and being sensitive to the scale of data and learning rate choice.\nPros:\n\u2022 Simple and easy to implement.\n\u2022 Intuitive and easy to understand.\n\u2022 Converges to the global minimum for convex functions.\n\u2022 Suitable for small-scale problems.\nCons:\n\u2022 Computationally expensive on large datasets.\n\u2022 May get stuck in local minima.\n\u2022 Requires a large number of iterations.\n\u2022 Sensitive to the choice of learning rate.\nWhen to Use: Gradient Descent is best used for small datasets where gradient computation is\ncheap and simplicity and clarity are preferred.\n\n5.4.2\n\nStochastic Gradient Descent (SGD)\n\nStochastic Gradient Descent (SGD) is a variant of Gradient Descent that focuses on reducing computation\nper iteration.\nHow it Works: SGD updates parameters using a single or few data points at each iteration, introducing randomness in updates. It reduces the computational burden per iteration and often converges\nfaster than batch Gradient Descent. However, it requires a smaller learning rate due to higher variance\nand benefits from momentum to stabilise updates.\nPros:\n\u2022 Fast and handles large datasets well.\n\u2022 Efficient memory usage.\n\n28\n\n\f\u2022 Simple and easy to implement.\n\u2022 Can escape local minima due to noise.\nCons:\n\u2022 High variance in updates can lead to instability.\n\u2022 Can overshoot the minimum.\n\u2022 Sensitive to the choice of learning rate.\n\u2022 Can be slower to converge compared to batch methods.\nWhen to Use: SGD is ideal for large datasets, incremental learning scenarios, and real-time learning\nenvironments where computational resources are limited.\n\n5.4.3\n\nMini-batch Gradient Descent\n\nMini-batch Gradient Descent combines the efficiency of SGD and the stability of batch Gradient Descent,\noffering a compromise between batch and stochastic approaches.\nHow it Works: It splits data into small batches and updates parameters using gradients averaged\nover each mini-batch. This reduces variance compared to SGD and is more efficient than batch Gradient\nDescent, helping in generalising the updates.\nPros:\n\u2022 Balances between efficiency and stability.\n\u2022 More generalisable updates.\n\u2022 Reduces the variance of parameter updates.\n\u2022 Provides a compromise between SGD and batch.\nCons:\n\u2022 Requires tuning of batch size.\n\u2022 Can still be computationally expensive for very large datasets.\n\u2022 More complex implementation.\n\u2022 Can require more iterations than full-batch Gradient Descent.\nWhen to Use: Mini-batch Gradient Descent is suitable for most deep learning tasks, especially\nwhen working with moderate to large datasets.\n\n5.4.4\n\nAdaGrad\n\nAdaptive Gradient Algorithm (AdaGrad) is designed for sparse data and high-dimensional models, adjusting learning rates to improve performance on sparse data.\nHow it Works: AdaGrad adapts the learning rate for each parameter based on historical gradient information, accumulating squared gradients. This approach prevents large updates for frequent\nparameters and helps in dealing with sparse features.\nPros:\n\u2022 Adapts learning rate for each parameter.\n\u2022 Good for sparse data.\n\u2022 No need to manually tune learning rates.\n\u2022 Works well with high-dimensional data.\nCons:\n\u2022 Learning rate can diminish to zero, stopping learning.\n29\n\n\f\u2022 May require more tuning for convergence.\n\u2022 Accumulation of squared gradients can lead to overly small learning rates.\n\u2022 Can slow down significantly.\nWhen to Use: AdaGrad is useful for sparse datasets like text and images where learning rates need\nto adapt to feature frequency.\n\n5.4.5\n\nRMSprop\n\nRoot Mean Square Propagation (RMSprop) is an adaptive learning rate method designed to perform\nbetter on non-stationary and online problems.\nHow it Works: RMSprop modifies AdaGrad by using a moving average of squared gradients to\nadapt learning rates based on recent gradient magnitudes. It maintains a running average of squared\ngradients to help in maintaining steady learning rates.\nPros:\n\u2022 Addresses the diminishing learning rate problem of AdaGrad.\n\u2022 Adapts learning rate based on recent gradients.\n\u2022 Effective for recurrent neural networks.\n\u2022 More robust against non-stationary targets.\nCons:\n\u2022 Can still get stuck in local minima on non-convex problems.\n\u2022 Requires hyperparameter tuning.\n\u2022 Requires careful tuning of the decay rate.\n\u2022 Can be sensitive to the initial learning rate.\nWhen to Use: RMSprop is best for non-convex optimisation problems, training RNNs and LSTMs,\nand dealing with noisy or non-stationary objectives.\n\n5.4.6\n\nAdaDelta\n\nAdaptive Delta (AdaDelta) improves on AdaGrad and RMSprop, focusing on adaptive learning rates\nwithout diminishing too quickly.\nHow it Works: AdaDelta eliminates the need for a default learning rate by using a moving window\nof gradient updates. It adapts learning rates based on recent gradient magnitudes to ensure consistent\nupdates even with sparse gradients.\nPros:\n\u2022 Eliminates the need to set a default learning rate.\n\u2022 Addresses the diminishing learning rate issue.\n\u2022 Does not require manual tuning of the learning rate.\n\u2022 Handles gradient sparsity well.\nCons:\n\u2022 More complex than RMSprop and AdaGrad.\n\u2022 Can have slower convergence initially.\n\u2022 Can require more iterations to converge.\n\u2022 Implementation can be more complex.\nWhen to Use: AdaDelta is suitable for scenarios similar to RMSprop but is preferred when avoiding\nmanual learning rate setting.\n30\n\n\f5.4.7\n\nAdam\n\nAdaptive Moment Estimation (Adam) combines the advantages of AdaGrad and RMSprop, making it\nsuitable for problems with large datasets and high-dimensional spaces.\nHow it Works: Adam uses running averages of both gradients and their squared values to compute adaptive learning rates for each parameter. It includes bias correction and often achieves faster\nconvergence than other methods.\nPros:\n\u2022 Combines advantages of AdaGrad and RMSprop.\n\u2022 Adaptive learning rates.\n\u2022 Includes bias correction.\n\u2022 Fast convergence.\n\u2022 Works well with large datasets and high-dimensional spaces.\nCons:\n\u2022 Requires tuning of hyperparameters (though it often works well with defaults).\n\u2022 Computationally intensive.\n\u2022 Can lead to overfitting if not regularised properly.\n\u2022 Requires more memory.\nWhen to Use: Adam is widely used in most deep learning applications due to its efficiency and\neffectiveness, particularly in complex neural network architectures.\n\n5.4.8\n\nAdamW\n\nAdamW is an extension of Adam that includes weight decay regularisation to address overfitting issues\npresent in Adam.\nHow it Works: AdamW integrates L2 regularisation directly into the parameter updates, decoupling\nweight decay from the learning rate. This improves generalisation and is suitable for fine-tuning large\nmodels.\nPros:\n\u2022 Includes weight decay for better regularisation.\n\u2022 Combines Adam\u2019s adaptive learning rate with L2 regularisation.\n\u2022 Improves generalisation.\n\u2022 Reduces overfitting compared to Adam.\nCons:\n\u2022 Slightly more complex than Adam.\n\u2022 Requires careful tuning of the weight decay parameter.\n\u2022 Slightly slower than Adam due to additional computations.\n\u2022 Requires more memory.\nWhen to Use: AdamW is ideal for scenarios where regularisation is needed, such as preventing\noverfitting in large models and fine-tuning pre-trained models.\nA comprehensive collection of optimisation algorithms implemented within the PyTorch library can be\nfound in here. The Hugging Face Transformers package also offers a variety of optimisers for initialising\nand fine-tuning language models, available here.\n\n31\n\n\f5.5\n\nChallenges in Training Setup\n\n1. Ensuring compatibility and proper configuration of high-performance hardware like GPUs or TPUs\ncan be complex and time-consuming.\n2. Managing dependencies and versions of deep learning frameworks and libraries to avoid conflicts\nand leverage the latest features.\n3. Selecting an appropriate learning rate is critical, as too high a rate can cause suboptimal convergence, while too low a rate can make the training process excessively slow.\n4. Determining the optimal batch size that balances memory constraints and training efficiency, especially given the large memory requirements of LLMs.\n5. Choosing the right number of epochs to avoid underfitting or overfitting the model, requiring careful\nmonitoring and validation.\n6. Selecting the most suitable optimiser for the specific training task to efficiently update the model\u2019s\nweights.\n7. Choosing the correct loss function to accurately measure model performance and guide the optimisation process.\n\n5.6\n\nBest Practices\n\n\u2022 Optimal Learning Rate: Use a lower learning rate, typically between 1e-4 to 2e-4, to ensure\nstable convergence. A learning rate schedule, such as learning rate warm-up followed by a linear\ndecay, can also be beneficial. This helps in initially stabilising the training and then allowing the\nmodel to converge more accurately.\n\u2022 Batch Size Considerations: Opt for a batch size that balances memory constraints and training\nefficiency. Smaller batch sizes can help in achieving faster convergence but may require more\nfrequent updates. Conversely, larger batch sizes can be more memory-intensive but may lead to\nmore stable updates. Experiment with different batch sizes to find the optimal balance for your\nspecific use case.\n\u2022 Save Checkpoints Regularly: Regularly save model weights at various intervals across 5-8\nepochs to capture optimal performance without overfitting. Implement early stopping mechanisms\nto halt training once the model performance starts to degrade on the validation set, thereby preventing overfitting [51].\n\u2022 Hyperparameter Tuning: Utilise hyperparameter tuning methods like grid search, random\nsearch, and Bayesian optimisation to find the optimal set of hyperparameters. Tools such as\nOptuna, Hyperopt, and Ray Tune can automate this process and help in efficiently exploring the\nhyperparameter space [49].\n\u2022 Data Parallelism and Model Parallelism: For large-scale training, consider using data parallelism or model parallelism techniques to distribute the training workload across multiple GPUs or\nTPUs. Libraries like Horovod and DeepSpeed can facilitate efficient distributed training, helping\nto reduce training time and manage memory usage effectively [52, 53].\n\u2022 Regular Monitoring and Logging: Implement robust monitoring and logging to track training\nmetrics, resource usage, and potential bottlenecks. Tools like TensorBoard, Weights & Biases, and\nMLflow can provide real-time insights into the training process, allowing for timely interventions\nand adjustments.\n\u2022 Handling Overfitting and Underfitting: Ensure that your model generalises well by implementing techniques to handle overfitting and underfitting. regularisation techniques such as L2\nregularisation, dropout, and data augmentation can help prevent overfitting. Conversely, if your\nmodel is underfitting, consider increasing the model complexity or training for more epochs.\n\n32\n\n\f\u2022 Use Mixed Precision Training: Mixed precision training involves using both 16-bit and 32-bit\nfloating-point types to reduce memory usage and increase computational efficiency. This technique\ncan significantly speed up training and reduce the required memory footprint, especially when\nusing large models. NVIDIA\u2019s Apex and TensorFlow\u2019s mixed precision API provide support for\nimplementing mixed precision training [54].\n\u2022 Evaluate and Iterate: Continuously evaluate the model performance using a separate validation\nset and iterate on the training process based on the results. Regularly update your training data\nand retrain the model to keep it current with new data trends and patterns.\n\u2022 Documentation and Reproducibility: Maintain thorough documentation of your training\nsetup, including the hardware configuration, software environment, and hyperparameters used.\nEnsure reproducibility by setting random seeds and providing detailed records of the training\nprocess. This practice not only aids in debugging and further development but also facilitates\ncollaboration and sharing of results with the broader research community.\n\n33\n\n\fChapter 6\n\nStage 4: Selection of Fine-Tuning\nTechniques and Appropriate Model\nConfigurations\nThis chapter focuses on selecting appropriate fine-tuning techniques and model configurations that suit\nthe specific requirements of various tasks. Fine-tuning is a crucial stage where pre-trained models are\nadapted to specific tasks or domains.\n\n6.1\n\nSteps Involved in Fine-Tuning\n\nThe following steps outline the fine-tuning process, integrating advanced techniques and best practices.\n1. Initialise the Pre-Trained Tokenizer and Model: Begin by loading the pre-trained tokenizer\nand model. The tokenizer ensures that the input text is converted into a format the model can\nprocess, while the pre-trained model serves as the foundation for further adaptation. Depending\non the task, select a model that has been pre-trained on relevant data to provide a strong starting\npoint.\n2. Modify the Model\u2019s Output Layer: Adjust the model\u2019s output layer to align with the specific\nrequirements of the target task. This may involve modifying existing layers or adding new layers.\nFor instance, tasks like classification may require a softmax layer with the appropriate number of\nclasses, while text generation tasks might involve changes in the decoding mechanism.\n3. Choose an Appropriate Fine-Tuning Strategy: Select the fine-tuning strategy that best fits\nthe task and the model architecture. Some Options include:\n\u2022 Task-Specific Fine-Tuning: For tasks such as text summarisation, code generation, classification, and question answering, adapt the model using relevant datasets.\n\u2022 Domain-Specific Fine-Tuning: Tailor the model to comprehend and generate text relevant\nto specific domains, such as medical, financial, or legal fields.\n\u2022 Parameter-Efficient Fine-Tuning (PEFT): Techniques like LoRA, QLoRA, and adapters\nallow for fine-tuning with reduced computational costs by updating a small subset of model\nparameters.\n\u2022 Half Fine-Tuning (HFT): Balance between retaining pre-trained knowledge and learning\nnew tasks by updating only half of the model\u2019s parameters during each fine-tuning round.\n4. Set Up the Training Loop: Establish the training loop, incorporating the selected fine-tuning\nstrategy. The loop should include data loading, loss computation, backpropagation, and parameter\nupdates. When using PEFT methods, ensure that only the relevant parameters are updated\nto maximise efficiency. Implement techniques like dynamic learning rates and early stopping to\nenhance the training process.\n\n34\n\n\f5. Incorporate Techniques for Handling Multiple Tasks: If fine-tuning for multiple tasks,\nconsider strategies like fine-tuning with multiple adapters or leveraging Mixture of Experts (MoE)\narchitectures. These methods allow a single model to handle various tasks by utilising specialised\nsub-networks or adapters for each task.\n6. Monitor Performance on a Validation Set: Regularly evaluate the model\u2019s performance on\na validation set to ensure it generalises well to unseen data. Adjust hyperparameters such as\nlearning rate, batch size, and dropout rates based on the validation performance. Utilise advanced\nmonitoring tools to track metrics like accuracy, loss, and overfitting.\n7. Optimise Model Using Advanced Techniques: Employ techniques such as Proximal Policy\nOptimisation (PPO) for reinforcement learning scenarios, or Direct Preference Optimisation (DPO)\nfor aligning model outputs with human preferences. These techniques are particularly useful in\nfine-tuning models for tasks requiring nuanced decision-making or human-like responses.\n8. Prune and optimise the Model (if necessary): To deploy the model in resource-constrained\nenvironments, consider pruning techniques to reduce its size and complexity. This involves removing\nunnecessary parameters or components without significantly affecting performance. Utilise dynamic\npruning methods during inference to optimise the model on-the-fly for different scenarios.\n9. Continuous Evaluation and Iteration: Continuously evaluate the model\u2019s performance across\nvarious tasks using appropriate benchmarks. Iterate on the fine-tuning process, making adjustments\nbased on performance metrics and real-world testing. This iterative approach helps in refining the\nmodel to meet specific performance criteria.\n\n6.2\n\nFine-Tuning Strategies for LLMs\n\n6.2.1\n\nTask-Specific Fine-Tuning\n\nTask-specific fine-tuning adapts large language models (LLMs) for particular downstream tasks using\nappropriately formatted and cleaned data. Below is a summary of key tasks suitable for fine-tuning\nLLMs, including examples of LLMs tailored to these tasks.\nTask\nText Summarisation\n\nCode Generation\n\nClassification\n\nQ&A\n\nDescription\nCondensing long texts into coherent summaries while retaining key information. Approaches include Extractive (selecting key\nsentences) and Abstractive summarisation\n(generating new sentences).\nAutomatically generating programming code\nbased on natural language descriptions, partial code snippets, or structured data inputs.\nCategorising text into predefined labels such\nas Sentiment Analysis, Topic Classification,\nand Entity Classification.\nUnderstanding and generating accurate, contextually relevant answers to natural language questions.\n\nKey Models\nBERTSUM, GPT-3, T5\n\nCodex, GPT-3, CodeBERT\n\nBERT, RoBERTa, GPT-4\n\nBERT, GPT-3, T5\n\nTable 6.1: Overview of tasks such as text summarisation, code generation, classification, and Q&A, along\nwith their key LLMs and descriptions.\n\n6.2.2\n\nDomain-Specific Fine-Tuning\n\nDomain-specific fine-tuning focuses on tailoring the model to comprehend and produce text relevant to\na specific domain or industry. By fine-tuning the model on a dataset derived from the target domain,\nit enhances the model\u2019s contextual understanding and expertise in domain-specific tasks. Below are\nexamples of domain-specific LLMs.\n\n35\n\n\fMedical Domain\nModel Description: Med-PaLM 2 is trained on meticulously curated medical datasets and is capable\nof accurately answering medical questions, achieving performance comparable to that of medical professionals [55].\nBase Model: PaLM 2\nFine-tuned Model Parameters: Not Known\nFine-Tuning Techniques Used: Instruction fine-tuning\nDatasets Used:\n\u2022 MedQA\n\u2022 MedMCQA\n\u2022 LiveQA\n\u2022 MedicationQA\n\u2022 HealthSearchQA\nResults: Med-PaLM 2 outperformed GPT-4 in several key medical benchmarks, demonstrating superior\nperformance in handling complex medical knowledge and reasoning tasks.\nFinance Domain\nModel Description: FinGPT, an open-source LLM tailored for the financial sector, enhances financial\nresearch and cooperation by promoting data accessibility and handling finance-specific issues like data\nacquisition and quality [56].\nBase Model: LlaMA, ChatGLM, and other Transformer Models\nFine-tuned Model Parameters: Not Known\nFine-Tuning Techniques Used: LoRA, Reinforcement Learning on Stock Prices (RLSP)\nDatasets Used:\n\u2022 Financial News (Reuters, CNBC, Yahoo Finance)\n\u2022 Social Media (Twitter, Facebook, Reddit, Weibo)\n\u2022 Regulatory Filings (e.g., SEC filings)\n\u2022 Trends (Seeking Alpha, Google Trends)\n\u2022 Academic Datasets\nResults: Not Applicable\nLegal Domain\nModel Description: LAWGPT, the first open-source model specifically designed for Chinese legal\napplications, demonstrates superior capability in handling Chinese legal tasks [57].\nBase Model: Chinese Alpaca Plus 7B base model\nFine-tuned Model Parameters: Not Known\nFine-Tuning Techniques Used: LoRA with Alpaca template\nDatasets Used:\n\u2022 Open-source dataset: 200,000 examples containing crime type prediction and crime consultation\ntasks.\n\u2022 JEC-QA dataset: 20,000 examples containing legal question answering tasks.\n\u2022 Constructed legal dataset: 80,000 examples, refined from open-source and JEC-QA datasets using\nChatGPT.\nResults: LAWGPT demonstrates notable performance improvements over the LLaMA 7B model in\nvarious legal tasks, but still trails behind proprietary models like GPT-3.5 Turbo and GPT-4.\n\n36\n\n\fPharmaceutical Domain\nModel Description: PharmaGPT, a suite of domain-specific large language models tailored to the\nbiopharmaceutical and chemical industries, sets a new benchmark for precision in these fields [58].\nBase Model: LlaMA series\nFine-tuned Model Parameters: 13B and 70B\nFine-Tuning Techniques Used: Instruction fine-tuning and RLHF\nDatasets Used:\n\u2022 Specific-domain data from academic papers and clinical reports\n\u2022 Text data from NLP dataset formats (e.g., question answering, summarisation, dialogue)\n\u2022 Instruction fine-tuning dataset for multitask learning\n\u2022 RLHF dataset with human preference expert-annotated instructions\nResults: PharmaGPT models demonstrated impressive performance on various pharmaceutical benchmarks, consistently outperforming GPT-3.5 Turbo.\nFinance Domain\nModel Description: Palmyra-Fin-70B-32K, developed by Writer, is a leading large language model\nspecifically designed for the financial sector. [59]\nBase Model: LlaMA\nFine-tuned Model Parameters: 70B\nFine-Tuning Techniques Used: Not Known\nDatasets Used: Not Known\nResults: Palmyra-Fin-70B-32K exhibits state-of-the-art performance, achieving leading results across\nvarious financial datasets and excelling in financial document analysis, market trend prediction, and risk\nassessment.\n\n6.3\n\nParameter-Efficient Fine-Tuning (PEFT) Techniques\n\nParameter Efficient Fine Tuning (PEFT) is an impactful NLP technique that adeptly adapts pre-trained\nlanguage models to various applications with remarkable efficiency. PEFT methods fine-tune only a\nsmall subset of (additional) model parameters while keeping most of the pre-trained LLM parameters\nfrozen, thereby significantly reducing computational and storage costs. This approach mitigates the issue\nof catastrophic forgetting, a phenomenon where neural networks lose previously acquired knowledge and\nexperience a significant performance decline on previously learned tasks when trained on new datasets.\nPEFT methods have demonstrated superior performance compared to full fine-tuning, particularly in\nlow-data scenarios, and exhibit better generalisation to out-of-domain contexts. This technique is applicable to various modalities, such as financial sentiment classification and machine translation of medical\nterminologies. A taxonomy of PEFT-based fine-tuning approaches is provided in Figure6.1. We will\nfurther discuss a few key PEFT-based approaches in the following sections.\n\n6.3.1\n\nAdapters\n\nAdapter-based methods introduce additional trainable parameters after the attention and fully connected\nlayers of a frozen pre-trained model, aiming to reduce memory usage and accelerate training. The specific\napproach varies depending on the adapter; it might involve adding an extra layer or representing the\nweight updates delta (W) as a low-rank decomposition of the weight matrix. Regardless of the method,\nadapters are generally small yet achieve performance comparable to fully fine-tuned models, allowing for\nthe training of larger models with fewer resources.\nHuggingFace supports adapter configurations through the PEFT library. During fine-tuning, new adapters\nare integrated into the model using LoraConfig 1 . HuggingFace uses PeftConfig to load existing pretrained models and apply PEFT techniques. Additionally, HuggingFace provides built-in support to\n1 https://huggingface.co/docs/peft/en/package_reference/lora\n\n37\n\n\fFigure 6.1: Comprehensive Taxonomy of Parameter-Efficient Fine-Tuning (PEFT) Methods for Large\nLanguage Models (LLMs). This figure categorises various PEFT techniques, highlighting their distinct\napproaches, from additive and selective fine-tuning to reparameterised and hybrid methods. It details\nspecific strategies within each category, such as Adapter-Based Fine-Tuning, Soft Prompt-Based FineTuning, and their respective sub-techniques like LoRA and its derivatives, showcasing the diverse and\nevolving landscape of LLM fine-tuning. (adapted from [60])\nrun the fine-tuning process across any distributed configuration using Accelerate2 , making large-scale\ntraining and inference simple, efficient, and adaptable.\n\n6.3.2\n\nLow-Rank Adaptation (LoRA)\n\nLow-Rank Adaptation (LoRA)[62] is a technique designed for fine-tuning large language models, which\nmodifies the fine-tuning process by freezing the original model weights and applying changes to a separate\nset of weights, added to the original parameters. LoRA transforms the model parameters into a lowerrank dimension, reducing the number of trainable parameters, speeding up the process, and lowering\ncosts. This method is particularly useful in scenarios where multiple clients require fine-tuned models\nfor different applications, allowing for the creation of specific weights for each use case without the\nneed for separate models. By employing low-rank approximation methods, LoRA effectively reduces\ncomputational and resource requirements while preserving the pre-trained model\u2019s adaptability to specific\ntasks or domains.\nBenefits of Using LoRA\n1. Parameter Efficiency: LoRA significantly reduces the number of parameters that need to be\ntrained by focusing only on the low-rank matrices, resulting in lower memory and storage requirements compared to full fine-tuning.\n2. Efficient Storage: The storage of the trained model is more efficient as it only requires storing\nthe low-rank matrices instead of the full model weights.\n2 https://huggingface.co/docs/accelerate/en/index\n\n38\n\n\fFigure 6.2: Schematic representation of the Adapter Architecture used in LLMs. The diagram showcases\nthe integration of adapters within the Transformer architecture, including the feed-forward up and down\nlayers and their role in enabling efficient model adaptation by inserting additional parameters while\nmaintaining the model\u2019s core structure (adapted from [61])\n3. Reduced Computational Load: Training with low-rank matrices requires fewer computational\nresources, making it faster and more scalable.\n4. Lower Memory Footprint: Since fewer parameters are being updated, the memory footprint\nduring training is reduced, enabling the use of larger batch sizes or more complex models within\nthe same hardware constraints.\n5. Flexibility: LoRA can be easily integrated with existing pre-trained models without extensive\nmodifications to the model architecture.\n6. Compatibility: It can be used alongside other fine-tuning techniques, such as adapter layers or\nprompt-tuning, to further enhance performance.\n7. Comparable Results: Despite the reduction in the number of trainable parameters, LoRA has\nbeen shown to achieve performance comparable to full fine-tuning in many tasks.\n8. Task-Specific Adaptation: It effectively adapts the pre-trained model to specific tasks, leveraging the knowledge already embedded in the original model.\n9. Avoiding Overfitting: By focusing on low-rank updates, LoRA can help in mitigating overfitting,\nespecially when dealing with smaller task-specific datasets.\nLimitations\nWhile LoRA demonstrates considerable power, it also presents challenges:\n\u2022 Fine-tuning Scope: LoRA may face difficulties when applied to tasks demanding substantial\nalterations to the pre-trained model\u2019s internal representations.\n\u2022 Hyperparameter Optimisation: Tuning the rank parameter \u2018r\u2019 requires meticulous adjustment\nfor optimal performance.\n\u2022 Ongoing Research: Despite its promise, LoRA is still in active research stages, and its long-term\nimplications remain to be fully explored.\n\n39\n\n\fFigure 6.3: A comparison between weight updates in regular fine-tuning and LoRA fine-tuning. In\nregular fine-tuning, the entire weight update matrix (\u2206W ) is applied to the pre-trained weights. In\ncontrast, LoRA fine-tuning introduces two low-rank matrices (A and B) that approximate the weight\nupdate matrix (\u2206W ), significantly reducing the number of trainable parameters by leveraging the inner\ndimension (r), which is a hyperparameter. This method is more efficient in terms of memory and\ncomputation, making it ideal for fine-tuning large models. (adapted from [63])\nDespite these challenges, LoRA stands as a pioneering technique with vast potential to democratise access\nto the capabilities of LLMs. Continued research and development offer the prospect of overcoming current\nlimitations and unlocking even greater efficiency and adaptability.\nTutorial for Fine-Tuning LLM Using LoRA\nAn open-source template for fine-tuning LLMs using the LoRA method with the Hugging Face library\ncan be found here. This template is designed specifically for adapting LLMs for instruction fine-tuning\nprocesses.\n\n6.3.3\n\nQLoRA\n\nQLoRA[64] is an extended version of LoRA designed for greater memory efficiency in large language models (LLMs) by quantising weight parameters to 4-bit precision. Typically, LLM parameters are stored\nin a 32-bit format, but QLoRA compresses them to 4-bit, significantly reducing the memory footprint.\nThis allows fine-tuning on less powerful hardware, including consumer GPUs. QLoRA also quantises the\nweights of the LoRA adapters from 8-bit to 4-bit, further decreasing memory and storage requirements\n(see Figure 6.4). Despite the reduction in bit precision, QLoRA maintains performance levels comparable\nto traditional 16-bit fine-tuning.\nIt achieves this by backpropagating gradients through a frozen, 4-bit quantised pre-trained language\nmodel into Low-Rank Adapters, making the fine-tuning process efficient while preserving model effectiveness. The QLoRA configuration is supported by HuggingFace via the PEFT library, utilising LoraConfig\nand BitsAndBytesConfig for quantisation. Innovations such as an optimal 4-bit data type, double quantisation of constants, and memory spike management enable QLoRA to reduce memory usage from 96\nbits per parameter in traditional fine-tuning to 5.2 bits per parameter, an 18-fold reduction.\nPerformance-wise, QLoRA outperforms naive 4-bit quantisation and matches 16-bit quantised models\non benchmarks. Additionally, QLoRA enabled the fine-tuning of a high-quality 4-bit chatbot using a\nsingle GPU in 24 hours, achieving quality comparable to ChatGPT.\nThis tutorial explains the end-to-end steps of fine-tuning QLoRA on a custom dataset for the Phi-2\nmodel.\n40\n\n\fFigure 6.4: Quantised Low-Rank Adaptation (QLoRA) Optimisation Workflow. This figure illustrates\nthe QLoRA optimisation process, showing how the optimisation states, adapters, and the model interact\nduring fine-tuning. It demonstrates the use of different bit-widths (32-bit, 16-bit, and 4-bit) to optimise\nthe memory and computational efficiency during the fine-tuning of large language models (adapted from\n[65]).\n\n6.3.4\n\nWeight-Decomposed Low-Rank Adaptation (DoRA)\n\nIn the context of optimising model fine-tuning, the pattern analysis of LoRA and Full Fine-Tuning\n(FT) reveals significant differences in learning behaviours and updates. LoRA, employing a strategy of\nincrementally updating pre-trained weights using the product of two low-rank matrices, maintains the\noriginal weights largely static during the fine-tuning process, which allows for efficient inference. Despite\nits computational efficiency, previous studies have suggested that LoRA\u2019s limited number of trainable\nparameters might contribute to its performance discrepancies when compared to FT.\nWeight-Decomposed Low-Rank Adaptation (DoRA) [66] is a novel fine-tuning methodology designed to\noptimise pre-trained models by decomposing their weights into magnitude and directional components.\nThis approach leverages the efficiency of Low-Rank Adaptation (LoRA) for directional updates, facilitating substantial parameter updates without altering the entire model architecture. DoRA addresses\nthe computational challenges associated with traditional full fine-tuning (FT) by maintaining model\nsimplicity and inference efficiency, while simultaneously bridging the performance gap typically observed\nbetween LoRA and FT. Empirical and theoretical evaluations demonstrate that DoRA not only achieves\nlearning outcomes comparable to FT across diverse tasks\u2014including natural language processing and\nvision-language applications\u2014but also consistently surpasses LoRA in performance, providing a robust\nsolution for enhancing the adaptability and efficiency of large-scale models.\nPython Library - DoRA is facilitated via the HuggingFace LoraConfig package. To incorporate DoRA\ninto the fine-tuning process, it is essential to specify the \u2019use dora = True\u2019 parameter during the Lora\nconfiguration. Further information on initialisation can be found here.\nBenefits of DoRA\n1. Enhanced Learning Capacity: DoRA achieves a learning capacity closely resembling full finetuning (FT) by decomposing pre-trained weights into magnitude and directional components, allowing for more nuanced updates.\n2. Efficient Fine-Tuning: By utilising the structural advantages of Low-Rank Adaptation (LoRA)\nfor directional updates, DoRA enables efficient fine-tuning without altering the entire model architecture.\n3. No Additional Inference Latency: Despite its improved learning capabilities, DoRA does not\nintroduce any additional inference latency over LoRA, maintaining model simplicity and efficiency.\n4. Superior Performance: Experimental results demonstrate that DoRA consistently outperforms\nLoRA across a wide range of tasks, including natural language processing (NLP), visual instruction\ntuning, and image/video-text understanding. For example, it shows significant improvements in\ncommonsense reasoning and visual instruction tuning benchmarks.\n5. Versatility Across Backbones: DoRA has been validated across various model backbones,\nincluding large language models (LLM) and vision-language models (LVLM), indicating its broad\n41\n\n\fFigure 6.5: An overview of DoRA (Decomposed Representations for Adaptation), which is a method for\nweight decomposed low-rank adaptation. The figure illustrates how pre-trained weights are decomposed\nand adapted for fine-tuning. In the left section, pre-trained weights are decomposed into a magnitude and\ndirection. The right section shows how these decomposed weights are merged with trainable parameters\nduring fine-tuning, resulting in updated weights that combine both frozen (blue) and trainable (green)\ncomponents. The process emphasises efficient adaptation by focusing on the most significant directions\nin the parameter space, facilitating effective fine-tuning while maintaining the integrity of the original\nmodel (adapted from [66]).\napplicability and robustness in different domains.\n6. Innovative Analysis: The introduction of a novel weight decomposition analysis helps uncover\nfundamental differences in the learning patterns of FT and various parameter-efficient fine-tuning\n(PEFT) methods, contributing to a deeper understanding of model fine-tuning dynamics.\nComparison between LoRA and DoRA\nLow-Rank Adaptation (LoRA) and Weight-Decomposed Low-Rank Adaptation (DoRA) are both advanced techniques designed to improve the efficiency and effectiveness of fine-tuning large pre-trained\nmodels. While they share the common goal of reducing computational overhead, they employ different\nstrategies to achieve this (see Table6.2).\n\n42\n\n\fCriteria\nObjective\n\nApproach\n\nModel Architecture\n\nLoRA (Low-Rank Adaptation)\nProvide an efficient method for\nfine-tuning pre-trained models by\nusing low-rank matrix products\nto update weights incrementally\nwithout increasing inference latency.\nImplements a low-rank decomposition where the weight update is\nmodelled as the product of two\nlow-rank matrices (B and A), keeping the original weights static.\nKeeps the pre-trained weight matrix (W0) unchanged and applies\nupdates using low-rank matrices\n(B and A). Matrix A is initialised\nwith a uniform Kaiming distribution, while B is set to zero initially.\n\nDoRA (Weight-Decomposed\nLow-Rank Adaptation)\nImproves learning capacity by\nclosely mimicking the learning patterns of full fine-tuning, optimising magnitude and direction separately.\nUses weight decomposition analysis to reparameterise the weight\nmatrix into separate magnitude\nand direction components for distinct updates.\nRestructures the weight matrix\ninto magnitude and directional\ncomponents, ensuring directional\nvectors are unit vectors for more\ndetailed adjustments.\n\nTable 6.2: A detailed comparison between LoRA (Low-Rank Adaptation) and DoRA (WeightDecomposed Low-Rank Adaptation), highlighting their objectives, approaches, and the specific architectural strategies they employ for fine-tuning large language models.\nTutorial for Fine-Tuning LLM using DoRA\nThis tutorial offers an in-depth guide and detailed explanation of the steps involved in implementing\nDoRA from scratch, as well as insights into the fine-tuning process essential for optimising performance.\n\n6.3.5\n\nFine-Tuning with Multiple Adapters\n\nDuring fine-tuning, we have explored the method of freezing the parameters of the LLM and focusing\nsolely on fine-tuning a few million trainable parameters using LoRA. For example, fine-tuning an LLM\nfor translation involves training a translation adapter with relevant data. This approach allows us to\nfine-tune separate adapters for each specific task we want the LLM to perform. However, a key question\narises: can we consolidate multiple adapters into a unified multi-task adapter? For instance, if we have\nseparate adapters for translation and summarisation tasks, can we merge them so that the LLM can\nproficiently handle both tasks? (Illustrated via Figure6.6).\nThe PEFT library simplifies the process of merging adapters with its add weighted adapter function 3 ,\nwhich offers three distinct methods:\n1. Concatenation: This straightforward method concatenates the parameters of the adapters. For\ninstance, if two adapters each have a rank of 16, the resulting adapter will have a rank of 32. This\nmethod is highly efficient.\n2. Linear Combination: Although less documented, this method appears to perform a weighted\nsum of the adapters\u2019 parameters.\n3. SVD: The default method employs singular value decomposition through torch.linalg.svd. While\nversatile, it is notably slower than the other methods, particularly for adapters with high ranks\n(greater than 100), which can take several hours.\nEach method allows for customising the combination by adjusting weights. For instance, when merging\ntwo adapters, X and Y, assigning more weight to X ensures that the resulting adapter prioritises behaviour\nsimilar to X over Y.\nThis approach is particularly suited for consolidating a single LLM to handle multiple tasks rather than\ncreating separate models for each task domain. By adopting this method, there is no longer a need to\n3 https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraModel.add_weighted_adapter\n\n43\n\n\findividually fine-tune a model for each task. Instead, a single adapter layer can be fine-tuned for each\ntask, allowing queries to yield the desired responses efficiently.\n\nFigure 6.6: Overview of how multiple adapters can be used with a pre-trained LLM to fine-tune it for\nvarious specific tasks, such as summarisation, proofreading, sentiment analysis, and more. (adapted from\n[67])\n\nSteps for Fine-Tuning LLM with LoRA for Multiple Tasks and Adapters\n1. Adapter Creation: Create multiple adapters, each fine-tuned for specific tasks using different\nprompt formats or task-identifying tags (e.g., [translate fren], [chat]).\n2. LoRA Integration: Implement LoRA to efficiently integrate these adapters into the pre-trained\nLLM. Utilise LoRA\u2019s methods such as concatenation, linear combination, or singular value decomposition (SVD) to combine adapters while minimising computational overhead and maintaining\nperformance.\n3. Task-Specific Adaptation: Fine-tune each adapter with task-specific data to enhance performance for individual tasks. Ensure adapters are trained with data relevant to their respective\ntasks, optimising their ability to generate accurate responses.\n4. Behaviour Adjustment: Monitor the behaviour of combined adapters to identify any undesired\ninherited behaviours from individual adapters (e.g., short response generation from a translation\n\n44\n\n\fadapter). Adjust the combination weights or types to modify adapter behaviour as needed, ensuring\neach adapter performs optimally for its intended task.\n5. Evaluation and Iteration: Evaluate the performance of the combined model across multiple\ntasks using validation datasets. Iterate on the fine-tuning process, making adjustments to adapter\ncombinations and training parameters based on performance metrics and user feedback.\nTherefore, for optimal performance, it is advisable to combine adapters that have been fine-tuned with\ndistinctly varied prompt formats. However, even when using adapters with different prompt formats, the\nresulting adapter may not exhibit desired behaviour. For example, a newly combined adapter designed for\nchatting may only generate short responses, inheriting this tendency from an adapter that was originally\ntrained to halt after producing a single sentence. To adjust the behaviour of the combined adapter,\none can prioritise the influence of a specific adapter during the combination process and/or modify the\nmethod of combination used.\nAn illustrative tutorial demonstrating the fine-tuning of large language models (LLMs) using multiple\nadapter layers for various tasks can be found here.\n\n6.4\n\nHalf Fine Tuning\n\nHalf Fine-Tuning (HFT)[68] is a technique designed to balance the retention of foundational knowledge\nwith the acquisition of new skills in large language models (LLMs). HFT involves freezing half of the\nmodel\u2019s parameters during each fine-tuning round while updating the other half, allowing the model to\nretain pre-trained knowledge and enhance new task performance without altering the model architecture.\nEach repetitive transformer layer is divided into three blocks: self-attention, feed-forward, and layernorm,\nwith half of the parameters in each block updated and the other half frozen, varying with each round.\nThis strategic parameter update helps maintain knowledge parity across training rounds and enhances\nscalability in successive training sessions.\nResearch on models like LLAMA 2-7B demonstrated that HFT could significantly restore forgotten basic\nknowledge while preserving high general ability performance. This method\u2019s robustness and efficiency\nmake it applicable to various fine-tuning scenarios, including supervised fine-tuning, direct preference\noptimisation, and continual learning. Additionally, HFT\u2019s ability to maintain the model architecture\nsimplifies its implementation and ensures compatibility with existing systems, further promoting its\npractical adoption.\n\n6.4.1\n\nBenefits of using Half Fine tuning\n\n1. Recovery of Pre-Trained Knowledge: By rolling back half of the fine-tuned parameters to their\npre-trained state, HFT effectively recovers a portion of the original knowledge, thereby mitigating\ncatastrophic forgetting of previously acquired capabilities.\n2. Enhanced Performance: Research experiments shows that HFT maintains or even surpasses\nthe performance of full fine-tuning (FFT) on downstream tasks, demonstrating its effectiveness in\nbalancing knowledge retention with task-specific learning.\n3. Robustness: The method is robust to different selection strategies and the number of parameters\nchosen for updating, ensuring consistent performance across various configurations.\n4. Simplicity and Scalability: HFT does not alter the model architecture, which simplifies implementation and allows for scalable applications, particularly beneficial in successive fine-tuning\nscenarios.\n5. Versatility: The technique has proven effective across diverse fine-tuning scenarios, including\nsupervised fine-tuning, direct preference optimisation, and continual learning.\n\n45\n\n\fFigure 6.7: Schematic illustration of the Half Fine-Tuning (HFT) method as applied to LLAMA 2\u2019s\narchitecture. The diagram shows multiple stages of fine-tuning, where specific model parameters are\nselectively activated (orange) while others remain frozen (blue). This approach optimises training by\nreducing computational requirements while still effectively adapting the model to new tasks or data.\n(adapted from [68])\n\n6.4.2\n\nComparison between HFT and LoRA\n\nCriteria\nObjective\n\nApproach\n\nModel Architecture\n\nPerformance\n\nHFT\nThe goal is to retain the foundational knowledge acquired during pre-training while learning new\ntask-specific skills, thus balancing\nbetween maintaining existing capabilities and acquiring new ones.\nHFT involves freezing half of the\nmodel\u2019s parameters during each\nfine-tuning round and updating\nonly the other half.\n\nHFT does not alter the model\u2019s architecture or introduce new parameters, making it straightforward\nto apply without additional structural changes.\nResearch has shown that HFT\ncan restore forgotten basic knowledge while maintaining high performance in general abilities.\n\nLoRA\nLoRA aims to reduce computational and memory requirements\nduring fine-tuning, making it more\nefficient and feasible to train large\nmodels on limited hardware resources.\nLoRA reduces the number of trainable parameters by introducing\nlow-rank decomposition into the\nweight matrices of the neural network. This involves injecting lowrank matrices into the model\u2019s layers during fine-tuning.\nLoRA modifies the model by\nadding low-rank matrices, which\nchanges the training dynamics and\nrequires additional computations\nfor the low-rank updates.\nLoRA is designed to achieve competitive performance with full finetuning but with significantly fewer\ntrainable parameters and lower\ncomputational costs.\n\nTable 6.3: Comparative Analysis of Half Fine-Tuning (HFT) and Low-Rank Adaptation (LoRA).\n\n46\n\n\f6.5\n\nLamini Memory Tuning\n\nLamini [69] was introduced as a specialised approach to fine-tuning Large Language Models (LLMs),\ntargeting the reduction of hallucinations. This development was motivated by the need to enhance the\nreliability and precision of LLMs in domains requiring accurate information retrieval. Traditional training\nmethods typically consist of running stochastic gradient descent on vast datasets, which, despite fitting\nthe training data well, often produce models that fail to generalise effectively and are prone to such errors.\nFoundation models often follow a training regimen similar to the Chinchilla recipe, which prescribes\ntraining for a single epoch on a massive corpus, such as training Llama 2 7B on about one trillion\ntokens. This approach results in substantial loss and is geared more towards enhancing generalisation\nand creativity where a degree of randomness in token selection is permissible. However, it falls short for\ntasks demanding high factual precision. In contrast, Lamini Memory Tuning delves deeper by analysing\nthe loss of individual facts, significantly improving the accuracy of factual recall. By augmenting a\nmodel with additional parameters specifically for memory (e.g., an 8B parameter model with an extra 2B\nparameters for weights), Lamini enables the model to memorise and accurately recall a significant number\nof facts, closely aligning performance with LLM scaling laws without compromising on generalisation.\n\n6.5.1\n\nLamini-1 - A model architecture based on Lamini\n\nDeparting from traditional transformer-based designs, the Lamini-1 model architecture (Figure 6.8) employs a massive mixture of memory experts (MoME). This system features a pre-trained transformer\nbackbone augmented by adapters that are dynamically selected from an index using cross-attention\nmechanisms. These adapters function similarly to experts in MoE architectures, and the network is\ntrained end-to-end while freezing the backbone. This setup allows for specific facts to be stored exactly\nin the selected experts.\n\nFigure 6.8: Diagram of the Lamini-1 Model Architecture, featuring a Massive Array of Memory Experts\n(MoME). This architecture integrates a pre-trained transformer backbone with dynamically selected\nadapters via cross-attention mechanisms. Each adapter, functioning as a memory expert, is capable of\nstoring specific factual data. (adopted from [69])\nAt inference time, only the relevant experts are retrieved from the index, enabling the LLM to store a\nlarge number of facts while maintaining low inference latency. Specialised GPU kernels written in Triton\nare used to accelerate the lookup of experts, optimising the system for quick access to stored knowledge.\nSystems Optimisations for Banishing Hallucinations\nThe MoME architecture is designed to minimise the computational demand required to memorise facts.\nDuring training, a subset of experts, such as 32 out of a million, is selected for each fact. The weights of\nthe backbone network and the cross attention used to select the expert are frozen, and gradient descent\nsteps are taken until the loss is sufficiently reduced to memorise the fact. This approach prevents the\nsame expert from being selected multiple times for different facts by first training the cross attention\n\n47\n\n\fselection mechanism during a generalisation training phase, then freezing its weights.\nThis method ensures that computation scales with the number of training examples, not the total\nnumber of parameters, thereby significantly reducing the computation required for memory tuning.\nThis optimised approach allows Lamini-1 to achieve near-zero loss in memory tuning on real and random\nanswers efficiently, demonstrating its efficacy in eliminating hallucinations while improving factual recall.\n\n6.6\n\nMixture of Experts\n\nA mixture of experts (MoE) is an architectural design for neural networks that divides the computation\nof a layer or operation (e.g., linear layers, MLPs, or attention projection) into several specialised subnetworks, referred to as \u201dexperts\u201d. Each expert independently carries out its computation, and the results\nare aggregated to produce the final output of the MoE layer. MoE architectures can be categorised as\neither dense, where every expert is engaged for each input, or sparse, where only a subset of experts is\nutilised for each input.\n\n6.6.1\n\nMixtral 8x7B Architecture and Performance\n\nMixtral [70] 8x7B employs a Sparse Mixture of Experts (SMoE) architecture (Figure 6.9), mirroring the\nstructure of Mistral 7B but incorporating eight feedforward blocks (experts) in each layer. For every\ntoken at each layer, a router network selects two experts to process the current state and combine their\noutputs. Although each token interacts with only two experts at a time, the selected experts can vary at\neach timestep. Consequently, each token has access to 47 billion parameters but utilises only 13 billion\nactive parameters during inference. Mixtral 8x7B not only matches but often surpasses Llama 2 70B\nand GPT-3.5 across all evaluated benchmarks. Its performance is notably superior to Llama 2 70B in\nmathematics, code generation, and multilingual tasks.\n\nFigure 6.9: Diagram of the Mixtral 8x7B Mixture of Experts (MoE) model architecture. The model is\ncomposed of a router network that dynamically selects the most relevant experts from a pool of eight\ntransformer-based experts, each with 7 billion parameters. The experts are organised into transformer\nblocks, where the router directs data to the appropriate expert based on the input, optimising computational efficiency and model performance. This architecture allows for scalability and specialised\nprocessing within large language models. (adapted from [71])\n\n48\n\n\f6.7\n\nMixture of Agents\n\nDespite the numerous LLMs and their notable accomplishments, they continue to encounter fundamental\nlimitations regarding model size and training data. Scaling these models further is prohibitively expensive, often necessitating extensive retraining on multiple trillion tokens. Simultaneously, different LLMs\nexhibit distinct strengths and specialise in various aspects of tasks. A recent study has investigated\nleveraging the collective expertise of multiple LLMs to develop a more capable and robust model, a\nmethod known as Mixture of Agents (MoA) [72].\nMoA functions using a layered architecture, where each layer comprises multiple LLM agents (Figure\n6.10). This structure reveals a phenomenon known as the \u201ccollaborativeness of LLMs.\u201d The innovative MoA framework utilises the combined capabilities of several LLMs to enhance both reasoning and\nlanguage generation proficiency. Research indicates that LLMs naturally collaborate, demonstrating improved response quality when incorporating outputs from other models, even if those outputs are not\nideal.\n\nFigure 6.10: Illustration for Mixture of Agents (MoA) LLM configuration. The model consists of multiple\nlayers, each incorporating several agents that process the input independently before concatenating their\noutputs to form an intermediate result. The process continues across layers, refining the output at each\nstage to generate the final output based on the given prompt (adapted from [72]).\n\n6.7.1\n\nMethodology\n\nTo enhance collaboration among multiple LLMs, it is essential to understand their individual strengths\nand classify them accordingly. The classification includes:\n1. Proposers: These models excel at generating valuable reference responses for other models. While\nthey may not perform exceptionally on their own, they provide useful context and varied perspectives that improve the final output when utilised by an aggregator.\n49\n\n\f2. Aggregators: These models are adept at merging responses from various models into a single\nhigh-quality result. An effective aggregator should maintain or even enhance the quality of the\nfinal response, regardless of the quality of the individual inputs.\nThe careful selection of LLMs for each MoA layer is crucial Performance metrics, such as average win\nrates in a given layer, help assess the suitability of models for subsequent layers, ensuring the production\nof higher-quality outputs. Diversity in model outputs is vital, as varied responses from different models\ncontribute significantly more than homogeneous outputs from a single model. In MoA, given an input\nprompt, the output of the ith MoA layer yi is calculated as follows:\nyi =\n\nn\nM\n\n[Ai,j (xi )] + x1 , xi+1 = yi\n\n(6.1)\n\nj=1\n\n6.7.2\n\nAnalogy with MoE\n\nMixture-of-Experts (MoE) is a well-established machine learning technique where multiple expert networks, each with specialised skills, collaborate to address complex problems. This approach has demonstrated significant success across various applications and serves as the inspiration for the Mixture-ofAgents (MoA) method. In a typical MoE design, a stack of layers, known as MoE layers, consists of\nmultiple expert networks, a gating network, and residual connections to improve gradient flow. The\noutput for layer yi is calculated as follows:\nyi =\n\nn\nX\n\nGi,j (xi )Ei,j (xi ) + xi\n\n(6.2)\n\nj=1\n\nThe MoA framework advances the MoE concept by operating at the model level through prompt-based\ninteractions rather than altering internal activations or weights. Instead of relying on specialised subnetworks within a single model, MoA utilises multiple full-fledged LLMs across different layers. In this\napproach, the gating and expert networks\u2019 functions are integrated within an LLM, leveraging its ability\nto interpret prompts and generate coherent outputs without additional coordination mechanisms.\n\n6.7.3\n\nWhat makes MoA works well?\n\n1. MoA\u2019s Superior Performance: MoA significantly outperforms LLM-based rankers, which select\none answer from the proposals rather than generating new responses. This suggests that MoA\u2019s\napproach of aggregating all generated responses provides more effective results than simply choosing\nfrom pre-existing options.\n2. Effective Incorporation of Proposals: The aggregator in MoA demonstrates a tendency to\nintegrate the best proposed answers. This is supported by positive correlations between aggregator\nresponses and various similarity metrics, such as BLEU scores, which measure n-gram overlaps. The\nuse of alternative similarity measures also shows a consistent positive correlation with preference\nscores, indicating that the aggregator effectively utilises the proposed responses.\n3. Influence of Model Diversity and Proposer Count: Increasing the number of proposers\nimproves output quality, highlighting the benefits of additional auxiliary information. Additionally,\nusing a diverse set of LLMs as proposers consistently yields better results compared to using a single\nLLM. This suggests that both the number and diversity of LLM agents in each MoA layer contribute\nto enhanced performance, with potential for further improvement through scaling.\n4. Model Specialisation: Analysis of model roles within the MoA ecosystem reveals that GPT-4o,\nQwen, and LLaMA-3 are effective in both assisting and aggregating tasks. In contrast, WizardLM\nexcels as a proposer but struggles with aggregating responses from other models.\n\n6.8\n\nProximal Policy Optimisation (PPO)\n\nPPO [73] is a widely recognised reinforcement learning algorithm used for training agents to perform tasks\nin diverse environments. This algorithm leverages policy gradient methods, where policies\u2014represented\n\n50\n\n\fby neural networks\u2014determine the actions taken by the agent based on the current state. PPO effectively handles the dynamic nature of training data generated through continuous agent-environment\ninteractions, a feature that differentiates it from static datasets used in supervised learning.\nThe innovation of PPO lies in its \u201dsurrogate\u201d objective function, optimised via stochastic gradient ascent.\nThis approach allows for multiple updates from the same batch of data, enhancing both training efficiency\nand stability over traditional policy gradient methods. Developed by OpenAI, PPO was designed to\nbalance ease of implementation with the robust performance characteristics of more complex algorithms\nlike Trust Region Policy Optimisation (TRPO), but without the associated computational complexity.\nPPO operates by maximising expected cumulative rewards through iterative policy adjustments that\nincrease the likelihood of actions leading to higher rewards. A key feature of PPO is its use of a clipping\nmechanism in the objective function, which limits the extent of policy updates, thus preventing drastic\nchanges and maintaining stability during training.\n\nFigure 6.11: Schematic of Proximal Policy Optimisation (PPO) applied in the context of Reinforcement\nLearning from Human Feedback (RLHF) for fine-tuning a Large Language Model (LLM). The process\ninvolves using a prompt dataset to train the LLM. The PPO algorithm adjusts the LLM\u2019s policy based\non rewards provided by the reward model, which is fine-tuned through human feedback. (adapted from\n[73])\nPython Library - HuggingFace Transformer Reinforcement Learning (TRL4 ) package supports the\nPPO Trainer5 for training language models from the preference data.\nThe PPOTrainer expects to align a generated response with a query given the rewards obtained from the\nReward model. During each step of the PPO algorithm we sample a batch of prompts from the dataset,\nwe then use these prompts to generate the a responses from the SFT model. Next, the Reward model\nis used to compute the rewards for the generated response. Finally, these rewards are used to optimise\nthe SFT model using the PPO algorithm. Therefore the dataset should contain a text column which we\ncan rename to query. Each of the other data-points required to optimise the SFT model are obtained\nduring the training loop.\n\n6.8.1\n\nBenefits of PPO\n\n1. Stability: Proximal Policy Optimisation (PPO) is designed to ensure stable and reliable policy\nupdates. The clipped surrogate objective function is central to this stability, as it limits policy\nupdates to prevent large, potentially destabilising changes. This results in smoother and more\nconsistent learning.\n2. Ease of Implementation: Compared to advanced algorithms TRPO, PPO is relatively straightforward to implement. It avoids the need for second-order optimisation techniques, making it more\n4 https://huggingface.co/docs/trl/en/index\n5 https://huggingface.co/docs/trl/main/en/ppo_trainer\n\n51\n\n\faccessible to less experienced practitioners.\n3. Sample Efficiency: PPO achieves data efficiency through its use of the clipped surrogate objective. This mechanism regulates policy updates, ensuring stability while effectively reusing training\ndata. Consequently, PPO tends to be more sample-efficient than other reinforcement learning\nalgorithms, performing well with fewer samples, which is advantageous in scenarios where data\ncollection is costly or time-consuming.\n\n6.8.2\n\nLimitations of PPO\n\n1. Complexity and Computational Cost: Proximal Policy Optimisation (PPO) involves intricate\npolicy and value networks, necessitating substantial computational resources for training. This\ncomplexity often results in extended training durations and increased operational expenses.\n2. Hyperparameter Sensitivity: PPO\u2019s performance is highly dependent on several hyperparameters, such as the clipping range, learning rate, and discount factor. Achieving optimal performance\nrequires meticulous tuning of these parameters. Incorrect settings can lead to suboptimal policy\noutcomes or instability during the learning process.\n3. Stability and Convergence Issues: Although PPO is designed to enhance stability compared\nto earlier methods, it can still encounter convergence issues, particularly in highly dynamic or\ncomplex environments. Maintaining stable policy updates remains a significant challenge.\n4. Reward Signal Dependence: PPO\u2019s effectiveness is heavily reliant on a well-defined reward\nsignal to guide the learning process. In scenarios where designing an appropriate reward function\nis challenging or impractical, PPO may struggle to attain the desired results.\n\n6.8.3\n\nTutorial for training models using PPO technique\n\nThe tutorial for tuning GPT2 to generate positive movie reviews based on the IMDB dataset using PPO\ntechnique can be found here.\n\n6.9\n\nDirect Preference Optimisation (DPO)\n\nDirect Preference Optimisation (DPO) [74] offers a streamlined approach to aligning language models\n(LMs) with human preferences, bypassing the complexity of reinforcement learning from human feedback\n(RLHF). Large-scale unsupervised LMs typically lack precise behavioural control, necessitating methods like RLHF that fine-tune models using human feedback. However, RLHF is intricate, involving the\ncreation of reward models and the fine-tuning of LMs to maximise estimated rewards, which can be\nunstable and computationally demanding. DPO addresses these challenges by directly optimising LMs\nwith a simple classification objective that aligns responses with human preferences. This approach eliminates the need for explicit reward modelling and extensive hyperparameter tuning, enhancing stability\nand efficiency. DPO optimises the desired behaviours by increasing the relative likelihood of preferred\nresponses while incorporating dynamic importance weights to prevent model degeneration. Thus, DPO\nsimplifies the preference learning pipeline, making it an effective method for training LMs to adhere to\nhuman preferences.\nPython Library - HuggingFace TRL package supports the DPO Trainer6 for training language models\nfrom the preference data. The DPO training process requires a dataset formatted in a very specific\nmanner. If you are utilising the default DPODataCollatorWithPadding data collator, your final dataset\nobject must include three specific entries, which should be labelled as follows:\n\u2022 Prompt\n\u2022 Chosen\n\u2022 Rejected\nHuggingFace offers datasets compatible with DPO and can be accessed here.\n6 https://huggingface.co/docs/trl/main/en/dpo_trainer\n\n52\n\n\fFigure 6.12: Direct Preference Optimisation (DPO) Process Flow. This figure illustrates the Direct\nPreference Optimisation (DPO) technique used in fine-tuning large language models. The process begins\nwith preference data (Yw > Yl ), where Yw represents preferred outputs, and Yl represents less preferred\noutputs. Through a maximum likelihood estimation process, this preference data is used to optimise\nthe model\u2019s parameters, resulting in the final large language model (LLM). The method is designed to\nimprove the alignment of model outputs with desired user preferences, enhancing the model\u2019s effectiveness\nin specific tasks. (adapted from [74])\n\n6.9.1\n\nBenefits of DPO\n\n1. Direct Alignment with Human Preferences: DPO directly optimises models to generate\nresponses that align with human preferences, thereby producing more favourable outputs.\n2. Minimised Dependence on Proxy Objectives: In contrast to methods that rely on nextword prediction, DPO leverages explicit human preferences, resulting in responses that are more\nreflective of human behaviour.\n3. Enhanced Performance on Subjective Tasks: For tasks requiring subjective judgement, such\nas dialogue generation or creative writing, DPO excels in aligning the model with human preferences.\n\n6.9.2\n\nBest Practices for DPO\n\n1. High-Quality Preference Data: The performance of the model is heavily influenced by the\nquality of preference data. Ensure the dataset includes clear and consistent human preferences.\n2. Optimal Beta Value: Experiment with various beta values to manage the influence of the\nreference model. Higher beta values prioritise the reference model\u2019s preferences more strongly.\n3. Hyperparameter Tuning: optimise hyperparameters such as learning rate, batch size, and LoRA\nconfiguration to determine the best settings for your dataset and task.\n4. Evaluation on Target Tasks: Continuously assess the model\u2019s performance on the target task\nusing appropriate metrics to monitor progress and ensure the achievement of desired results.\n5. Ethical Considerations: Pay attention to potential biases in the preference data and take steps\nto mitigate them, preventing the model from adopting and amplifying these biases.\n\n6.9.3\n\nTutorial for training models using DPO technique\n\nThe tutorial for DPO training, including the full source code of the training scripts for SFT and DPO,\nis available here.\n\n6.9.4\n\nIs DPO Superior to PPO for LLM Alignment?\n\nThe recent study on DPO superior to PPO for LLM Alignment[75] investigates the efficacy of rewardbased and reward-free methods within RLHF. Reward-based methods, such as those developed by OpenAI, utilise a reward model constructed from preference data and apply actor-critic algorithms like\nProximal Policy Optimisation (PPO) to optimise the reward signal. Conversely, reward-free methods,\nincluding Direct Preference Optimisation (DPO), RRHF, and PRO, forego an explicit reward function,\n53\n\n\fwith DPO focusing exclusively on policy optimisation through a logarithmic representation of the reward\nfunction.\nOne of the objectives of this study is to determine whether DPO is genuinely superior to PPO in the\nRLHF domain. The study combines theoretical and empirical analyses to uncover the inherent limitations of DPO and identify critical factors that enhance PPO\u2019s practical performance in RLHF.\nTheoretical findings suggest that DPO may yield biased solutions by exploiting out-of-distribution responses. Empirical results indicate that DPO\u2019s performance is notably affected by shifts in the distribution between model outputs and the preference dataset. Furthermore, the study highlights that while\niterative DPO may offer improvements over static data training, it still fails to enhance performance\nin challenging tasks such as code generation. Ablation studies on PPO reveal essential components for\noptimal performance, including advantage normalisation, large batch sizes, and exponential moving average updates for the reference model\u2019s parameters. These findings form the basis of practical tuning\nguidelines, demonstrating PPO\u2019s robust effectiveness across diverse tasks and its ability to achieve stateof-the-art results in challenging code competition tasks. Specifically, on the CodeContest dataset, the\nPPO model with 34 billion parameters surpasses AlphaCode-41B, showing a significant improvement in\nperformance metrics.\n\n6.10\n\nOdds-Ratio Preference Optimization (ORPO)\n\nOdds-Ratio Preference Optimization (ORPO) is a novel approach designed to align the output of language models with desired responses by introducing a penalisation mechanism for undesirable outputs.\nUnlike traditional supervised fine-tuning (SFT) approaches, which focus solely on maximising the likelihood of correct responses, ORPO adds a specific odds-ratio based loss to penalise unwanted generations.\nThis technique provides a refined method for improving preference alignment without relying on a reference model, making it efficient for large-scale implementations.\nGiven an input sequence x, the log-likelihood of generating an output sequence y of length m is\ncomputed as:\nm\n\nlog P\u03b8 (y|x) =\n\n1 X\nlog P\u03b8 (yi |x)\nm i=1\n\nThe odds of generating the output sequence y given input x is expressed as:\nodds\u03b8 (y|x) =\n\nP\u03b8 (y|x)\n1 \u2212 P\u03b8 (y|x)\n\nORPO introduces an odds-ratio that contrasts the likelihood of generating a preferred (chosen) response yw with a less preferred (rejected) response yl , defined as:\nodds\u03b8 (yw |x)\nodds\u03b8 (yl |x)\n\nOR\u03b8 (yw , yl |x) =\n\nThe ORPO loss function incorporates two components:\n\u2022 Supervised Fine-tuning Loss (SFT):\nM\n\nLSF T = \u2212\n\n|V |\n\n1 XX k\nyi log pki\nM\ni=1\nk=1\n\nwhere yik is a binary indicator for the i-th token in the vocabulary, and pki is its predicted probability.\n\u2022 Odds-Ratio Loss:\n\n\u0012\n\nodds\u03b8 (yw |x)\nLOR = \u2212 log \u03c3 log\nodds\u03b8 (yl |x)\n\n\u0013\n\nwhere \u03c3 is the sigmoid function applied to stabilise the log odds ratio.\n\n54\n\n\fThus, the total ORPO objective is:\nLORP O = LSF T + \u03bbLOR\nwhere \u03bb controls the strength of preference alignment. This loss function effectively guides the\nmodel towards generating the chosen response while discouraging the rejected one, facilitating efficient\nalignment without the need for additional reference models [76].\nAdvantages of ORPO: ORPO\u2019s strength lies in its ability to perform preference alignment in a\nmonolithic manner, bypassing the need for separate phases of fine-tuning and preference optimisation.\nThis reduces computational overhead and provides state-of-the-art performance across various models,\nincluding LLaMA and Mistral, when evaluated on benchmark tasks such as AlpacaEval and MT-Bench\n[77].\n\n6.11\n\nPruning LLMs\n\nPruning LLMs involves eliminating unnecessary or redundant components from a neural network to\nreduce its size and complexity, thereby enhancing its efficiency and performance. This process assists AI\ndevelopers and engineers in addressing the challenges associated with deploying AI models in resourcelimited environments, such as mobile devices, edge computing, or embedded systems. Pruning AI models\ncan be achieved through various techniques, each suited to the type and structure of the neural network,\nthe pruning objective, and the pruning criterion. The following are common approaches:\n1. Weight Pruning: Involves removing weights or connections with minimal magnitude or impact on\nthe output. This method reduces the number of parameters and operations in the model, although\nit may not necessarily decrease memory footprint or latency.\n2. Unit Pruning: Eliminates entire units or neurons with the lowest activation or contribution to\nthe output. This technique can reduce the model\u2019s memory footprint and latency but may require\nretraining or fine-tuning to maintain performance.\n3. Filter Pruning: Involves removing entire filters or channels in convolutional neural networks that\nhave the least importance or relevance to the output. This strategy also reduces memory footprint\nand latency, though it may necessitate retraining or fine-tuning to preserve performance [78].\n\n6.11.1\n\nWhen to Prune AI Models?\n\nPruning AI models can be conducted at various stages of the model development and deployment cycle,\ncontingent on the chosen technique and objective.\n1. Pre-Training Pruning: Leverages prior knowledge or heuristics to determine the optimal network\nstructure before training begins. This approach can save time and resources during training but\nmay necessitate careful design and experimentation to identify the best configuration.\n2. Post-Training Pruning: Involves using metrics or criteria to assess the importance or impact of\neach network component after training. This method helps maintain model performance but may\nrequire additional validation and testing to ensure quality and robustness.\n3. Dynamic Pruning: Adjusts the network structure during inference or runtime based on feedback\nor signals. This approach can optimise the model for different scenarios or tasks but may involve\nhigher computational overhead and complexity to implement and execute.\n\n6.11.2\n\nBenefits of Pruning\n\n1. Reduced Size and Complexity: Pruning decreases the size and complexity of AI models, making\nthem easier to store, transmit, and update.\n2. Improved Efficiency and Performance: Pruned models are faster, more energy-efficient, and\nmore reliable.\n3. Enhanced generalisation and Accuracy: Pruning can make models more robust, less prone\nto overfitting, and more adaptable to new data or tasks.\n55\n\n\f6.11.3\n\nChallenges of Pruning\n\n1. Balance Between Size Reduction and Performance: Achieving the optimal balance between\nreducing size and complexity and maintaining performance is challenging; excessive or insufficient\npruning can degrade model quality and functionality.\n2. Choosing Appropriate Techniques: Selecting the right pruning technique, criterion, and objective for the specific neural network type and structure is crucial, as different methods can produce\nvarying effects and outcomes.\n3. Evaluation and Validation: Pruned models need thorough evaluation and validation to ensure\npruning has not introduced errors, biases, or vulnerabilities that could impact performance and\nrobustness.\n\n56\n\n\fChapter 7\n\nStage 5: Evaluation and Validation\n7.1\n\nSteps Involved in Evaluating and Validating Fine-Tuned\nModels\n\n1. Set Up Evaluation Metrics: Choose appropriate evaluation metrics, such as cross-entropy, to\nmeasure the difference between the predicted and actual distributions of the data.\n2. Interpret Training Loss Curve: Monitor and analyse the training loss curve to ensure the\nmodel is learning effectively, avoiding patterns of underfitting or overfitting.\n3. Run Validation Loops: After each training epoch, evaluate the model on the validation set to\ncompute relevant performance metrics and track the model\u2019s generalisation ability.\n4. Monitor and Interpret Results: Consistently observe the relationship between training and\nvalidation metrics to ensure stable and effective model performance.\n5. Hyperparameter Tuning and Adjustments: Adjust key hyperparameters such as learning\nrate, batch size, and number of training epochs to optimise model performance and prevent overfitting.\n\n7.2\n\nSetting Up Evaluation Metrics\n\nCross-entropy is a key metric for evaluating LLMs during training or fine-tuning. Originating from\ninformation theory, it quantifies the difference between two probability distributions.\n\n7.2.1\n\nImportance of Cross-Entropy for LLM Training and Evaluation\n\nCross-entropy is crucial for training and fine-tuning LLMs. It serves as a loss function, guiding the model\nto produce high-quality predictions by minimising discrepancies between the predicted and actual data.\nIn LLMs, each potential word functions as a separate class, and the model\u2019s task is to predict the next\nword given the context. This task is inherently complex, requiring the model to understand syntax,\nsemantics, and context deeply.\n\n7.2.2\n\nBeyond Cross-Entropy: Advanced LLM Evaluation Metrics\n\nWhile cross-entropy remains fundamental, evaluating LLMs effectively necessitates additional metrics\ntailored to various aspects of model performance. Here are some advanced metrics employed in LLM\nevaluation:\nPerplexity\nPerplexity measures how well a probability distribution or model predicts a sample. In the context of\nLLMs, it evaluates the model\u2019s uncertainty about the next word in a sequence. Lower perplexity indicates\nbetter performance, as the model is more confident in its predictions.\n\n57\n\n\fFactuality\nFactuality assesses the accuracy of the information produced by the LLM. It is particularly important for\napplications where misinformation could have serious consequences. Higher factuality scores correlate\nwith higher output quality.\nLLM Uncertainty\nLLM uncertainty is measured using log probability, helping to identify low-quality generations. Lower\nuncertainty indicates higher output quality. This metric leverages the log probability of each generated\ntoken, providing insights into the model\u2019s confidence in its responses.\nPrompt Perplexity\nThis metric evaluates how well the model understands the input prompt. Lower prompt perplexity\nindicates a clear and comprehensible prompt, which is likely to yield better model performance.\nContext Relevance\nIn retrieval-augmented generation (RAG) systems, context relevance measures how pertinent the retrieved context is to the user query. Higher context relevance improves the quality of generated responses\nby ensuring that the model utilises the most relevant information.\nCompleteness\nCompleteness assesses whether the model\u2019s response fully addresses the query based on the provided\ncontext. High completeness ensures that all relevant information is included in the response, enhancing\nits utility and accuracy.\nChunk Attribution and Utilisation\nThese metrics evaluate how effectively the retrieved chunks of information contribute to the final response.\nHigher chunk attribution and utilisation scores indicate that the model is efficiently using the available\ncontext to generate accurate and relevant answers.\nData Error Potential\nThis metric quantifies the difficulty the model faces in learning from the training data. Higher data\nquality results in lower error potential, leading to better model performance.\nSafety Metrics\nSafety metrics ensure that the LLM\u2019s outputs are appropriate and non-harmful. These are included in\nthe final sections of the chapter.\nIntegrating these advanced metrics provides a holistic view of LLM performance, enabling developers to\nfine-tune and optimise models more effectively. By employing a metrics-first approach, it is possible to\nensure that LLMs not only produce accurate and high-quality outputs but also do so consistently and\nreliably across diverse applications1 .\n\n7.3\n\nUnderstanding the Training Loss Curve\n\nThe training loss curve plots the loss value against training epochs and is essential for monitoring model\nperformance.\n1 https://www.rungalileo.io/blog/metrics-first-approach-to-llm-evaluation\n\n58\n\n\f7.3.1\n\nInterpreting Loss Curves\n\nAn ideal training loss curve shows a rapid decrease in loss during initial stages, followed by a gradual\ndecline and eventual plateau. Specific patterns to look for include:\n1. Underfitting: High loss value that does not decrease significantly over time, suggesting the model\ncannot learn the data.\n2. Overfitting: Decreasing training loss with increasing validation loss, indicating the model memorises the training data.\n3. Fluctuations: Significant variations may indicate a high learning rate or noisy gradients.\n\nFigure 7.1: Example training loss curve showing the decline in loss over iterations during the fine-tuning\nof Llama2 13B on a financial Q/A dataset. The curve illustrates the effectiveness of the fine-tuning\nprocess in reducing the loss and improving model performance.\n\n7.3.2\n\nAvoiding Overfitting\n\nTechniques to prevent overfitting include:\n1. Regularisation: Adds a penalty term to the loss function to encourage smaller weights.\n2. Early Stopping: Stops training when validation performance no longer improves.\n3. Dropout: Randomly deactivates neurons during training to reduce sensitivity to noise.\n4. Cross-Validation: Splits data into multiple subsets for training and validation to assess model\ngeneralisation.\n5. Batch Normalisation: Normalises inputs to each layer during training to stabilise the learning\nprocess.\n6. Larger Datasets and Batch Sizes: Reduces overfitting by increasing the amount of diverse\ndata and batch sizes.\n\n59\n\n\f7.3.3\n\nSources of Noisy Gradients\n\nNoisy gradients are common during the training of machine learning models, including LLMs. They arise\nfrom variability in gradient estimates due to stochastic gradient descent and its variants. Strategies to\nmanage noisy gradients include:\n1. Learning Rate Scheduling: Gradually decreasing the learning rate during training can reduce\nthe impact of noisy gradients.\n2. Gradient Clipping: Setting a threshold for gradient values prevents large updates that can\ndestabilise training.\n\n7.4\n\nRunning Validation Loops\n\nValidation loops provide an unbiased evaluation of model performance. Typical steps include:\n1. Split Data: Divide the dataset into training and validation sets.\n2. Initialise Validation: Evaluate the model on the validation set at the end of each epoch.\n3. Calculate Metrics: Compute relevant performance metrics, such as cross-entropy loss.\n4. Record Results: Log validation metrics for each epoch.\n5. Early Stopping: Optionally stop training if validation loss does not improve for a predefined\nnumber of epochs.\n\n7.5\n\nMonitoring and Interpreting Results\n\nMonitoring validation results involves analysing trends in validation metrics over epochs. Key aspects\ninclude:\n1. Consistent Improvement: Indicates good model generalisation if both training and validation\nmetrics improve and plateau.\n2. Divergence: Suggests overfitting if training metrics improve while validation metrics deteriorate.\n3. Stability: Ensure validation metrics do not fluctuate significantly, indicating stable training.\n\n7.6\n\nHyperparameter Tuning and Other Adjustments\n\nFine-tuning involves adjusting key hyperparameters to achieve optimal performance. Important hyperparameters include:\n1. Learning Rate: Determines the step size for updating model weights. A good starting point is\n2e-4, but this can vary.\n2. Batch Size: Larger batch sizes lead to more stable updates but require more memory.\n3. Number of Training Epochs: Balancing the number of epochs ensures the model learns sufficiently without overfitting or underfitting.\n4. Optimiser: Optimisers like Paged ADAM optimise memory usage, advantageous for large models.\nOther tunable parameters include dropout rate, weight decay, and warmup steps.\n\n7.6.1\n\nData Size and Quality\n\nThe efficacy of LLMs is directly impacted by the quality of their training data. Ensuring that datasets\nare clean, relevant, and adequate is crucial. Data cleanliness refers to the absence of noise, errors, and\ninconsistencies within the labelled data. For example, having a phrase like \u201cThis article suggests. . . \u201d\nmultiple times in the training data can corrupt the response of LLMs and add a bias towards using this\nspecific phrase more often and in inappropriate situations.\n60\n\n\f7.7\n\nBenchmarking Fine-Tuned LLMs\n\nModern LLMs are assessed using standardised benchmarks such as GLUE, SuperGLUE, HellaSwag,\nTruthfulQA, and MMLU (See Table 7.1). These benchmarks evaluate various capabilities and provide\nan overall view of LLM performance.\n\nBenchmark\nGLUE\n\nSuperGLUE\nHellaSwag\nTruthfulQA\nMMLU\nIFEval\nBBH (Big Bench Hard)\nMATH\nGPQA\nMuSR\nMMLU-PRO\nARC\nCOQA\nDROP\nSQuAD\n\nTREC\nWMT\nXNLI\nPiQA\nWinogrande\n\nDescription\nProvides a standardised set of diverse NLP tasks to\nevaluate the effectiveness of different language models\nCompares more challenging and diverse tasks with\nGLUE, with comprehensive human baselines\nEvaluates how well an LLM can complete a sentence\nMeasures truthfulness of model responses\nEvaluates how well the LLM can multitask\nTests a model\u2019s ability to follow explicit instructions,\nfocusing on formatting adherence\n23 challenging tasks from the BigBench dataset to\nevaluate LLMs using objective metrics\nCompilation of high-school level competition problems formatted using LaTeX and Asymptote\nChallenging knowledge dataset with questions\ncrafted by PhD-level domain experts\nDataset with complex problems requiring models to\nintegrate reasoning with long-range context parsing\nRefined version of MMLU with higher quality and\nmore challenging multiple-choice questions\nMeasures machine reasoning with a dataset of gradeschool science questions\nA dataset for building conversational questionanswering systems\nEvaluates the ability to perform discrete reasoning\nover paragraphs of text\nA reading comprehension dataset for evaluating\nmodels\u2019 ability to answer questions based on passages of text\nA benchmark for evaluating text retrieval methodologies\nA dataset and benchmark for evaluating machine\ntranslation models\nA dataset for evaluating cross-lingual language understanding\nA dataset for evaluating models\u2019 understanding of\nphysical interactions\nA large-scale dataset for evaluating commonsense\nreasoning\n\nReference URL\nSource\n\nSource\nSource\nSource\nSource\nSource\nSource\nSource\nSource\nSource\nSource\nSource\nSource\nSource\nSource\n\nSource\nSource\nSource\nSource\nSource\n\nTable 7.1: Detailed Overview of Benchmark Datasets Used for Evaluating Language Model Performance.\nAs LLMs evolve, so do benchmarks, with new standards such as BigCodeBench challenging current\nbenchmarks and setting new standards in the domain. Given the diverse nature of LLMs and the tasks\nthey can perform, the choice of benchmarks depends on the specific tasks the LLM is expected to handle.\nFor generic applicability, various benchmarks for different downstream applications and reasoning should\nbe utilised. For domain/task-specific LLMs, benchmarking can be limited to relevant benchmarks like\nBigCodeBench for coding.\n\n61\n\n\f7.8\n\nEvaluating Fine-Tuned LLMs on Safety Benchmark\n\nThe safety aspects of Large Language Models (LLMs) are increasingly scrutinised due to their ability\nto generate harmful content when influenced by jailbreaking prompts. These prompts can bypass the\nembedded safety and ethical guidelines within the models, similar to code injection techniques used in\ntraditional computer security to circumvent safety protocols. Notably, models like ChatGPT, GPT3, and InstructGPT are vulnerable to such manipulations that remove content generation restrictions,\npotentially violating OpenAI\u2019s guidelines. This underscores the necessity for robust safeguards to ensure\nLLM outputs adhere to ethical and safety standards.\nDecodingTrust [79] provides a comprehensive evaluation of the trustworthiness of LLMs, notably comparing GPT-4 with GPT-3.5 (ChatGPT). This evaluation spans several critical areas:\n1. Toxicity: Optimisation algorithms and generative models are employed to create challenging\nprompts that test the model\u2019s ability to avoid generating harmful content.\n2. Stereotype Bias: An array of demographic groups and stereotype topics are utilised to assess\nmodel bias, helping to understand and mitigate prejudiced responses.\n3. Adversarial Robustness: The resilience of models against adversarial attacks is tested by challenging them with sophisticated algorithms intended to deceive or mislead.\n4. Out-of-Distribution (OOD) Robustness: Models are evaluated on their ability to handle\ninputs that differ significantly from their training data, such as poetic or Shakespearean styles.\n5. Robustness to Adversarial Demonstrations: Demonstrations that contain misleading information are used to test the model\u2019s robustness across various tasks.\n6. Privacy: Various levels of privacy evaluation assess how well models safeguard sensitive information during interactions and understand privacy-related contexts.\n7. Hallucination Detection: Identifies instances where the model generates information not grounded\nin the provided context or factual data. Lower hallucination rates improve the reliability and trustworthiness of the LLM\u2019s outputs.\n8. Tone Appropriateness: Assesses whether the model\u2019s output maintains an appropriate tone for\nthe given context. This is particularly important for applications in customer service, healthcare,\nand other sensitive areas.\n9. Machine Ethics: Ethical assessments involve testing models with scenarios that require moral\njudgments, using datasets like ETHICS and Jiminy Cricket.\n10. Fairness: The fairness of models is evaluated by generating tasks that vary protected attributes,\nensuring equitable responses across different demographic groups.\nThe dataset employed for evaluating the aforementioned eight safety dimensions can be found here.\nIn partnership with HuggingFace, the LLM Safety Leaderboard utilises DecodingTrust\u2019s framework to\nprovide a unified evaluation platform for LLM safety. This allows researchers and practitioners to\nbetter understand the capabilities, limitations, and risks associated with LLMs. Users are encouraged to\nsubmit their models to HuggingFace for evaluation, ensuring they meet the evolving standards of safety\nand reliability in the field.\n\n7.9\n\nEvaluating Safety of Fine-Tuned LLM using AI Models\n\n7.9.1\n\nLlama Guard\n\nLlama Guard 2[80] is a safeguard model built on LLMs for managing risks in conversational AI applications. It effectively categorises both input prompts and responses from AI agents using a detailed safety\nrisk taxonomy tailored to identify potential legal and policy risks in AI interactions. It utilises a detailed\nsafety risk taxonomy designed to identify and manage potential legal and policy risks in interactions\ninvolving conversational AI. This taxonomy enables effective classification in areas such as:\n\u2022 Violence & Hate, addressing content that could incite violent acts or discrimination.\n62\n\n\f\u2022 Sexual Content, targeting sexually explicit material or behaviour, especially involving minors.\n\u2022 Guns & Illegal Weapons, concerning the promotion or instruction of illegal armaments.\n\u2022 Regulated or Controlled Substances, covering illegal drugs and other controlled substances.\n\u2022 Suicide & Self-Harm, aimed at content that could encourage self-destructive behaviour.\n\u2022 Criminal Planning, for content that could assist in planning or executing criminal activities.\nThe core of Llama Guard 2 is its robust framework that allows for both prompt and response classification, supported by a high-quality dataset that enhances its ability to monitor conversational exchanges.\nOperating on a Llama2-7b model, Llama Guard 2 has been instruction-tuned to deliver strong performance on benchmarks like the OpenAI Moderation Evaluation dataset and ToxicChat, where it matches\nor surpasses the capabilities of existing content moderation tools.\nThe model supports multi-class classification and generates binary decision scores. Its instruction finetuning allows for extensive customisation of tasks and adaptation of output formats. This feature enables\nusers to modify taxonomy categories to align with specific use cases and supports flexible prompting\ncapabilities, including zero-shot and few-shot applications. The adaptability and effectiveness of Llama\nGuard make it a vital resource for developers and researchers. By making its model weights publicly\navailable, Llama Guard 2 encourages ongoing development and customisation to meet the evolving needs\nof AI safety within the community.\nLlama Guard 3 represents the latest advancement over Llama Guard 2, having been fine-tuned on the\nLlama 3 8b model. The key difference between the two versions is that Llama Guard 3 expands upon\nthe capabilities of Llama Guard 2 by introducing three new categories: Defamation, Elections, and\nCode Interpreter Abuse.\nPython Library: Llama Guard 3 is accessible via HuggingFace\u2019s AutoModelForCausalLM.2 A detailed\ntutorial is available at this link. Please note that access to the model requires submitting a request to\nHugging Face with the user details. Additionally, the model weights can be downloaded from the Meta\nplatform by providing user details, and the link can be found here.\nThe prompt formats for these two models also differ, with the specific formats for Llama Guard 2 available\nhere and Llama Guard 3 is accessible here.\n\n7.9.2\n\nShield Gemma\n\nShieldGemma [81] is an advanced content moderation model built on the Gemma2 platform, designed\nto enhance the safety and reliability of interactions between LLMs and users. It effectively filters both\nuser inputs and model outputs to mitigate key harm types, including offensive language, hate speech,\nmisinformation, and explicit content. The model\u2019s scalability, with options ranging from 2B to 27B\nparameters, allows for tailored applications that meet specific needs, such as reducing latency in online\nsafety applications or enhancing performance in complex decision-making tasks.\nA distinguishing feature of ShieldGemma is its novel approach to data curation. It leverages synthetic\ndata generation techniques to create high-quality datasets that are robust against adversarial prompts\nand fair across diverse identity groups. This reduces the need for extensive human annotation, streamlining the data preparation process while ensuring the model\u2019s effectiveness. Compared to existing content\nmoderation tools like LlamaGuard and WildGuard, which typically offer fixed-size models and limited\ncustomisation, ShieldGemma\u2019s flexible architecture and advanced data handling capabilities provide a\nmore adaptable and efficient solution. These innovations position ShieldGemma as a significant advancement in LLM-based content moderation, offering developers and researchers a versatile tool that\npromotes safer and more reliable AI interactions across various platforms.\nPython Library: The ShieldGemma series is available on HuggingFace via AutoModelForCausalLM.\nThe models can be accessed here. A tutorial for running ShieldGemma 2B on Google Colab can be found\nhere. Similar to Llama Guard series, ShieldGemma series also has guidelines for prompting and it can\nbe found here.\n\n7.9.3\n\nWILDGUARD\n\nWILDGUARD [82] is an innovative open-source tool developed to enhance the safety of interactions\nwith large language models (LLMs). This tool addresses three critical moderation tasks: detecting\n2 https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForCausalLM\n\n63\n\n\fharmful intent in user prompts, identifying safety risks in model responses, and determining when a\nmodel appropriately refuses unsafe requests. Central to its development is WILDGUARD MIX3 , a\nmeticulously curated dataset comprising 92,000 labelled examples that include both benign prompts and\nadversarial attempts to bypass safety measures. The dataset is divided into WILDGUARD TRAIN, used\nfor training the model, and WILDGUARD TEST, consisting of high-quality human-annotated examples\nfor evaluation.\nThe WILDGUARD model itself is fine-tuned on the Mistral-7B language model using the WILDGUARD\nTRAIN dataset, enabling it to perform all three moderation tasks in a unified, multi-task manner. Results\nshow that WILDGUARD surpasses existing open-source moderation tools in effectiveness, particularly\nexcelling in handling adversarial prompts and accurately detecting model refusals. On many benchmarks,\nWILDGUARD\u2019s performance is on par with or exceeds that of GPT-4, a much larger, closed-source\nmodel.\nThe quick start guide and additional information on WILDGUARD are available in GitHub and it can\nbe accessed here.\n\n3 https://huggingface.co/datasets/allenai/wildguardmix\n\n64\n\n\fChapter 8\n\nStage 6: Deployment\n8.1\n\nSteps Involved in Deploying the Fine-Tuned Model\n\n1. Model Export: Save the fine-tuned model in a suitable format (e.g., ONNX, TensorFlow SavedModel, PyTorch) for deployment.\n2. Infrastructure Setup: Prepare the deployment environment, including necessary hardware, cloud\nservices, and containerisation tools.\n3. API Development: Create APIs to allow applications to interact with the model, facilitating\nprediction requests and responses.\n4. Deployment: Deploy the model to the production environment, making it accessible to end-users\nor applications.\n\n8.2\n\nCloud-Based Providers for LLM Deployment\n\nCloud-based large language model (LLM) inferencing frequently employs a pricing model based on the\nnumber of tokens processed. Users are charged according to the volume of text analysed or generated\nby the model. While this pricing structure can be cost-effective for sporadic or small-scale usage, it may\nnot always be economical for larger or continuous workloads.\nIn some scenarios, hosting an LLM solution in-house may offer better long-term cost savings, especially if\nthere is consistent or high-volume usage. Managing your own infrastructure provides greater control over\nresource allocation and allows for cost optimisation based on specific needs. Additionally, self-hosting\noffers advantages in terms of data privacy and security, as sensitive information remains within your own\nenvironment.\nHowever, it is crucial to carefully evaluate the total cost of ownership when comparing cloud-based\nsolutions with self-hosted alternatives. This evaluation should consider factors such as hardware expenses,\nmaintenance, and operational overheads. Ultimately, the decision should be informed by a comprehensive\ncost-benefit analysis, considering both short-term affordability and long-term sustainability.\nSeveral companies offer deployment services for large language models (LLMs), providing a range of\ntools and platforms to efficiently implement and manage these models. Here\u2019s a detailed list of some\nprominent providers and their services:\n\u2022 Amazon Web Services (AWS)\n\u2013 Amazon Bedrock: This service offers a suite of foundation models including Amazon Titan, which supports various NLP tasks such as summarisation and text generation. Bedrock\nintegrates seamlessly with other AWS services for scalable and secure deployment.\n\u2013 Amazon SageMaker: Provides an end-to-end machine learning service that includes tools\nfor building, training, and deploying LLMs. SageMaker JumpStart offers pre-trained models\nand step-by-step guides to simplify the deployment process.\n\n65\n\n\f\u2013 Tutorial: This tutorial explains the deployment of LLM Agents on Amazon Bedrock. Another tutorial explains end-to-end fine-tuning and deployment of LLMs with Sagemaker Canvas and Amazon Bedrock. General guidelines of Amazon Bedrock for LLM users can be found\nhere.\n\u2022 Microsoft Azure\n\u2013 Azure OpenAI Service: This service offers access to OpenAI\u2019s powerful models like GPT3.5 and Codex. It provides capabilities for embedding, image generation with DALL-E, and\nspeech-to-text with Whisper. Azure\u2019s integration with OpenAI models ensures robust deployment options for various applications.\n\u2013 Azure Machine Learning: Supports the deployment of custom and pre-trained models,\noffering tools for model management, deployment, and monitoring. It integrates with Azure\u2019s\nbroader ecosystem for scalable and secure ML operations.\n\u2013 Tutorial: Here is the tutorial for creating and deploying an Azure OpenAI Service in Microsoft Azure platform.\n\u2022 Google Cloud Platform (GCP)\n\u2013 Vertex AI: This platform allows the deployment of large language models with tools for\ntraining, tuning, and serving models. Vertex AI supports models like BERT and GPT-3,\nproviding extensive MLOps capabilities for end-to-end management.\n\u2013 Cloud AI API: Offers APIs for NLP tasks such as translation, sentiment analysis, and\nentity recognition. These APIs are backed by Google\u2019s powerful infrastructure, ensuring high\nperformance and reliability.\n\u2013 Tutorial: This document contains a tutorial for training and deploying an LLM in GCP.\n\u2022 Hugging Face\n\u2013 Inference API: This service allows users to deploy and manage LLMs hosted on Hugging\nFace\u2019s infrastructure. It supports various models from the Transformers library and provides\nan easy-to-use API for integrating these models into applications.\n\u2013 Spaces: A collaborative environment where users can deploy and share models using Hugging\nFace\u2019s hosting platform. It supports deploying custom models and interactive demos.\n\u2013 Tutorial: This document contains a tutorial for training and deploying an LLM using HuggingFace Inference API.\n\u2022 Other Platforms\n\u2013 OpenLLM: Provides deployment solutions here.\n\u2013 Deepseed: Offers deployment solutions here.\n\n8.3\n\nTechniques for Optimising Model Performance During Inference\n\nOptimising model performance during inference is crucial for the efficient deployment of large language\nmodels (LLMs). The following advanced techniques offer various strategies to enhance performance,\nreduce latency, and manage computational resources effectively.\n\n8.3.1\n\nTraditional On-Premises GPU-Based Deployments\n\nThis conventional approach to deploying large language models (LLMs) involves using Graphics Processing Units (GPUs) due to their parallel processing capabilities, which enable fast and efficient inference.\nHowever, this method requires upfront hardware investment and may not be suitable for applications\nwith fluctuating demand or limited budgets. GPU-based deployments face several challenges:\n1. Resource utilisation may suffer during periods of low demand due to idle servers.\n2. Scaling up or down often requires physical hardware modifications, which can be time-consuming.\n66\n\n\f3. Centralised servers can introduce single points of failure and scalability limitations.\nTo mitigate these issues, strategies such as load balancing between multiple GPUs, fallback routing, model\nparallelism, and data parallelism can be employed to achieve better results. Optimisation techniques like\ndistributed inference using PartialState from accelerate can further enhance efficiency.\nExample use case: Large-Scale NLP Application\nFor instance, a large e-commerce platform implemented traditional on-premises GPU-based deployment\nto handle millions of customer queries daily. By utilising load balancing and model parallelism, they\nwere able to achieve a significant reduction in latency and improved customer satisfaction.\n\n8.3.2\n\nDistributed LLM: Torrent-Style Deployment and Parallel Forward Passes\n\nAn innovative deployment strategy for large language models (LLMs) involves distributing them across\nmultiple GPUs in a decentralised, torrent-style manner. Libraries like Petals1 can perform this task.\nPetals functions as a decentralised pipeline designed for rapid neural network inference by partitioning\nthe model into distinct blocks or layers, which are distributed across multiple geographically dispersed\nservers. Users can connect their own GPUs to this network, acting as both contributors and clients who\ncan access and apply the model to their data.\nWhen a client request is received, the network routes it through a series of servers optimised to minimise\nthe total forward pass time. Each server dynamically selects the most optimal set of blocks, adapting to\nthe current bottlenecks in the pipeline. This framework leverages decentralisation principles to distribute\ncomputational load across diverse regions, sharing computational resources and GPUs in a way that\nreduces the financial burden on individual organisations. This collaborative approach not only optimises\nresource utilisation but also fosters a global community dedicated to shared AI goals.\n\nFigure 8.1: Conceptual Representation of Distributed LLM Deployment Using a Torrent-Style Approach.\nThis figure illustrates the distributed deployment of a Large Language Model (LLM) using a torrent-style\napproach, where multiple GPT model layers (stacks) are distributed across different nodes (represented\nby chefs) and perform parallel forward passes. The process mimics the flow of orders from customers\n(input data) through restaurants (intermediate processing layers) to chefs (model layers), highlighting\nthe efficiency of parallel processing and distributed computing in handling large-scale language models.\nThis approach is essential for reducing inference latency and improving the scalability of LLMs across\ndiverse computational environments. (adapted from [83])\n1 https://github.com/bigscience-workshop/petals\n\n67\n\n\fExample use case: Global Research Collaboration\nA consortium of research institutions implemented a distributed LLM using the Petals framework to\nanalyse large datasets across different continents. By leveraging the decentralised nature of Petals, they\nachieved high efficiency in processing and collaborative model development.\n\n8.3.3\n\nWebGPU-Based Deployment of LLM\n\nThis deployment option for large language models (LLMs) involves utilising WebGPU, a web standard\nthat provides a low-level interface for graphics and compute applications on the web platform. With\nWebGPU, organisations can harness the power of GPUs directly within web browsers, enabling efficient inference for LLMs in web-based applications. WebGPU enables high-performance computing and\ngraphics rendering directly within the client\u2019s web browser. It allows developers to utilise the client\u2019s\nGPU for tasks such as rendering graphics, accelerating computational workloads, and performing parallel processing, all without the need for plugins or additional software installations. This capability\npermits complex computations to be executed efficiently on the client\u2019s device, leading to faster and\nmore responsive web applications.\n\n8.3.4\n\nLLM on WebGPU using WebLLM\n\nClients can access powerful large language models and chatbots directly in their browser, leveraging\nWebGPU acceleration. This approach eliminates server dependencies, providing users with exceptional\nperformance and enhanced privacy. WebLLM facilitates the use of large language models directly in the\nclient\u2019s browser to perform tasks such as filtering out personally identifiable information (PII) or named\nentity recognition (NER) on data without transmitting it over the network. This ensures enhanced\nprivacy and security by retaining sensitive information on the client side.\n\n68\n\n\fFigure 8.2: WebGPU-Based Deployment of LLM: This diagram illustrates the architecture of deploying\na large language model (LLM) using WebGPU technology. The CPU manages the distribution of prompt\ninferencing tasks to multiple GPUs, which then process these prompts in parallel, enhancing efficiency\nand scalability in LLM deployment across web-based platforms. (adapted from [83])\nAdditional Use Cases for WebLLM\n1. Language Translation: Enable real-time translation of text directly in the browser, allowing\nusers to communicate across language barriers without transmitting their messages over the network.\n2. Code Autocompletion: Develop code editors that provide intelligent autocompletion suggestions\nbased on context, leveraging WebLLM to understand and predict code snippets.\n3. Customer Support Chatbots: Implement chatbots on websites to provide instant customer\nsupport and answer frequently asked questions without relying on external servers.\n4. Data Analysis and Visualisation: Create browser-based tools for analysing and visualising\ndata, with WebLLM assisting in data processing, interpretation, and generating insights.\n5. Personalised Recommendations: Develop recommendation engines that offer personalised\nproduct recommendations, content suggestions, or movie/music recommendations based on user\npreferences and behaviour.\n6. Privacy-Preserving Analytics: Develop analytics platforms that perform data analysis directly\nin the browser, ensuring that sensitive information remains on the client side and reducing the risk\nof data breaches.\n\n69\n\n\fExample use case: Privacy-Focused Web Application\nA healthcare startup deployed an LLM using WebLLM to process patient information directly within the\nbrowser, ensuring data privacy and compliance with healthcare regulations. This approach significantly\nreduced the risk of data breaches and improved user trust.\n\n8.3.5\n\nQuantised LLMs\n\nModel quantisation is a technique utilised to reduce the size of an AI model by representing its parameters\nwith fewer bits. In traditional machine learning models, each parameter (e.g., weights and biases in neural\nnetworks) is typically stored as a 32-bit floating-point number, necessitating significant memory and\ncomputational resources, particularly for large models. Quantisation aims to alleviate this by reducing\nthe precision of these parameters. For instance, instead of storing each parameter as a 32-bit floatingpoint number, they may be represented using fewer bits, such as 8-bit integers. This compression\nreduces the memory footprint of the model, making it more efficient to deploy and execute, especially in\nresource-constrained environments like mobile devices or edge devices. QLoRA is a popular example of\nthis quantisation for LLMs and can be used to deploy LLMs locally or host them on external servers.\nExample use case: Edge Device Deployment\nA tech company used quantised LLMs to deploy advanced NLP models on mobile devices, enabling offline\nfunctionality for applications such as voice recognition and translation. This deployment significantly\nimproved app performance and user experience by reducing latency and reliance on internet connectivity.\n\n8.3.6\n\nvLLMs\n\nThe vLLM2 system efficiently handles requests by employing a block-level memory management method\nand preemptive request scheduling. It utilises the PagedAttention[84] algorithm to manage the keyvalue (KV) cache, thereby reducing memory waste and fragmentation. By batching requests and sharing\nphysical blocks across multiple samples, vLLM optimises memory usage and enhances throughput. Performance tests indicate that vLLM surpasses other systems in various decoding scenarios. Consider a\ntransformer-based model tasked with summarising a lengthy book. Traditional transformers process the\nentire book simultaneously, which can be both computationally and memory-intensive, especially for extended texts. With PagedAttention, the book is divided into smaller segments or pages. The model then\nfocuses on summarising one page at a time, rather than the entire book simultaneously. This approach\nreduces computational complexity and memory requirements, making it more feasible to process and\nsummarise lengthy texts efficiently.\nExample use case: High-Volume Content Generation\nA content marketing agency implemented vLLMs for generating large volumes of SEO-optimised content.\nBy leveraging the efficient memory management of vLLMs, they were able to handle multiple concurrent\nrequests, significantly increasing their content production rate while maintaining high quality.\n\n8.4\n\nKey Considerations for Deployment of LLMs\n\nDeploying large language models (LLMs) effectively requires careful planning and consideration of various\nfactors to ensure optimal performance, cost-efficiency, and security. Key considerations include:\n\u2022 Infrastructure Requirements:\n\u2013 Compute Resources: Ensure adequate CPU/GPU resources to handle the model\u2019s computational demands. High-performance GPUs are typically required for efficient inference and\ntraining.\n\u2013 Memory: LLMs, especially those with billions of parameters, require substantial memory.\nMemory management techniques such as quantisation and model parallelism can be employed\nto optimise usage.\n2 https://docs.vllm.ai/en/stable/\n\n70\n\n\f\u2022 Scalability:\n\u2013 Horizontal Scaling: Plan for horizontal scaling to distribute the load across multiple servers,\nwhich can improve performance and handle increased demand.\n\u2013 Load Balancing: Implement load balancing strategies to ensure even distribution of requests\nand prevent any single point of failure.\n\u2022 Cost Management:\n\u2013 Token-based Pricing: Understand the cost implications of token-based pricing models offered by cloud providers. This model charges based on the number of tokens processed, which\ncan become expensive with high usage.\n\u2013 Self-Hosting: Evaluate the costs and benefits of self-hosting versus cloud hosting. Selfhosting might offer long-term savings for consistent, high-volume usage but requires significant\nupfront investment in hardware and ongoing maintenance.\n\u2022 Performance Optimisation:\n\u2013 Latency: Minimise latency to ensure real-time performance, particularly for applications\nrequiring instant responses like chatbots and virtual assistants.\n\u2013 Throughput: Maximise throughput to handle a high volume of requests efficiently. Techniques like batching and efficient memory management (e.g., PagedAttention) can help.\n\u2022 Security and Privacy:\n\u2013 Data Security: Implement robust security measures to protect sensitive data, including\nencryption and secure access controls.\n\u2013 Privacy: Ensure compliance with data privacy regulations by keeping sensitive data within\nyour environment if self-hosting, or ensuring cloud providers comply with relevant privacy\nstandards.\n\u2022 Maintenance and Updates:\n\u2013 Model Updates: Regularly update the model to incorporate new data and improve performance. Automate this process if possible to reduce manual effort.\n\u2013 System Maintenance: Plan for regular maintenance of the infrastructure to prevent downtime and ensure smooth operation.\n\u2022 Flexibility and Customisation:\n\u2013 Fine-Tuning: Allow for model fine-tuning to adapt the LLM to specific use cases and\ndatasets. Fine-tuning can improve accuracy and relevance in responses.\n\u2013 API Integration: Ensure the deployment platform supports easy integration with existing\nsystems and workflows through APIs and SDKs.\n\u2022 User Management:\n\u2013 Access Control: Implement role-based access control to manage who can deploy, use, and\nmaintain the LLM.\n\u2013 Monitoring and Logging: Set up comprehensive monitoring and logging to track usage,\nperformance, and potential issues. This helps in proactive troubleshooting and optimisation.\n\u2022 Compliance:\n\u2013 Regulatory Compliance: Ensure that the deployment adheres to all relevant regulatory\nand legal requirements, including data protection laws like GDPR, HIPAA, etc.\n\u2013 Ethical Considerations: Implement ethical guidelines to avoid biases and ensure the responsible use of LLMs.\n\u2022 Support and Documentation:\n\u2013 Technical Support: Choose a deployment platform that offers robust technical support and\nresources.\n\u2013 Documentation: Provide comprehensive documentation for developers and users to facilitate smooth deployment and usage.\n\n71\n\n\fChapter 9\n\nStage 7: Monitoring and\nMaintenance\n9.1\n\nSteps Involved in Monitoring and Maintenance of Deployed\nFine-Tuned LLMs\n\nContinuous monitoring and maintenance of fine-tuned LLMs are essential to ensure their optimal performance, accuracy, and security over time. Below are the key steps involved in this process:\n1. Setup Initial Baselines: Establish initial performance baselines by evaluating the model on a\ncomprehensive test dataset. Record metrics such as accuracy, latency, throughput, and error rates\nto serve as reference points for future monitoring.\n2. Performance Monitoring: Implement systems to continuously track key performance metrics\nsuch as response time, server load, and token usage. Regularly compare these metrics against the\nestablished baselines to detect any deviations.\n3. Accuracy Monitoring: Continuously evaluate the model\u2019s predictions against a ground truth\ndataset. Use metrics like precision, recall, F1 score, and cross-entropy loss to ensure the model\nmaintains high accuracy levels.\n4. Error Monitoring: Track and analyse errors, including runtime errors and prediction errors.\nImplement logging mechanisms to capture detailed information about each error for troubleshooting\nand improvement.\n5. Log Analysis: Maintain comprehensive logs for each prediction request and response, including\ninput data, output predictions, response times, and encountered errors. Regularly review logs to\nidentify patterns and areas for improvement.\n6. Alerting Mechanisms: Set up automated alerting systems to notify stakeholders of any anomalies\nor deviations from expected performance metrics. Integrate alerts with communication tools like\nSlack, PagerDuty, or email for timely responses.\n7. Feedback Loop: Establish a feedback loop with end-users to gather insights on model performance\nand user satisfaction. Use this feedback to continuously refine and improve the model.\n8. Security Monitoring: Implement robust security measures to monitor for threats, including\nunauthorised access, data breaches, and adversarial attacks. Use encryption, access control, and\nregular security audits to protect the model and data.\n9. Drift Detection: Continuously monitor for data and concept drift using statistical tests and\ndrift detectors. Regularly evaluate the model on holdout datasets to detect changes in input data\ndistribution or model performance.\n10. Model Versioning: Maintain version control for different iterations of the model. Track performance metrics for each version to ensure that the best-performing model is in production.\n\n72\n\n\f11. Documentation and Reporting: Keep detailed documentation of monitoring procedures, metrics, and findings. Generate regular reports to provide stakeholders with insights into the model\u2019s\nperformance and maintenance activities.\n12. Periodic Review and Update: Regularly assess and update the monitoring processes to incorporate new techniques, tools, and best practices, ensuring the monitoring system remains effective\nand up-to-date.\n\n9.2\n\nContinuous Monitoring of Model Performance\n\nWhile large language model (LLM) applications undergo some form of evaluation, continuous monitoring\nremains inadequately implemented in most cases. This section outlines the components necessary to\nestablish an effective monitoring programme aimed at safeguarding users and preserving brand integrity.\n\n9.2.1\n\nFunctional Monitoring\n\nInitially, it is crucial to monitor fundamental metrics consistently. This includes tracking metrics such\nas request volume, response times, token utilisation, costs incurred, and error rates.\n\n9.2.2\n\nPrompt Monitoring\n\nFollowing functional metrics, attention should be directed towards monitoring user-generated prompts\nor inputs. Metrics like readability can provide valuable insights. LLM evaluators should be employed to\ndetect potential toxicity in responses. Additionally, metrics such as embedding distances from reference\nprompts prove insightful, ensuring adaptability to varying user interactions over time.\nIntroducing a new evaluation category involves identifying adversarial attempts or malicious prompt\ninjections, often overlooked in initial evaluations. Comparison against reference sets of known adversarial\nprompts helps identify and flag malicious activities. Evaluative LLMs play a crucial role in classifying\nprompts as benign or malicious.\n\n9.2.3\n\nResponse Monitoring\n\nMonitoring responses involves several critical checks to ensure alignment with expected outcomes. Parameters such as relevance, coherence (hallucination), topical alignment, sentiment, and their evolution\nover time are essential. Metrics related to toxicity and harmful output require frequent monitoring due\nto their critical impact. Prompt leakage represents an adversarial tactic wherein sensitive prompt information is illicitly extracted from the application\u2019s stored data. Monitoring responses and comparing\nthem against the database of prompt instructions can help detect such breaches. Embedding distance\nmetrics are particularly effective in this regard. Regular testing against evaluation datasets provides\nbenchmarks for accuracy and highlights any performance drift over time. Tools capable of managing\nembeddings allow exportation of underperforming output datasets for targeted improvements.\n\n9.2.4\n\nAlerting Mechanisms and Thresholds\n\nEffective monitoring necessitates well-calibrated alerting thresholds to avoid excessive false alarms. Implementing multivariate drift detection and alerting mechanisms can enhance accuracy. Consideration\nof false alarm rates and best practices for setting thresholds is paramount for effective monitoring system design. Alerting features should include integration with communication tools such as Slack and\nPagerDuty. Some systems offer automated response blocking in case of alerts triggered by problematic\nprompts. Similar mechanisms can be employed to screen responses for personal identifiable information\n(PII), toxicity, and other quality metrics before delivery to users. Custom metrics tailored to specific\napplication nuances or innovative insights from data scientists can significantly enhance monitoring efficacy. Flexibility to incorporate such metrics is essential to adapt to evolving monitoring needs and\nadvancements in the field.\n\n73\n\n\f9.2.5\n\nMonitoring User Interface (UI)\n\nThe monitoring system\u2019s UI is pivotal, typically featuring time-series graphs of monitored metrics. Differentiated UIs facilitate in-depth analysis of alert trends, aiding root cause analysis. Advanced UI\ncapabilities may include visualisations of embedding spaces through clustering and projections, providing insights into data patterns and relationships. Mature monitoring systems categorise data by users,\nprojects, and teams, ensuring role-based access control (RBAC) to protect sensitive information. Optimising alert analysis within the UI interface remains an area where improvements can significantly\nreduce false alarm rates and enhance operational efficiency.\n\n9.3\n\nUpdating LLM Knowledge\n\nTo improve the knowledge base of an LLM, continued pretraining is used to help LLM evolve with the\nlatest knowledge and information. The world and language are constantly evolving. New information\nemerges, trends shift, and cultural references change. LLMs trained on static data can become outdated,\nleading to:\n\u2022 Factual Errors: Outdated information can cause LLMs to provide inaccurate responses.\n\u2022 Irrelevance: Models might miss the context of current events or use outdated references.\n\u2022 Bias Perpetuation: Biases present in training data can become entrenched if not addressed\nthrough updates.\n\n9.3.1\n\nRetraining Methods\n\n\u2022 Periodic Retraining: This involves refreshing the model\u2019s knowledge base at regular intervals\n(weekly, monthly, yearly) with new data. This is a straightforward method but requires a steady\nstream of high-quality, unbiased data.\n\u2022 Trigger-Based Retraining: This approach monitors the LLM\u2019s performance. When metrics like\naccuracy or relevance fall below a certain threshold, a retraining process is triggered. This method\nis more dynamic but requires robust monitoring systems and clear performance benchmarks.\n\n9.3.2\n\nAdditional Methods\n\n\u2022 Fine-Tuning: LLMs can be fine-tuned for specific tasks by training them on smaller, domainspecific datasets. This allows for specialisation without complete retraining.\n\u2022 Active Learning: This approach involves selectively querying the LLM to identify areas where\nit lacks knowledge. The retrieved information is then used to update the model.\n\n9.3.3\n\nKey Considerations\n\n\u2022 Data Quality and Bias: New training data must be carefully curated to ensure quality and\nmitigate bias. Techniques like human annotation and fairness checks are crucial.\n\u2022 Computational Cost: Retraining LLMs can be computationally expensive, requiring significant\nresources. Optimisations like transfer learning (using pre-trained models as a starting point) can\nhelp reduce costs.\n\u2022 Downtime: Retraining often takes time, leading to LLM downtime. Strategies like rolling updates\nor deploying multiple models can minimise service disruptions.\n\u2022 Version Control: Tracking different versions of the LLM and their training data is essential for\nrollbacks in case of performance issues.\n\n74\n\n\f9.4\n\nThe Future of LLM Updates\n\nResearch is ongoing to develop more efficient and effective LLM update strategies. One promising area\nis continuous learning, where LLMs can continuously learn and adapt from new data streams without\nretraining from scratch. Continuous learning aims to reduce the need for frequent full-scale retraining by\nenabling models to update incrementally with new information. This approach can significantly enhance\nthe model\u2019s ability to remain current with evolving knowledge and language use, improving its long-term\nperformance and relevance.\nInnovations in transfer learning and meta-learning are also contributing to advancements in LLM updates.\nThese techniques allow models to leverage pre-existing knowledge and adapt quickly to new tasks or\ndomains with minimal additional training. By integrating these advanced learning methods, future\nLLMs can become more adaptable and efficient in processing and understanding new information.\nFurthermore, ongoing improvements in hardware and computational resources will support more frequent\nand efficient updates. As processing power increases and becomes more accessible, the computational\nburden of updating large models will decrease, enabling more regular and comprehensive updates.\nCollaboration between academia and industry is vital in driving these advancements. By sharing research\nfindings and best practices, the field can collectively move towards more robust and efficient LLM update\nmethodologies, ensuring that models remain accurate, relevant, and valuable over time.\n\n75\n\n\fChapter 10\n\nIndustrial Fine-Tuning Platforms\nand Frameworks for LLMs\nThe evolution of fine-tuning techniques has been propelled by leading tech companies and platforms that\nhave introduced innovative frameworks and services. Companies like HuggingFace, Amazon Web Services\n(AWS), Microsoft Azure, and OpenAI have developed tools and platforms that simplify and democratise\nthe fine-tuning process. These advancements have not only lowered the barrier to entry for leveraging\nstate-of-the-art AI models but have also enabled a wide range of applications across various industries,\nfrom healthcare and finance to customer service and content creation. Each of these platforms offers\nunique capabilities that cater to different needs, whether it be through automated fine-tuning workflows,\nscalable cloud-based training environments, or accessible API interfaces for deploying custom models.\nHuggingFace, for example, has made significant strides with its Transformers library1 and tools like Autotrain2 and SetFit, which allow users to fine-tune models with minimal coding and data. Their platform\nprovides a robust infrastructure that supports both the research community and industry practitioners,\nfacilitating the rapid development and deployment of custom AI solutions. Similarly, AWS\u2019s SageMaker3\nand SetFit4 provides an extensive suite of services that cover the entire machine learning lifecycle, from\ndata preparation and training to model deployment and optimisation, making it a comprehensive solution for enterprise-level applications.\nOn the other hand, Microsoft Azure integrates its fine-tuning capabilities with enterprise-grade tools\nand services, offering solutions like Azure Machine Learning and the Azure OpenAI Service that cater to\nlarge organisations looking to incorporate advanced AI into their operations. Azure\u2019s focus on MLOps\nand seamless integration with other Azure services ensures that fine-tuned models can be efficiently deployed and maintained in production environments. Meanwhile, OpenAI has pioneered the concept of\n\u201dfine-tuning as a service\u201d allowing businesses to leverage their powerful models like GPT-4 through a\nuser-friendly API 5 , enabling custom model adaptations without the need for in-house AI expertise or\ninfrastructure.\nThe collective efforts of these tech companies have not only enhanced the efficiency and scalability of\nfine-tuning but also democratised access to sophisticated AI tools. By reducing the technical barriers\nand providing comprehensive, user-friendly platforms, these innovations have enabled a wider range of\nindustries to deploy advanced AI models tailored to their specific needs. Tables 10.1 and 10.2 offer a\nquick comparison of LLM fine-tuning tools and frameworks from different providers.\n1 https://huggingface.co/docs/transformers/en/index/\n2 https://huggingface.co/autotrain\n3 https://huggingface.co/autotrain\n4 https://aws.amazon.com/sagemaker/\n5 https://platform.openai.com/docs/guides/fine-tuning/fine-tuning-integrations\n\n76\n\n\fParameter\n\nNVIDIA\nNeMo\n\nHugging Face\nAutoTrain\nAPI\nFine-tuning\nand deployment\nof LLMs with\nminimal code.\n\nAmazon\nBedrock\n\nPrimary Use\nCase\n\nCustom\nfinetuning of LLMs\nwith advanced\nNVIDIA GPUs.\n\nModel Support\n\nSupports a variety of large, pretrained models,\nincluding Megatron series.\n\nSupports various LLMs like\nAmazon Titan\nand third-party\nmodels.\n\nUsers\nprovide\ntask-specific\ndata for finetuning,\nprocessed\nusing\nNVIDIA\u2019s\ninfrastructure.\nHigh; extensive\ncontrol\nover\nfine-tuning process and model\nparameters.\nHigh; leverages\nNVIDIA\u2019s GPU\ncapabilities for\nefficient scaling.\n\nSupports a wide\nrange of pretrained models\nfrom the Hugging Face model\nhub.\nUploads\ndatasets\nvia\na simple interface; AutoTrain\nhandles\npreprocessing and\nmodel training.\nModerate; automated process\nwith\nsome\ncustomisation\noptions.\nHigh;\nscalable\nvia\nHugging\nFace\u2019s\ncloud\ninfrastructure.\n\nData Handling\n\nDeployment\nOptions\n\nOn-premises\nor cloud deployment\nvia\nNVIDIA infrastructure.\n\nDeployed\nvia\nHugging Face\u2019s\ncloud or can be\nexported for local deployment.\n\nIntegration with\nEcosystem\n\nDeep integration\nwith\nNVIDIA\ntools\n(e.g.,\nTensorRT) and\nGPU-based\nworkflows.\nUsers\nmust\nensure\ndata\nprivacy compliance; NVIDIA\nhandles\ndata\nduring processing.\nEnterprises and\ndevelopers needing\nadvanced\ncustomisation\nand\nperformance in LLM\nfine-tuning.\nHigh\nresource\ndemand\nand\npotential costs;\ndependency on\nNVIDIA ecosystem.\n\nIntegrates\nwell with the\nHugging\nFace\necosystem and\nother ML tools.\n\nCustomisation\nLevel\n\nScalability\n\nData Privacy\n\nTarget Users\n\nLimitations\n\nFine-tuning and\ndeploying LLMs\non AWS infrastructure.\n\nData\nhandled\nwithin Hugging\nFace\u2019s environment;\nprivacy\ndepends\non\ndata\nhandling\npractices.\nDevelopers and\nbusinesses looking for easy,\nautomated LLM\nfine-tuning solutions.\n\nData is uploaded\nand\nmanaged\nwithin the AWS\nenvironment;\nintegrates with\nAWS data services.\nHigh;\ndetailed\nconfiguration\nand integration\nwith AWS services.\nVery\nHigh;\nscalable across\nAWS\u2019s extensive\ncloud infrastructure.\nIntegrated into\nAWS services,\neasily\ndeployable\nacross\nAWS\u2019s\nglobal\ninfrastructure.\nSeamless integration\nwith\nAWS\nservices (e.g., S3,\nLambda, SageMaker).\nStrong\nfocus\non data privacy\nwithin\nAWS\nenvironment;\ncompliant with\nvarious\nstandards.\nBusinesses and\ndevelopers integrated into or\nseeking to leverage AWS cloud\nservices.\n\nLess\ncontrol\nover fine-tuning\nspecifics; cloudbased,\nmay\nnot suit all onpremises needs.\n\nDependency\non AWS; potential\nvendor\nlock-in,\ncost\nmanagement\ncomplexity.\n\nAWS\nSageMaker JumpStart\nSimplified finetuning and deployment within\nthe AWS ecosystem.\nPre-trained\nmodels\nfrom\nAWS and partners; integration\nwith\ncustom\nmodels.\nUploads\nand\nprocesses data\nwithin\nAWS;\nsupports various\ndata formats.\n\nModerate;\npre-configured\nsettings\nwith\nsome customisation available.\nHigh;\nscalable\nwithin the AWS\ncloud\necosystem.\nAWS\ncloud\ndeployment;\nintegrates with\nother AWS services.\n\nHugging Face\nTrainer API\nManual\nfinetuning of LLMs\nwith\ndetailed\ncontrol\nover\ntraining\nprocesses.\nSupports a vast\narray of models\nfrom the Hugging Face model\nhub.\nUsers manually\npreprocess data\nand\nmanage\ntraining steps.\n\nVery\nHigh;\ndetailed\ncontrol over every\naspect of finetuning.\nHigh; scalability\ndepends on the\ninfrastructure\nused (e.g., local\nvs. cloud).\nDeployable locally, in cloud,\nor exported to\nother platforms.\n\nStrong integration with AWS\nservices;\neasy\nto connect with\ndata\npipelines\nand analytics.\nStrong\nAWS\nprivacy\nand\nsecurity\nmeasures; compliant\nwith\nindustry\nstandards.\n\nIntegrates with\nHugging\nFace\necosystem and\nother Pythonbased ML tools.\n\nEnterprises and\ndevelopers seeking streamlined\nAI/ML solutions\nwithin AWS.\n\nResearchers,\ndevelopers, and\nML\nengineers\nneeding detailed\ncontrol\nover\ntraining.\n\nLimited\nto\nAWS services;\npre-configured\noptions\nmay\nlimit deep customisation.\n\nRequires technical expertise;\nmore\ncomplex\nsetup and management.\n\nUser-managed;\ndepends\non\nwhere the models and data are\nhosted.\n\nTable 10.1: Detailed Comparison of LLM Fine-Tuning Platforms (Part I). This table provides a comprehensive comparison of various fine-tuning tools for Large Language Models (LLMs), including NVIDIA\nNeMo, Hugging Face AutoTrain API, Amazon Bedrock, AWS SageMaker JumpStart, and Hugging Face\nTrainer API. It covers multiple aspects such as the primary use case, model support, data handling,\ncustomisation level, scalability, deployment options, integration with the ecosystem, data privacy, target\nusers, and limitations for each tool.\n\n77\n\n\fParameter\nPrimary Use\nCase\n\nModel Support\n\nData Handling\n\nCustomisation\nLevel\n\nScalability\n\nOpenAI\nFineTuning API\nAPI-based\nfinetuning for OpenAI\nmodels with custom\ndatasets.\nLimited to OpenAI\nmodels like GPT-3\nand GPT-4.\n\nGoogle Vertex AI\nStudio\nEnd-to-end\nML\nmodel development\nand\ndeployment\nwithin Google Cloud.\nSupports\nGoogle\u2019s\npre-trained\nmodels\nand user-customised\nmodels.\n\nMicrosoft\nAzure\nAI Studio\nEnd-to-end AI development, fine-tuning,\nand deployment on\nAzure.\nSupports Microsoft\u2019s\nmodels and custom\nmodels\nfine-tuned\nwithin Azure.\n\nUsers upload datasets\nvia API; OpenAI\nhandles preprocessing and fine-tuning.\nModerate; focuses on\nease of use with limited deep customisation.\nHigh;\nscalable\nthrough\nOpenAI\u2019s\ncloud infrastructure.\n\nData managed within\nGoogle Cloud; supports multiple data\nformats.\nHigh; offers custom\nmodel training and\ndeployment with detailed configuration.\nVery High; leverages\nGoogle Cloud\u2019s infrastructure for scaling.\nDeployed\nwithin\nGoogle Cloud; integrates with other\nGCP services.\n\nData\nintegrated\nwithin Azure ecosystem; supports various\nformats and sources.\nHigh; extensive customisation\noptions\nthrough Azure\u2019s AI\ntools.\nVery High; scalable\nacross Azure\u2019s global\ninfrastructure.\n\nSeamless integration\nwith Google Cloud\nservices (e.g., BigQuery, AutoML).\nStrong privacy and\nsecurity\nmeasures\nwithin Google Cloud\nenvironment.\nDevelopers and businesses integrated into\nGoogle Cloud or seeking to leverage GCP.\n\nDeep integration with\nAzure\u2019s services (e.g.,\nData Factory, Power\nBI).\nStrong privacy and\nsecurity\nmeasures\nwithin Azure environment.\nEnterprises and developers\nintegrated\ninto Azure or seeking\nto leverage Azure\u2019s\nAI tools.\nLimited to Azure\necosystem; potential\ncost\nand\nvendor\nlock-in.\n\nDeployment\nOptions\n\nDeployed via API, integrated into applications using OpenAI\u2019s\ncloud.\n\nIntegration with\nEcosystem\n\nLimited to OpenAI\necosystem; integrates\nwell with apps via\nAPI.\nManaged by OpenAI;\nusers must consider\ndata transfer and privacy implications.\nDevelopers and enterprises\nlooking\nfor straightforward,\nAPI-based\nLLM\nfine-tuning.\nLimited customisation; dependency on\nOpenAI\u2019s infrastructure; potential cost.\n\nData Privacy\n\nTarget Users\n\nLimitations\n\nLimited to Google\nCloud ecosystem; potential cost and vendor lock-in.\n\nDeployed\nwithin\nAzure;\nintegrates\nwith Azure\u2019s suite of\nservices.\n\nLangChain\nBuilding applications\nusing LLMs with\nmodular and customisable workflows.\nSupports integration\nwith various LLMs\nand AI tools (e.g.,\nOpenAI, GPT-4, Cohere).\nData handling is flexible, dependent on\nthe specific LLM and\nintegration used.\nVery High; allows detailed customisation\nof workflows, models,\nand data processing.\nHigh; scalability depends on the specific\ninfrastructure\nand\nmodels used.\nDeployed\nwithin\ncustom\ninfrastructure; integrates with\nvarious cloud and\non-premises services.\nFlexible integration\nwith multiple tools,\nAPIs,\nand\ndata\nsources.\nDependent on the integrations and infrastructure used; users\nmanage privacy.\nDevelopers needing\nto build complex,\nmodular LLM-based\napplications\nwith\ncustom workflows.\nComplexity in chaining multiple models\nand data sources; requires more setup.\n\nTable 10.2: Detailed Comparison of LLM Fine-Tuning Platforms (Part II). This table continues the\ncomparison of LLM fine-tuning tools, focusing on OpenAI Fine-Tuning API, Google Vertex AI Studio,\nMicrosoft Azure AI Studio, and LangChain. It evaluates the tools based on the primary use case,\nmodel support, data handling, customisation level, scalability, deployment options, integration with the\necosystem, data privacy, target users, and limitations, offering a complete view of their capabilities and\nconstraints.\n\n10.1\n\nAutotrain\n\nAutotrain is HuggingFace\u2019s innovative platform that automates the fine-tuning of large language models,\nmaking it accessible even to those with limited machine learning expertise. The complexity and resource\ndemands of fine-tuning LLMs can be daunting, but Autotrain simplifies the process by handling the most\nchallenging aspects, such as data preparation, model configuration, and hyperparameter optimisation.\nThis automation is particularly valuable for small teams or individual developers who need to deploy\ncustom LLMs quickly and efficiently.\n\n10.1.1\n\nSteps Involved in Fine-Tuning Using Autotrain\n\nFollowing are the steps involved in fine-tuning LLMs using Autotrain. Figure 10.1 represents the visual\nworkflow.\n\u2022 Dataset Upload and Model Selection:\n78\n\n\fFigure 10.1: Overview of the Autotrain Workflow. This diagram illustrates the step-by-step process\nwithin the Autotrain system, beginning with the upload of datasets and model selection by users. The\nworkflow then moves to data preparation and model configuration, followed by automated hyperparameter tuning to optimise model performance. The fine-tuning phase adjusts the model based on the\nprovided datasets, culminating in the deployment of the fully fine-tuned model for practical use.\n\u2013 Users begin by uploading their datasets to the Autotrain platform.\n\u2013 They then select a pre-trained model from the extensive HuggingFace Model Hub.\n\u2022 Data Preparation:\n\u2013 Autotrain automatically processes the uploaded data, including tasks like tokenization to\nconvert text into a format the LLM can understand.\n\u2022 Model Configuration:\n\u2013 The platform configures the model for fine-tuning, setting up the training environment and\nnecessary parameters.\n\u2022 Automated Hyperparameter Tuning:\n\u2013 Autotrain explores various hyperparameter configurations (such as learning rate, batch size,\nand sequence length) and selects the best-performing ones.\n\u2022 Fine-Tuning:\n\u2013 The model is fine-tuned on the prepared data with the optimised hyperparameters.\n\u2022 Deployment:\n\u2013 Once fine-tuning is complete, the model is ready for deployment in various NLP applications,\nsuch as text generation, completion, and language translation.\n79\n\n\f10.1.2\n\nBest Practices of Using Autotrain\n\n\u2022 Data Quality: Ensure high-quality, well-labelled data for better model performance.\n\u2022 Model Selection: Choose pre-trained models that are well-suited to your specific task to minimize\nfine-tuning effort.\n\u2022 Hyperparameter Optimisation: Leverage Autotrain\u2019s automated hyperparameter tuning to\nachieve optimal performance without manual intervention.\n\n10.1.3\n\nChallenges of Using Autotrain\n\n\u2022 Data Privacy: Ensuring the privacy and security of sensitive data during the fine-tuning process.\n\u2022 Resource Constraints: Managing computational resources effectively, especially in environments\nwith limited access to powerful hardware.\n\u2022 Model Overfitting: Avoiding overfitting by ensuring diverse and representative training data\nand using appropriate regularization techniques.\n\n10.1.4\n\nWhen to Use Autotrain\n\n1. Lack of Deep Technical Expertise: Ideal for individuals or small teams without extensive\nmachine learning or LLM background who need to fine-tune models quickly and effectively.\n2. Quick Prototyping and Deployment: Suitable for rapid development cycles where time is\ncritical, such as proof-of-concept projects or MVPs.\n3. Resource-Constrained Environments: Useful for scenarios with limited computational resources or where a quick turnaround is necessary.\nIn summary, Autotrain is an excellent tool for quick, user-friendly fine-tuning of LLMs for standard NLP\ntasks, especially in environments with limited resources or expertise. However, it may not be suitable\nfor highly specialised applications or those requiring significant customisation and scalability.\n\n10.1.5\n\nTutorials\n\n1. How To Create HuggingFace Custom AI Models Using AutoTrain\n2. Finetune models with HuggingFace AutoTrain\n\n10.2\n\nTransformers Library and Trainer API\n\nThe Transformers Library by HuggingFace stands out as a pivotal tool for fine-tuning large language\nmodels (LLMs) such as BERT, GPT-3, and GPT-4. This comprehensive library offers a wide array of\npre-trained models tailored for various LLM tasks, making it easier for users to adapt these models to\nspecific needs with minimal effort. Whether you\u2019re fine-tuning for tasks like sentiment analysis, text\nclassification, or generating customer support responses, the library simplifies the process by allowing\nseamless model selection from the HuggingFace Model Hub and straightforward customisation through\nits high-level APIs.\nCentral to the fine-tuning process within the Transformers Library is the Trainer API. This API includes\nthe Trainer class, which automates and manages the complexities of fine-tuning LLMs. After completing\ndata preprocessing, the Trainer class streamlines the setup for model training, including data handling,\noptimisation, and evaluation. Users only need to configure a few parameters, such as learning rate and\nbatch size, and the API takes care of the rest. However, it\u2019s crucial to note that running Trainer.train()\ncan be resource-intensive and slow on a CPU. For efficient training, a GPU or TPU is recommended.\nPlatforms like Google Colab provide free access to these resources, making it feasible for users without\nhigh-end hardware to fine-tune models effectively.\n\n80\n\n\fThe Trainer API also supports advanced features like distributed training and mixed precision, which\nare essential for handling the large-scale computations required by modern LLMs. Distributed training\nallows the fine-tuning process to be scaled across multiple GPUs or nodes, significantly reducing training\ntime. Mixed precision training, on the other hand, optimises memory usage and computation speed by\nusing lower precision arithmetic without compromising model performance. HuggingFace\u2019s dedication to\naccessibility is evident in the extensive documentation and community support they offer, enabling users\nof all expertise levels to fine-tune LLMs. This democratisation of advanced NLP technology empowers\ndevelopers and researchers to deploy sophisticated, fine-tuned models for a wide range of applications,\nfrom specialised language understanding to large-scale data processing.\n\n10.2.1\n\nLimitations of the Transformers Library and Trainer API\n\n\u2022 Limited Customisation for Advanced Users: While the Trainer API simplifies many aspects\nof training, it might not offer the deep customisation that advanced users or researchers might need\nfor novel or highly specialised applications.\n\u2022 Learning Curve: Despite the simplified API, there is still a learning curve associated with understanding and effectively using the Transformers Library and Trainer API, particularly for those\nnew to NLP and LLM.\n\u2022 Integration Limitations: The seamless integration and ease of use are often tied to the HuggingFace ecosystem, which might not be compatible with all workflows or platforms outside their\nenvironment.\nIn summary, the Transformers Library and Trainer API provide robust, scalable solutions for fine-tuning\nLLMs across a range of applications, offering ease of use and efficient training capabilities. However, users\nmust be mindful of the resource requirements and potential limitations in customisation and complexity\nmanagement.\n\n10.3\n\nOptimum: Enhancing LLM Deployment Efficiency\n\nOptimum6 is HuggingFace\u2019s tool designed to optimise the deployment of large language models (LLMs)\nby enhancing their efficiency across various hardware platforms. As LLMs grow in size and complexity,\ndeploying them in a cost-effective and performant manner becomes increasingly challenging. Optimum\naddresses these challenges by applying a range of hardware-specific optimisations, such as quantisation,\npruning, and model distillation, which reduce the model\u2019s size and improve inference speed without\nsignificantly affecting accuracy. The following are the key techniques supported by Optimum:\n\u2022 Quantisation: Quantisation is one of the key techniques supported by Optimum. This process involves converting the model\u2019s weights from high-precision floating-point numbers to lower-precision\nformats, such as int8 or float16. This reduction in precision decreases the model\u2019s memory footprint and computational requirements, enabling faster execution and lower power consumption,\nespecially on edge devices and mobile platforms. Optimum automates the quantisation process,\nmaking it accessible to users who may not have expertise in low-level hardware optimisation.\n\u2022 Pruning: Pruning is another critical optimisation strategy offered by Optimum. It involves identifying and removing less significant weights from the LLM, reducing its overall complexity and\nsize. This leads to faster inference times and lower storage needs, which are particularly beneficial\nfor deploying models in environments with limited computational resources. Optimum\u2019s pruning\nalgorithms carefully eliminate these redundant weights while maintaining the model\u2019s performance,\nensuring that it continues to deliver high-quality results even after optimisation.\n\u2022 Model Distillation: In addition to these techniques, Optimum supports model distillation, a\nprocess where a smaller, more efficient model is trained to replicate the behaviour of a larger, more\ncomplex model. This distilled model retains much of the knowledge and capabilities of the original\nwhile being significantly lighter and faster. Optimum provides tools to facilitate the distillation\nprocess, allowing users to create compact LLMs that are well-suited for real-time applications. By\noffering a comprehensive suite of optimisation tools, Optimum ensures that HuggingFace\u2019s LLMs\ncan be deployed effectively across a wide range of environments, from powerful cloud servers to\nresource-constrained edge devices.\n6 https://huggingface.co/docs/optimum/en/index\n\n81\n\n\f10.3.1\n\nBest Practices of Using Optimum\n\n\u2022 Understand Hardware Requirements: Assess the target deployment environment (e.g., edge\ndevices, cloud servers) to optimise model configuration accordingly.\n\u2022 Iterative Optimisation: Experiment with different optimisation techniques (quantisation levels,\npruning thresholds) to find the optimal balance between model size, speed, and accuracy.\n\u2022 Validation and Testing: Validate optimised models thoroughly to ensure they meet performance\nand accuracy requirements across different use cases.\n\u2022 Documentation and Support: Refer to HuggingFace\u2019s resources for detailed guidance on using\nOptimum\u2019s tools effectively, and leverage community support for troubleshooting and best practices\nsharing.\n\u2022 Continuous Monitoring: Monitor deployed models post-optimisation to detect any performance\ndegradation and adjust optimisation strategies as needed to maintain optimal performance over\ntime.\n\n10.3.2\n\nTutorials\n\n1. An Introduction to Using Transformers and Hugging Face\n\n10.4\n\nAmazon SageMaker JumpStart\n\nAmazon SageMaker JumpStart is a feature within the SageMaker ecosystem designed to simplify and\nexpedite the fine-tuning of large language models (LLMs). It provides users with a rich library of prebuilt models and solutions that can be quickly customised for various use cases. This tool is particularly\nvaluable for organisations looking to deploy NLP solutions efficiently without deep expertise in machine\nlearning or the extensive computational resources typically required for training LLMs from scratch. The\narchitecture depicted in Figure 10.2 outlines a comprehensive pipeline for the fine-tuning and deployment\nof large language models (LLMs) Utilising AWS services.\n\n10.4.1\n\nSteps Involved in Using JumpStart\n\n\u2022 Data Preparation and Preprocessing:\n\u2013 Data Storage: Begin by securely storing raw datasets in Amazon S3, AWS\u2019s scalable object\nstorage service.\n\u2013 Preprocessing: Utilise the EMR Serverless framework with Apache Spark for efficient data\npreprocessing. This step refines and prepares the raw data for subsequent model training and\nevaluation.\n\u2013 Data Refinement: Store the processed dataset back into Amazon S3 after preprocessing,\nensuring accessibility and readiness for the next stages.\n\u2022 Model Fine-Tuning with SageMaker JumpStart:\n\u2013 Model Selection: Choose from a variety of pre-built models and solutions available through\nSageMaker JumpStart\u2019s extensive library, tailored for tasks such as sentiment analysis, text\ngeneration, or customer support automation.\n\u2013 Fine-Tuning Execution: Utilise Amazon SageMaker\u2019s capabilities, integrated with SageMaker JumpStart, to fine-tune the selected model. This involves adjusting parameters and\nconfigurations to optimise the model\u2019s performance for specific use cases.\n\u2013 Workflow Simplification: Leverage pre-built algorithms and model templates provided by\nSageMaker JumpStart to streamline the fine-tuning workflow, reducing the time and effort\nrequired for deployment.\n\u2022 Model Deployment and Hosting:\n\n82\n\n\fFigure 10.2: A step-by-step workflow illustrating the Amazon SageMaker JumpStart process, starting\nfrom data preprocessing using EMR Serverless Spark to the fine-tuning of LLMs, and ending with model\ndeployment on Amazon SageMaker Endpoints. (adapted from [85])\n\u2013 Deployment Setup: Deploy the fine-tuned model using Amazon SageMaker\u2019s endpoint\ndeployment capabilities. This setup ensures that the model is hosted in a scalable environment\ncapable of handling real-time predictions efficiently.\n\u2013 Scalability: Benefit from AWS\u2019s infrastructure scalability, allowing seamless scaling of resources to accommodate varying workloads and operational demands.\n\u2013 Efficiency and Accessibility: Ensure that the deployed model is accessible via SageMaker\nendpoints, enabling efficient integration into production applications for real-time inference\ntasks.\n\n10.4.2\n\nBest Practices for Using JumpStart\n\n\u2022 Robust Data Management: Maintain secure and organised data storage practices in Amazon\nS3, facilitating efficient data access and management throughout the pipeline.\n\u2022 Cost-Effective Processing: Utilise serverless computing frameworks like EMR Serverless with\nApache Spark for cost-effective and scalable data preprocessing.\n\u2022 Optimised Fine-Tuning: Capitalise on SageMaker JumpStart\u2019s pre-built models and algorithms\nto expedite and optimise the fine-tuning process, ensuring optimal model performance without\n83\n\n\fextensive manual configuration.\n\u2022 Continuous Monitoring and Optimisation: Implement robust monitoring mechanisms postdeployment to track model performance metrics. This allows for timely optimisations and adjustments to maintain accuracy and efficiency over time.\n\u2022 Integration with AWS Services: Leverage AWS\u2019s comprehensive suite of services and integration capabilities to create end-to-end pipelines that ensure reliable and scalable deployment of\nlarge-scale language models across diverse operational environments.\n\n10.4.3\n\nLimitations of Using JumpStart\n\n\u2022 Limited Customisation: While JumpStart simplifies the process for common use cases, it may\noffer limited flexibility for highly specialised or complex applications that require significant customisation beyond the provided templates and workflows.\n\u2022 Dependency on AWS Ecosystem: JumpStart is tightly integrated with AWS services, which\nmay pose challenges for users who prefer or need to operate in multi-cloud environments or those\nwith existing infrastructure outside of AWS.\n\u2022 Resource Costs: Utilising SageMaker\u2019s scalable resources for fine-tuning LLMs, especially large\nmodels, can incur substantial costs, which might be a barrier for smaller organisations or those\nwith limited budgets.\n\n10.4.4\n\nTutorials\n\n1. Fine-Tuning LLaMA 2 with Amazon SageMaker JumpStart\n2. LLM Agents Using AWS SageMaker JumpStart Foundation Models\n\n10.5\n\nAmazon Bedrock\n\nAmazon Bedrock7 is a fully managed service designed to simplify access to high-performing foundation\nmodels (FMs) from top AI innovators like AI21 Labs, Anthropic, Cohere, Meta, Mistral AI, Stability\nAI, and Amazon. It provides a unified API that integrates these models and offers extensive capabilities\nfor developing secure, private, and responsible generative AI applications. With Amazon Bedrock, users\ncan effortlessly experiment with and assess leading FMs tailored to their specific needs. The service supports private customisation of models through fine-tuning and Retrieval Augmented Generation (RAG),\nenabling the creation of intelligent agents that leverage enterprise data and systems. Amazon Bedrock\u2019s\nserverless architecture allows for quick deployment, seamless integration, and secure customisation of\nFMs without the burden of infrastructure management, Utilising AWS tools to deploy these models into\napplications efficiently and securely.\n\n10.5.1\n\nSteps Involved in Using Amazon Bedrock\n\nAmazon Bedrock offers a streamlined workflow for deploying and fine-tuning LLMs, making it an ideal\nchoice for businesses looking to quickly integrate advanced AI capabilities into their operations. Here\u2019s\na high-level overview of how Bedrock operates:\n\u2022 Model Selection: Users start by choosing from a curated selection of foundation models available\nthrough Bedrock. These include models from AWS (like Amazon Titan) and third-party providers\n(such as Anthropic Claude and Stability AI).\n\u2022 Fine-Tuning:\n\u2013 Once a model is selected, users can fine-tune it to better fit their specific needs. This involves\nfeeding the model with domain-specific data or task-specific instructions to tailor its outputs.\n7 https://aws.amazon.com/bedrock/\n\n84\n\n\f\u2013 The fine-tuning process is handled via simple API calls, eliminating the need for extensive\nsetup or detailed configuration. Users provide their custom data, and Bedrock manages the\ntraining process in the background.\n\u2022 Deployment:\n\u2013 After fine-tuning, Bedrock takes care of deploying the model in a scalable and efficient manner.\nThis means that users can quickly integrate the fine-tuned model into their applications or\nservices.\n\u2013 Bedrock ensures that the model scales according to demand and handles performance optimisation, providing a seamless user experience.\n\u2022 Integration and Monitoring:\n\u2013 Bedrock integrates smoothly with other AWS services, allowing users to embed AI capabilities\ndirectly into their existing AWS ecosystem.\n\u2013 Users can monitor and manage the performance of their deployed models through AWS\u2019s\ncomprehensive monitoring tools, ensuring that the models continue to perform optimally.\n\n10.5.2\n\nLimitations of Using Amazon Bedrock\n\nWhile Amazon Bedrock offers a robust suite of tools and services for addressing certain AI challenges,\nit is not a comprehensive solution for all AI needs. One key limitation is that it does not eliminate the\nrequirement for human expertise. Organisations still need skilled professionals who understand the intricacies of AI technology to effectively develop, fine-tune, and optimise the models provided by Bedrock.\nAdditionally, Amazon Bedrock is not designed to function as a standalone service. It relies on integration\nwith other AWS services, such as Amazon S3 for data storage, AWS Lambda for serverless computing,\nand AWS SageMaker for machine learning model development. Therefore, businesses leveraging Amazon\nBedrock will also need to use these complementary AWS services to fully realise its potential. This\ninterconnectedness means that while Amazon Bedrock enhances the AI capabilities within an AWS\necosystem, it may present a steep learning curve and require significant infrastructure management for\nthose new to AWS.\n\n10.5.3\n\nTutorials\n\n1. Finetuning LLMs on Amazon Bedrock\n2. Amazon Bedrock for Generative AI\n\n10.6\n\nOpenAI\u2019s Fine-Tuning API\n\nOpenAI\u2019s Fine-Tuning API is a comprehensive platform that facilitates the customisation of OpenAI\u2019s\npre-trained LLMs to cater to specific tasks and domains. This service is designed to be user-friendly,\nenabling a broad range of users, from businesses to individual developers, to harness the power of\nadvanced AI without the complexities typically associated with model training and deployment.\n\n10.6.1\n\nSteps Involved in Using OpenAI\u2019s Fine-Tuning API\n\n\u2022 Model Selection:\n\u2013 Choosing a Pre-Trained Model: Users begin by selecting a base model from OpenAI\u2019s\nextensive lineup. This includes powerful models like GPT-4, which offer a robust starting\npoint for a wide range of language processing tasks.\n\u2013 Customisable Base: These models come pre-trained with vast amounts of data, providing\na solid foundation that can be further refined to suit specific requirements.\n\u2022 Data Preparation and Upload:\n\n85\n\n\f\u2013 Curating Relevant Data: Users need to gather and prepare a dataset that reflects the\nspecific task or domain they wish to fine-tune the model for. This data is crucial for teaching\nthe model to perform the desired function more effectively.\n\u2013 Uploading Data to the API: The Fine-Tuning API facilitates easy data upload. Users\ncan feed their curated datasets into the API through straightforward commands, making the\nprocess accessible even to those with limited technical backgrounds.\n\u2022 Initiating Fine-Tuning:\n\u2013 Automated Process: Once the data is uploaded, OpenAI\u2019s infrastructure handles the finetuning process. The API adjusts the model\u2019s parameters based on the new data to improve\nperformance on the specified tasks.\n\u2022 Deploying the Fine-Tuned Model:\n\u2013 API Integration: The fine-tuned model can be accessed and deployed via OpenAI\u2019s API.\nThis allows for seamless integration into various applications, such as chatbots, automated\ncontent creation tools, or specialised customer service systems.\n\n10.6.2\n\nLimitations of OpenAI\u2019s Fine-Tuning API\n\n\u2022 Pricing Models: Fine-tuning and using OpenAI\u2019s models through the API can be costly, especially for large-scale deployments or continuous usage. This can be a significant consideration for\nsmaller organisations or budget-constrained projects.\n\u2022 Data Privacy and Security: Users must upload their data to OpenAI\u2019s servers for the finetuning process. This raises potential concerns about data privacy and the security of sensitive or\nproprietary information.\n\u2022 Dependency on OpenAI Infrastructure: The reliance on OpenAI\u2019s infrastructure for model\nhosting and API access can lead to vendor lock-in, limiting flexibility and control over the deployment environment.\n\u2022 Limited Control Over Training Process: The fine-tuning process is largely automated and\nmanaged by OpenAI, offering limited visibility and control over the specific adjustments made to\nthe model.\n\n10.6.3\n\nTutorials\n\n1. Fine-Tuning GPT-3 Using the OpenAI API\n\n10.7\n\nNVIDIA NeMo Customizer\n\nNVIDIA NeMo Customiser8 is part of the NeMo framework, a suite of tools and models designed by\nNVIDIA to facilitate the development and fine-tuning of LLM models. The Customiser focuses specifically on making it easier to fine-tune large language models (LLMs) for specialised tasks and domains.\nLike other fine-tuning tools, NeMo Customiser is geared toward users who want to adapt pre-trained\nmodels for specific applications, such as conversational AI, translation, or domain-specific text generation. It delivers enterprise-ready models by offering accurate data curation, extensive customisation\noptions, retrieval-augmented generation (RAG), and improved performance features. The platform supports training and deploying generative AI models across diverse environments, including cloud, data\ncenter, and edge locations. It provides a comprehensive package with support, security, and reliable APIs\nas part of the NVIDIA AI Enterprise.\n8 https://developer.nvidia.com/blog/fine-tune-and-align-llms-easily-with-nvidia-nemo-customizer/\n\n86\n\n\f10.7.1\n\nKey Features of NVIDIA NeMo\n\nNVIDIA NeMo is designed to enhance AI projects with several standout features.[86]\n\u2022 State-of-the-Art Training Techniques NeMo employs GPU-accelerated tools like NeMo Curator for preparing large-scale, high-quality datasets. These tools facilitate efficient pretraining of\ngenerative AI models by leveraging thousands of compute cores, which significantly reduces training\ntime and enhances the accuracy of large language models (LLMs).\n\u2022 Advanced Customisation for LLMs The NeMo Customiser microservice allows for precise finetuning and alignment of LLMs for specific domains. It uses model parallelism to speed up training\nand supports scaling across multiple GPUs and nodes, enabling the fine-tuning of larger models.\n\u2022 Optimised AI Inference with NVIDIA Triton NeMo includes NVIDIA Triton Inference Server\nto streamline AI inference at scale. This integration accelerates generative AI inference, ensuring\nconfident deployment of AI applications both on-premises and in the cloud.\n\u2022 User-Friendly Tools for Generative AI NeMo features a modular, reusable architecture that\nsimplifies the development of conversational AI models. It supports comprehensive workflows from\ndata processing to deployment and includes pre-trained models for automatic speech recognition\n(ASR), natural language processing (NLP), and text-to-speech (TTS), which can be fine-tuned or\nused as-is.\n\u2022 Best-in-Class Pretrained Models NeMo Collections offer a variety of pre-trained models and\ntraining scripts, facilitating rapid application development or fine-tuning for specific tasks. Currently, NeMo supports models like Llama 2, Stable Diffusion, and NVIDIA\u2019s Nemotron-3 8B family.\n\u2022 Optimised Retrieval-Augmented Generation NeMo Retriever delivers high-performance, lowlatency information retrieval, enhancing generative AI applications with enterprise-grade retrievalaugmented generation (RAG) capabilities. This feature supports real-time business insights and\ndata Utilisation.\n\n10.7.2\n\nComponents of NVIDIA NeMo\n\n\u2022 NeMo Core Provides essential elements like the Neural Module Factory for training and inference,\nstreamlining the development of conversational AI models.\n\u2022 NeMo Collections Offers specialised modules and models for ASR, NLP, and TTS, including\npre-trained models and training scripts, making the platform versatile.\n\u2022 Neural Modules Serve as the building blocks of NeMo, defining trainable components such as\nencoders and decoders, which can be connected to create comprehensive models.\n\u2022 Application Scripts Simplify the deployment of conversational AI models with ready-to-use\nscripts, enabling quick training or fine-tuning on specific datasets for various AI applications.\n\n10.7.3\n\nCustomising Large Language Models (LLMs)\n\nWhile general-purpose LLMs, enhanced with prompt engineering or light fine-tuning, have enabled organisations to achieve successful proof-of-concept projects, transitioning to production presents additional\nchallenges. Figure 10.3 illustrates NVIDIA\u2019s detailed LLM customisation lifecycle, offering valuable\nguidance for organisations that are preparing to deploy customised models in a production environment\n[87].\n1. Model Selection or Development\nNVIDIA provides a range of pre-trained models, from 8B to 43B parameters, and supports the\nintegration of other open-source models of any size. Alternatively, users can develop their own\nmodels, starting with data curation, which includes selecting, labeling, cleansing, validating, and\nintegrating data. This process, better termed data engineering, involves additional analysis, designing storage, evaluating model training results, and incorporating reinforcement learning with\nhuman feedback (RLHF). While building a custom foundation model is often costly, complex, and\ntime-consuming, most enterprises opt to start with a pre-trained model and focus on customisation.\n\n87\n\n\fFigure 10.3: Nvidia NeMo Framework for Customising and Deploying LLMs. The Nvidia NeMo framework is designed for end-to-end customisation and deployment of large language models (LLMs). This\ndiagram illustrates the process from data curation and distributed training of foundation models, through\nmodel customisation, to accelerated inference with guardrails. The platform enables AI developers to\nintegrate in-domain, secure, and cited responses into enterprise applications, ensuring that LLMs are\neffectively tailored for specific tasks and industries. The NeMo framework, supported by Nvidia AI Enterprise, also offers robust support for various pre-trained foundation models like OpenAI\u2019s GPT family,\nensuring scalability and reliability in AI deployments. (adapted from [87])\n2. Model Customisation\nModel customisation involves optimising performance with task-specific datasets and adjusting\nmodel weights. NeMo offers recipes for customisation, and enterprises can choose models already\ntailored to specific tasks and then fine-tune them with proprietary data.\n3. Inference\nInference refers to running models based on user queries. This phase involves considering hardware,\narchitecture, and performance factors that significantly impact usability and cost in production.\n4. Guardrails\nNVIDIA employs guardrails as intermediary services between models and applications. These\nservices review incoming prompts for policy compliance, execute arbitration or orchestration steps,\nand ensure model responses adhere to policies. Guardrails help maintain relevance, accuracy, safety,\nprivacy, and security.\n5. Applications\nNVIDIA\u2019s framework presents enterprise applications as LLM-ready, though this is not always\nthe case. Existing applications may be connected to LLMs to enable new features. However,\ncreating assistants for knowledge access or task execution often involves designing new applications\nspecifically for natural language interfaces.\n\n10.7.4\n\nTutorials\n\n1. Introduction to NVIDIA NeMo \u2014 Tutorial and Example\n2. How to fine-tune a Riva NMT Bilingual model with Nvidia NeMo\n\n88\n\n\fChapter 11\n\nMultimodal LLMs and their\nFine-tuning\nA multimodal model is a machine learning model that can process information from various modalities,\nsuch as images, videos, and text. For instance, Google\u2019s multimodal model, Gemini[88], can analyse a\nphoto of a plate of cookies and produce a written recipe in response, and it can perform the reverse as well.\nThe difference between Generative AI and Multimodal AI is that generative AI refers to the use of\nmachine learning models to create new content, such as text, images, music, audio, and videos, typically\nfrom a single type of input. Multimodal AI extends these generative capabilities by processing information from multiple modalities, including images, videos, and text. This enables the AI to understand\nand interpret different sensory modes, allowing users to input various types of data and receive a diverse\nrange of content types in return.\n\nFigure 11.1: Timeline of Multimodal Model Developments \u2014 This figure illustrates the progression\nof significant multimodal models, highlighting key releases from major tech companies and research\ninstitutions from December 2023 to March 2024. The timeline showcases models like Google\u2019s TinyGPTV and Gemini Nano, along with other innovations such as MoE-LLAVA, DeepSeek-VL, and LLAVAGemma, indicating the rapid advancement in multimodal AI technologies (adapted from [89]).\n\n89\n\n\f11.1\n\nVision Language Model (VLMs)\n\nVision language models encompass multimodal models capable of learning from both images and text\ninputs. They belong to the category of generative models that utilise image and text data to produce\ntextual outputs. These models, especially at larger scales, demonstrate strong zero-shot capabilities,\nexhibit robust generalisation across various tasks, and effectively handle diverse types of visual data such\nas documents and web pages. Typical applications include conversational interactions involving images,\nimage interpretation based on textual instructions, answering questions related to visual content, understanding documents, generating captions for images, and more. Certain advanced vision language models\ncan also understand spatial attributes within images. They can generate bounding boxes or segmentation\nmasks upon request to identify or isolate specific subjects, localise entities within images, or respond to\nqueries regarding their relative or absolute positions. The landscape of large vision language models is\ncharacterised by considerable diversity in training data, image encoding techniques, and consequently,\ntheir functional capabilities.\n\n11.1.1\n\nArchitecture\n\nVision-language models adeptly integrate both visual and textual information, leveraging three fundamental components:\n\u2022 Image Encoder: This component translates visual data (images) into a format that the model\ncan process.\n\u2022 Text Encoder: Similar to the image encoder, this component converts textual data (words and\nsentences) into a format the model can understand.\n\u2022 Fusion Strategy: This component combines the information from both the image and text encoders, merging the two data types into a unified representation.\nThese elements work collaboratively, with the model\u2019s learning process (loss functions) specifically tailored to the architecture and learning strategy employed. Although the concept of vision-language models is not new, their construction has evolved significantly. Early models used manually crafted image\ndescriptions and pre-trained word vectors. Modern models, however, utilise transformers\u2014an advanced\nneural network architecture\u2014for both image and text encoding. These encoders can learn features either\nindependently or jointly.\nA crucial aspect of these models is pre-training. Before being applied to specific tasks, the models are\ntrained on extensive datasets using carefully selected objectives. This pre-training equips them with the\nfoundational knowledge required to excel in various downstream applications. Following is one of the\nexample architectures of VLMs.\n\n11.1.2\n\nContrastive Learning\n\nContrastive learning is a technique that focuses on understanding the differences between data points. It\ncomputes a similarity score between instances and aims to minimise contrastive loss, making it particularly useful in semi-supervised learning where a limited number of labelled samples guide the optimisation\nprocess to classify unseen data points.\nHow it works\nFor instance, to recognise a cat, contrastive learning compares a cat image with a similar cat image and\na dog image. The model learns to distinguish between a cat and a dog by identifying features such as\nfacial structure, body size, and fur. By determining which image is closer to the \u201danchor\u201d image, the\nmodel predicts its class.\nCLIP is a model that utilises contrastive learning to compute similarity between text and image embeddings through textual and visual encoders. It follows a three-step process for zero-shot predictions:\n\u2022 Pre-training: Trains a text and image encoder to learn image-text pairs.\n\u2022 Caption Conversion: Converts training dataset classes into captions.\n\u2022 Zero-Shot Prediction: Estimates the best caption for a given input image based on learned\nsimilarities.\n90\n\n\fFigure 11.2: Workflow of Contrastive Pre-Training for Multimodal Models. This figure illustrates the\nprocess of contrastive pre-training where text and image encoders are trained to align representations\nfrom both modalities. Step 1 involves contrastive pre-training by pairing text and image data, while\nStep 2 showcases the creation of a dataset classifier using label text encoded by the text encoder. Step\n3 demonstrates the model\u2019s application for zero-shot prediction by leveraging the pre-trained text and\nimage encoders. This method enables the model to generalise across various tasks without requiring\ntask-specific fine-tuning (adopted from [90]).\n\n11.2\n\nFine-tuning of multimodal models\n\nFor fine-tuning a Multimodal Large Language Model (MLLM), PEFT techniques such as LoRA and\nQLoRA can be utilised. The process of fine-tuning for multimodal applications is analogous to that for\nlarge language models, with the primary difference being the nature of the input data. In addition to\nLoRA, which employs matrix factorisation techniques to reduce the number of parameters, other tools\nsuch as LLM-Adapters and (IA)\u00b3[91] can be effectively used. LLM-Adapters integrate various adapter\nmodules into the pre-trained model\u2019s architecture, enabling parameter-efficient fine-tuning for diverse\ntasks by updating only the adapter parameters while keeping the base model parameters fixed. (IA)\u00b3,\nor Infused Adapters by Inhibiting and Amplifying Inner Activations, enhances performance by learning vectors to weight model parameters through activation multiplications, supporting robust few-shot\nperformance and task mixing without manual adjustments. Moreover, dynamic adaptation techniques\nlike DyLoRA[92] allow for the training of low-rank adaptation blocks across different ranks, optimising\nthe learning process by sorting the representations during training. LoRA-FA[93], a variant of LoRA,\noptimises the fine-tuning process by freezing the first low-rank matrix after initialisation and using it as a\nrandom projection while training the other, thereby reducing the number of parameters by half without\ncompromising performance.\nThe Efficient Attention Skipping (EAS)[94] module introduces a novel parameter and computationefficient tuning method for MLLMs, aiming to maintain high performance while reducing parameter and\ncomputation costs for downstream tasks. However, MemVP[95] critiques this approach, noting that it\nstill increases the input length of language models. To address this, MemVP integrates visual prompts\nwith the weights of Feed Forward Networks, thereby injecting visual knowledge to decrease training time\nand inference latency, ultimately outperforming previous PEFT methods.\n\n11.2.1\n\nFull-parameter Fine-Tuning\n\nMethods such as those introduced by LOMO[96] and MeZO[97] provide alternative solutions by focusing\non memory efficiency. LOMO utilises a low-memory optimisation technique derived from Stochastic\nGradient Descent (SGD), reducing memory consumption typically associated with the ADAM optimiser.\nMeZO, on the other hand, offers a memory-efficient optimiser that requires only two forward passes\nto compute gradients, enabling comprehensive fine-tuning of large models with a memory footprint\nequivalent to inference [89].\n\n91\n\n\f11.2.2\n\nCase study of fine-tuning MLLMs for Medical domain\n\nThe following section provides a case study on fine-tuning MLLMs for the Visual Question Answering\n(VQA) task. In this example, we present a PEFT for fine-tuning MLLM specifically designed for MedVQA applications. To ensure accurate performance measurement, human evaluations were conducted,\ndemonstrating that the model achieves an overall accuracy of 81.9% and surpasses the GPT-4v model\nby a substantial margin of 26% in absolute accuracy on closed-ended questions.\nThe model consists of three components: the vision encoder, a pre-trained Large Language Model (LLM)\nfor handling multimodal inputs and generating responses, and a single linear layer for projecting embeddings from the visual encoding space to the LLM space, as shown in figure 11.3.\nThe Vision Transformer (ViT) type backbone, EVA, encodes image tokens into visual embeddings,\nwith model weights remaining frozen during the fine-tuning process. The technique from MiniGPT-v2\nis utilised, grouping four consecutive tokens into one visual embedding to efficiently reduce resource\nconsumption by concatenating on the embedding dimension.\nThese grouped visual tokens are then processed through the projection layer, resulting in embeddings\n(length 4096) in the LLM space. A multimodal prompt template integrates both visual and question\ninformation, which is input into the pre-trained LLM, LLaMA2-chat(7B), for answer generation. The\nlow-rank adaptation (LoRA) technique is applied for efficient fine-tuning, keeping the rest of the LLM\nfrozen during downstream fine-tuning. A beam search with a width of 1 is utilised.\n\nFigure 11.3: Overview of Med VQA architecture integrating LoRA and a pre-trained LLM with a Vision\nEncoder for medical visual question answering tasks. The architecture includes stages for processing\nimages and generating contextually relevant responses, demonstrating the integration of vision and language models in a medical setting (adopted from [98]).\nThe multimodal prompt includes input images, questions, and a specific token for VQA tasks, following\nthe MiniGPT-v2 template. In Figure 11.3, the image features derived from linear projection are labelled\nas ImageFeature, with the corresponding questions serving as text instructions. The special token [VQA]\nis used as the task identifier, forming the complete multimodal instructional template:\n92\n\n\f[INST]<img><ImageFeature></img>[VQA] Instruction [/INST].\nModel Training\nWeights from MiniGPT-v2, pre-trained on general domain datasets, are further fine-tuned using multimodal medical datasets in two stages. The LoRA technique is employed for efficient fine-tuning, updating\nonly a small portion of the entire model, as detailed below:\n\u2022 Fine-tuning with image captioning: During this stage, the model is fine-tuned using the ROCO\nmedical image-caption dataset, which contains medical image-caption pairs of varying lengths. The\nprompt template used is <Img><ImageHere></Img>[caption] <instruction>, with the instruction prompt randomly selected from a pool of four candidates, such as \u201cBriefly describe this image.\u201d\nDuring training, only the linear projection layer and the LoRA layer in the LLM are fine-tuned,\nwhile other parts of the model remain frozen.\n\u2022 Fine-tuning on VQA: In the second stage, the model is fine-tuned on the Med-VQA dataset,\nVQA-RAD, which contains triplets of images, questions, and answers. Following the instruction\ntemplate proposed in MiniGPT-v2, the template used is: \u201c[INST] <img><ImageFeature></img>[VQA]\nInstruction [/INST]\u201d, where the instruction prompt is: \u201cBased on the image, respond to this\nquestion with a short answer: question,\u201d with question signifying the question corresponding to\nthe given medical image. The motivation for generating short answers is to validate against the\nexisting labelled data in VQA-RAD, where the answers are typically short in both open-ended and\nclosed-ended QA pairs. Similar to the first stage, the vision encoder and the LLM remain frozen\nwhile only the linear projection and LoRA layers in the LLM are updated.\n\n11.3\n\nApplications of Multimodal models\n\n1. Gesture Recognition - These models interpret and recognise human gestures, which is crucial\nfor sign language translation. Multimodal models facilitate inclusive communication by processing\ngestures and converting them into text or speech.\n2. Video Summarisation - Multimodal models can summarise lengthy videos by extracting key visual and audio elements. This capability streamlines content consumption, enables efficient content\nbrowsing, and enhances video content management platforms.\n3. DALL-E is a notable example of multimodal AI that generates images from textual descriptions.\nThis technology expands creative possibilities in content creation and visual storytelling, with\napplications in art, design, advertising, and more.\n4. Educational Tools - Multimodal models enhance learning experiences by providing interactive\neducational content that responds to both visual and verbal cues from students. They are integral\nto adaptive learning platforms that adjust content and difficulty based on student performance and\nfeedback.\n5. Virtual Assistants - Multimodal models power virtual assistants by understanding and responding to voice commands while processing visual data for comprehensive user interaction. They are\nessential for smart home automation, voice-controlled devices, and digital personal assistants.\n\n11.4\n\nAudio or Speech LLMs Or Large Audio Models\n\nAudio or speech LLMs are models designed to understand and generate human language based on audio\ninputs. They have applications in speech recognition, text-to-speech conversion, and natural language\nunderstanding tasks. These models are typically pre-trained on large datasets to learn generic language\npatterns, which are then fine-tuned on specific tasks or domains to enhance performance.\nAudio and Speech Large Language Models (LLMs) represent a significant advancement in the integration\nof language processing with audio signals. These models leverage a robust Large Language Model as a\nfoundational backbone, which is enhanced to handle multimodal data through the inclusion of custom\naudio tokens. This transformation allows the models to learn and operate within a shared multimodal\nspace, where both text and audio signals can be effectively processed.\n\n93\n\n\fUnlike text, which is inherently discrete, audio signals are continuous and need to be discretized into\nmanageable audio tokens. Techniques like HuBERT[99] and wav2vec[100] are employed for this purpose,\nconverting audio into a tokenized format that the LLM can process alongside text. The model, typically\nautoregressive and decoder-based, is pre-trained using a combination of self-supervised tasks, such as\npredicting masked tokens in interleaved text and audio, and supervised fine-tuning for specific tasks like\ntranscription or sentiment analysis. This capability to handle and generate audio and text simultaneously allows for a wide range of applications, from audio question answering to speech-based sentiment\ndetection, making Audio and Speech LLMs a versatile tool in multimodal AI. The figure 11.4 illustrates\nan example of a multimodal Audio LM architecture. In this setup, a prompt provides instructions in\nboth text and audio formats. The audio is tokenized using an audio tokenizer. The multimodal model\nthen combines these text and audio tokens and generates spoken speech through a vocoder (also known\nas a voice decoder).\n\nFigure 11.4: Multimodal Audio-Text Language Model architecture that integrates text and audio inputs for advanced multimodal processing. The architecture utilises text tokenizers and audio encoders/tokenizers to convert inputs into tokens, which are then processed by the audio-text LM. This\nmodel supports both discrete and continuous speech processing and enables tasks such as sentiment analysis and response generation in natural language. The audio tokens are further refined using a vocoder,\nwhile text tokens are detokenized to produce coherent text outputs (adapted from [101]).\n\n94\n\n\fAudio and speech LLMs like AudioPaLM[102], AudioLM[103], and various adaptations of models like\nWhisper and LLaMA, integrate capabilities for understanding and generating audio data, including\nspeech-to-text (STT), text-to-speech (TTS), and speech-to-speech (STS) translation. These models\nhave shown that LLMs, initially designed for text, can be effectively adapted for audio tasks through\nsophisticated tokenization and fine-tuning techniques.\n\n11.4.1\n\nTokenization and Preprocessing\n\nA key aspect of adapting LLMs for audio is the tokenization of audio data into discrete representations\nthat the model can process. For instance, AudioLM and AudioPaLM utilise a combination of acoustic\nand semantic tokens. Acoustic tokens capture the high-quality audio synthesis aspect, while semantic\ntokens help maintain long-term structural coherence in the generated audio. This dual-token approach\nallows the models to handle both the intricacies of audio waveforms and the semantic content of speech.\n\n11.4.2\n\nFine-Tuning Techniques\n\nFine-tuning audio and speech LLMs typically involve several key strategies:\n\u2022 Full Parameter Fine-Tuning: This involves updating all the model\u2019s parameters during finetuning. For instance, LauraGPT and SpeechGPT fine-tune all parameters to adapt pre-trained\ntext LLMs to various audio tasks, although this can be computationally expensive.\n\u2022 Layer-Specific Fine-Tuning: Techniques like LoRA (Low-Rank Adaptation) update only specific layers or modules of the model. This method significantly reduces computational requirements\nwhile still allowing effective adaptation. Models like Qwen-Audio leverage LoRA to fine-tune pretrained components for enhanced performance on speech recognition tasks.\n\u2022 Component-Based Fine-Tuning: Recent models, such as those integrating the Whisper encoder, freeze certain parts of the model (like the speech encoder) and only fine-tune a linear\nprojector or specific adapters to align the speech and text modalities. This approach simplifies the\ntraining process and enhances efficiency[104].\n\u2022 Multi-Stage Fine-Tuning: Models like AudioPaLM perform multi-stage fine-tuning, starting\nwith a text-based pre-training phase, followed by fine-tuning on a mixture of tasks that include\nboth text and audio data. This staged approach leverages the strengths of pre-trained text models\nwhile adapting them for multimodal tasks.\n\n11.4.3\n\nFine-Tuning Whisper for Automatic Speech Recognition (ASR)\n\nWhisper1 is an advanced Automatic Speech Recognition (ASR) model developed by OpenAI, designed\nto convert spoken language into text. Built upon the powerful Transformer architecture, Whisper excels\nat capturing and transcribing diverse speech patterns across various languages and accents. Unlike\ntraditional ASR models that require extensive labelled data, Whisper leverages a vast dataset and selfsupervised learning, enabling it to perform robustly in noisy environments and handle a wide range of\nspeech variations. Its versatility and high accuracy make it an ideal choice for applications such as voice\nassistants, transcription services, and multilingual speech recognition systems.\nWhy Fine-Tune Whisper?\nFine-tuning Whisper for specific ASR tasks can significantly enhance its performance in specialised\ndomains. Although Whisper is pre-trained on a large and diverse dataset, it might not fully capture\nthe nuances of specific vocabularies or accents present in niche applications. Fine-tuning allows Whisper\nto adapt to particular audio characteristics and terminologies, leading to more accurate and reliable\ntranscriptions. This process is especially beneficial in industries with domain-specific jargon, like medical,\nlegal, or technical fields, where the generic model might struggle with specialised vocabulary.\n1 https://openai.com/index/whisper/\n\n95\n\n\fSteps to Fine-Tune Whisper\n\u2022 Data Collection and Preparation: Gather a sizable dataset that matches the target domain or\ntask. Ensure the dataset includes diverse examples with clear transcriptions. Clean and preprocess\nthe audio files and transcripts, ensuring they are in a consistent format and aligned correctly. Tools\nlike FFmpeg2 can help standardise audio formats and sample rates.\n\u2022 Data Augmentation: To improve robustness, augment the dataset with variations such as different noise levels, accents, or speeds. Techniques like adding background noise, altering pitch, or\nchanging the tempo can help the model generalise better to real-world conditions.\n\u2022 Preprocessing: Convert the audio files into a format suitable for Whisper, typically into mel\nspectrograms or another time-frequency representation. This transformation is crucial as Whisper\nrelies on such representations to learn and transcribe speech effectively.\n\u2022 Model Configuration: Initialise the Whisper model with pre-trained weights. Configure the\nmodel to accommodate the target language or domain-specific adjustments. This includes setting\nappropriate hyperparameters, like learning rate and batch size, tailored to the dataset\u2019s size and\ncomplexity.\n\u2022 Training: Fine-tune the Whisper model on the prepared dataset using a framework like PyTorch\nor TensorFlow. Ensure to monitor the model\u2019s performance on a validation set to avoid overfitting.\nTechniques like gradient clipping, learning rate scheduling, and early stopping can help maintain\ntraining stability and efficiency.\n\u2022 Evaluation and Testing: After training, evaluate the model\u2019s performance on a separate test\nset to assess its accuracy and generalisability. Metrics like Word Error Rate (WER) or Character\nError Rate (CER) provide insights into how well the model transcribes audio compared to ground\ntruth transcriptions.\n\n11.4.4\n\nCase Studies and Applications\n\n1. Medical Transcription: Fine-tuning speech LLMs on medical data has led to significant improvements in transcribing doctor-patient interactions. Models like Whisper have been fine-tuned\non medical terminologies, resulting in more accurate and reliable transcriptions.\n2. Legal Document Processing: Legal firms have employed fine-tuned audio LLMs to transcribe\ncourt proceedings and legal discussions. Domain-specific fine-tuning has enhanced the models\u2019\nability to recognise and accurately transcribe legal jargon.\n3. Customer Service Automation: Companies are using fine-tuned speech models to automate\ncustomer service interactions. These models are trained on customer support data to understand\nand respond to queries more effectively, providing a more seamless user experience.\n\n2 https://ffmpeg.org/ffmpeg.html\n\n96\n\n\fChapter 12\n\nOpen Challenges and Research\nDirections\n12.1\n\nScalability Issues\n\nThe fine-tuning of Large Language Models (LLMs) such as GPT-4, PaLM1 , and T52 has become a critical\narea of research, presenting several significant challenges and opening up new avenues for exploration,\nparticularly in scaling these processes efficiently. This discussion focuses on the two main aspects: the\nchallenges in scaling fine-tuning processes and potential research directions for scalable solutions.\n\n12.1.1\n\nChallenges in Scaling Fine-Tuning Processes\n\n1. Computational Resources: Large-scale models such as GPT-3 and PaLM require enormous\ncomputational resources for fine-tuning. For instance, fine-tuning a 175-billion parameter model\nlike GPT-3 necessitates high-performance GPUs or TPUs capable of handling vast amounts of data\nand complex operations. The sheer volume of parameters translates to extensive computational\ndemands. Even a relatively smaller model, such as BERT-large with 340 million parameters, can\nbe computationally intensive to fine-tune.\n2. Memory Requirements: The memory footprint for fine-tuning LLMs is staggering. Each parameter in the model requires storage, and during training, additional memory is needed to store\nintermediate computations, gradients, and optimiser states. For example, loading a 7 billion parameter model (e.g., LLaMA 2) in FP32 (4 bytes per parameter) requires approximately 28 GB\nof GPU memory, while fine-tuning demands around 112 GB of GPU memory[105]. This memory\ndemand is beyond the capability of most consumer-grade hardware, making fine-tuning accessible\nprimarily to well-funded organisations or research institutions.\n3. Data Volume: LLMs typically require vast amounts of training data to achieve state-of-the-art\nperformance during fine-tuning. This data needs to be loaded, preprocessed, and fed into the model\nat high speeds to maintain efficient training. Managing large datasets can become a bottleneck,\nespecially if the data is stored in a distributed fashion across multiple systems or if it needs to be\nfetched from remote storage.\n4. Throughput and Bottlenecks: High throughput is essential to keep GPUs or TPUs fully\nutilised. However, data pipelines can become bottlenecks if not properly optimised. For example, shuffling large datasets or loading them into memory quickly enough to keep up with the\ntraining process can be challenging. Techniques like data packing, where multiple small examples\nare combined into larger batches, help improve throughput but add complexity to data handling\nroutines.[106]\n5. Efficient Use of Resources: The financial and environmental costs of fine-tuning large models\nare significant. Large-scale fine-tuning involves not just the direct cost of computational resources\nbut also the indirect costs associated with energy consumption and infrastructure maintenance.\n1 https://ai.google/discover/palm2/\n2 https://huggingface.co/docs/transformers/en/model_doc/t5\n\n97\n\n\fTechniques such as mixed-precision training and gradient checkpointing can reduce these costs by\noptimising memory and computational efficiency.\nThe challenges in scaling the fine-tuning processes of LLMs are multifaceted and complex, involving significant computational, memory, and data handling constraints. Innovations in PEFT, data throughput\noptimisation, and resource-efficient training methods are critical for overcoming these challenges. As\nLLMs continue to grow in size and capability, addressing these challenges will be essential for making\nadvanced AI accessible and practical for a wider range of applications.\n\n12.1.2\n\nResearch Directions for Scalable Solutions\n\nAdvanced PEFT Techniques and Sparse Fine-Tuning\nRecent advancements in PEFT techniques, like LoRA and its variant, Quantised LoRA, are revolutionising the scalability of LLMs. LoRA reduces the computational burden by updating only a low-rank\napproximation of the parameters, significantly lowering memory and processing requirements. Quantised\nLoRA further optimises resource usage by applying quantisation to these low-rank matrices, maintaining\nhigh model performance while minimising the need for extensive hardware. This has enabled efficient\nfine-tuning of massive models, such as in Meta\u2019s LLaMA project, where adapting a smaller set of influential parameters allowed the models to perform robustly across various tasks with less computational\nstrain.\nSparse fine-tuning techniques, such as SpIEL [107] complement these efforts by selectively updating\nonly the most impactful parameters. SpIEL fine-tunes models by only changing a small portion of the\nparameters, which it tracks with an index. The process includes updating the parameters, removing the\nleast important ones, and adding new ones based on their gradients or estimated momentum using an\nefficient optimiser.\nData Efficient Fine-Tuning (DEFT)\nTo address the scalability challenges, recently the concept of DEFT has emerged. This novel approach\nintroduces data pruning as a mechanism to optimise the fine-tuning process by focusing on the most\ncritical data samples.\nDEFT aims to enhance the efficiency and effectiveness of fine-tuning LLMs by selectively pruning the\ntraining data to identify the most influential and representative samples. This method leverages few-shot\nlearning principles, enabling LLMs to adapt to new data with minimal samples while maintaining or even\nexceeding performance levels achieved with full datasets [108].\nKey Components of DEFT\nHigh Accuracy Through Influence Score: DEFT introduces the concept of an influence score to\nevaluate and rank the importance of each data sample in the context of LLM fine-tuning. The influence\nscore estimates how removing a specific sample would impact the overall performance of the model. This\napproach allows for the selection of a small subset of data that is highly representative and influential,\nthereby enabling the model to maintain high accuracy with significantly fewer samples.\nHigh Efficiency Through Effort Score and Surrogate Models: To address the cost and complexity\nof evaluating large datasets, DEFT employs a surrogate model\u2014a smaller, computationally less intensive\nmodel\u2014to approximate the influence scores. This surrogate model helps estimate the impact of each\nsample without the heavy computational burden associated with directly using the LLM. Additionally,\nDEFT introduces an effort score to identify and prioritise more challenging samples that may require\nspecial attention from the LLM. This dual-score system ensures that the fine-tuning process remains\nboth efficient and effective.\nPractical Implications and Use Cases\n\u2022 Few-Shot Fine-Tuning for Rapid Adaptation: DEFT is particularly beneficial for applications where models need to quickly adapt to new data with minimal samples. In scenarios such as\n\n98\n\n\fpersonalised recommendations or adapting to sudden changes in user behaviour, DEFT allows for\nrapid fine-tuning, maintaining high performance with a fraction of the data typically required.\n\u2022 Reducing Computational Costs in Large-Scale Deployments: By focusing on the most\ninfluential data samples and using surrogate models, DEFT significantly reduces the computational\nresources needed for fine-tuning. This makes it feasible to maintain high-performing LLMs even in\nlarge-scale deployments where data volumes are substantial.\nFuture Directions\nThe DEFT introduces a data pruning task for fine-tuning large language models (LLMs), setting the\nstage for new research into efficient LLM-based recommendation systems and presenting numerous opportunities for future exploration. Key areas for further investigation include:\n\u2022 Applying the proposed DEALRec[109] approach to a broader range of LLM-based recommender\nmodels across diverse cross-domain datasets, thereby enhancing fine-tuning performance within\nresource constraints.\n\u2022 Addressing the limited context window of LLMs by selectively focusing on the most informative\nitems in user interaction sequences for fine-tuning purposes.\n\n12.1.3\n\nHardware and Algorithm Co-Design\n\nCo-designing hardware and algorithms tailored for LLMs can lead to significant improvements in the\nefficiency of fine-tuning processes. Custom hardware accelerators optimised for specific tasks or types of\ncomputation can drastically reduce the energy and time required for model training and fine-tuning.\n\u2022 Custom Accelerators: Developing hardware accelerators specifically for the sparse and lowprecision computations often used in LLM fine-tuning can enhance performance. These accelerators\nare designed to efficiently handle the unique requirements of LLMs, such as the high memory\nbandwidth and extensive matrix multiplications involved in transformer architectures.\n\u2022 Algorithmic Optimisation: Combining hardware innovations with algorithmic optimisation\ntechniques, such as those that minimise data movement or leverage hardware-specific features\n(e.g., tensor cores for mixed-precision calculations), can further enhance the efficiency of fine-tuning\nprocesses.\n\u2022 Example: NVIDIA\u2019s TensorRT3 is an example of hardware and algorithm co-design in action.\nIt optimises deep learning models for inference by leveraging NVIDIA GPUs\u2019 capabilities, significantly speeding up the process while reducing the resource requirements. TensorRT\u2019s optimisations\ninclude support for mixed-precision and sparse tensor operations, making it highly suitable for finetuning large models.\nAs the scale of language models continues to grow, addressing the challenges of fine-tuning them efficiently\nbecomes increasingly critical. Innovations in PEFT, sparse fine-tuning, data handling, and the integration\nof advanced hardware and algorithmic solutions present promising directions for future research. These\nscalable solutions are essential not only to make the deployment of LLMs feasible for a broader range of\napplications but also to push the boundaries of what these models can achieve.\n\n12.2\n\nEthical Considerations in Fine-Tuning LLMs\n\n12.2.1\n\nBias and Fairness\n\nWhen fine-tuning LLMs, the goal is often to optimise their performance for specific tasks or datasets.\nHowever, these datasets may inherently carry biases that get transferred to the model during the finetuning process. Biases can arise from various sources, including historical data, imbalanced training\nsamples, and cultural prejudices embedded in language. For instance, an LLM fine-tuned on a dataset\nprimarily sourced from English-speaking countries might underperform or make biased predictions when\n3 https://docs.nvidia.com/tensorrt/index.html\n\n99\n\n\fapplied to text from other linguistic or cultural backgrounds. Google AI\u2019s Fairness Indicators tool4 is a\npractical solution that allows developers to evaluate the fairness of their models by analysing performance\nmetrics across different demographic groups. This tool can be integrated into the fine-tuning pipeline to\nmonitor and address bias in real-time.\nAddressing Bias and Fairness\n\u2022 Diverse and Representative Data: Ensuring that fine-tuning datasets are diverse and representative of all user demographics can help mitigate bias.\n\u2022 Fairness Constraints: Incorporating fairness constraints, as suggested by the FairBERTa framework5 , ensures that fine-tuned models maintain equitable performance across different groups.\n\u2022 Example Application: In healthcare, an LLM fine-tuned to assist in diagnosing conditions might\ninitially be trained on data from predominantly white patients. Such a model could produce less\naccurate diagnoses for patients from other racial backgrounds. By using fairness-aware fine-tuning\ntechniques, healthcare providers can develop models that perform more equitably across diverse\npatient populations.\n\n12.2.2\n\nPrivacy Concerns\n\nFine-tuning often involves using sensitive or proprietary datasets, which poses significant privacy risks. If\nnot properly managed, fine-tuned models can inadvertently leak private information from their training\ndata. This issue is especially critical in domains like healthcare or finance, where data confidentiality is\nparamount.\nEnsuring Privacy During Fine-Tuning\n\u2022 Differential Privacy6 : Implementing differential privacy techniques during fine-tuning can prevent models from leaking sensitive information.\n\u2022 Federated Learning7 : Utilising federated learning frameworks allows models to be fine-tuned\nacross decentralised data sources, which enhances privacy by keeping data localised.\n\u2022 Example Application: In customer service applications, companies might fine-tune LLMs using\ncustomer interaction data. Employing differential privacy ensures that the model learns from these\ninteractions without memorising and potentially leaking personal information, thus maintaining\ncustomer confidentiality.\n\n12.2.3\n\nSecurity Risks\n\n\u2022 Security Vulnerabilities in Fine-Tuned Models: Fine-tuned LLMs are susceptible to security vulnerabilities, particularly from adversarial attacks. These attacks involve inputs designed to\nexploit model weaknesses, causing them to produce erroneous or harmful outputs. Such vulnerabilities can be more pronounced in fine-tuned models due to their specialised training data, which\nmay not cover all possible input scenarios.\n\u2022 Recent Research and Industry Practices: Microsoft\u2019s Adversarial ML Threat Matrix provides a comprehensive framework for identifying and mitigating adversarial threats during model\ndevelopment and fine-tuning. This matrix helps developers understand the potential attack vectors\nand implement defensive strategies accordingly.\n\u2022 Enhancing Security in Fine-Tuning:\n\u2013 Adversarial Training: Exposing models to adversarial examples during fine-tuning can\nenhance their robustness against attacks.\n\u2013 Security Audits: Regularly conducting security audits on fine-tuned models can help identify and address potential vulnerabilities.\n4 https://research.google/blog/fairness-indicators-scalable-infrastructure-for-fair-ml-systems/\n5 https://huggingface.co/facebook/FairBERTa\n6 https://privacytools.seas.harvard.edu/differential-privacy\n7 https://research.ibm.com/blog/what-is-federated-learning\n\n100\n\n\f12.3\n\nAccountability and Transparency\n\n12.3.1\n\nThe Need for Accountability and Transparency\n\nFine-tuning can significantly alter an LLM\u2019s behaviour, making it crucial to document and understand\nthe changes and their impacts. This transparency is essential for stakeholders to trust the model\u2019s\noutputs and for developers to be accountable for its performance and ethical implications.\n\n12.3.2\n\nRecent Research and Industry Practices\n\nMeta\u2019s Responsible AI framework8 underscores the importance of documenting the fine-tuning process\nand its effects on model behaviour. This includes maintaining detailed records of the data used, the\nchanges made during fine-tuning, and the evaluation metrics applied.\n\n12.3.3\n\nPromoting Accountability and Transparency\n\n\u2022 Comprehensive Documentation: Creating detailed documentation of the fine-tuning process\nand its impact on model performance and behaviour.\n\u2022 Transparent Reporting: Utilising frameworks like Model Cards9 to report on the ethical and\noperational characteristics of fine-tuned models.\n\u2022 Example Application: In content moderation systems, LLMs fine-tuned to identify and filter\nharmful content need clear documentation and reporting. This ensures that platform users and\nregulators understand how the model operates and can trust its moderation decisions.\n\n12.3.4\n\nProposed frameworks/techniques for Ethical Fine-Tuning\n\nFrameworks for Mitigating Bias\nBias-aware fine-tuning frameworks aim to incorporate fairness into the model training process. FairBERTa, introduced by Facebook, is an example of such a framework that integrates fairness constraints\ndirectly into the model\u2019s objective function during fine-tuning. This approach ensures that the model\u2019s\nperformance is balanced across different demographic groups.\nOrganisations can adopt fairness-aware frameworks to develop more equitable AI systems. For instance,\nsocial media platforms can use these frameworks to fine-tune models that detect and mitigate hate speech\nwhile ensuring fair treatment across various user demographics.\nTechniques for Privacy Preservation\nDifferential privacy and federated learning are key techniques for preserving privacy during fine-tuning.\nTensorFlow Privacy10 , developed by Google, provides built-in support for differential privacy, allowing\ndevelopers to fine-tune models securely without compromising data confidentiality.\nLLMs are highly effective but face challenges when applied in sensitive areas where data privacy is crucial. To address this, researchers focus on enhancing Small Language Models (SLMs) tailored to specific\ndomains. Existing methods often use LLMs to generate additional data or transfer knowledge to SLMs,\nbut these approaches struggle due to differences between LLM-generated data and private client data. In\nresponse, a new Federated Domain-specific Knowledge Transfer (FDKT)[110] framework is introduced.\nFDKT leverages LLMs to create synthetic samples that mimic clients\u2019 private data distribution using\ndifferential privacy. This approach significantly boosts SLMs\u2019 performance by approximately 5% while\nmaintaining data privacy with a minimal privacy budget, outperforming traditional methods relying\nsolely on local private data.\nIn healthcare, federated fine-tuning can allow hospitals to collaboratively train models on patient data\nwithout transferring sensitive information. This approach ensures data privacy while enabling the development of robust, generalisable AI systems.\n8 https://ai.meta.com/responsible-ai/\n9 https://huggingface.co/docs/hub/en/model-cards\n10 https://www.tensorflow.org/responsible_ai/privacy/guide\n\n101\n\n\fFrameworks for Enhancing Security\nAdversarial training and robust security measures[111] are essential for protecting fine-tuned models\nagainst attacks. The adversarial training approach involves training models with adversarial examples\nto improve their resilience against malicious inputs. Microsoft Azure\u2019s adversarial training tools provide\npractical solutions for integrating these techniques into the fine-tuning process, helping developers create\nmore secure and reliable models.\nIn cybersecurity, fine-tuned LLMs used for threat detection can benefit from adversarial training to\nenhance their ability to identify and respond to sophisticated attacks, thereby improving organisational\nsecurity.\nFrameworks for Ensuring Transparency\nTransparency and accountability frameworks, such as Model Cards and AI FactSheets11 , provide structured ways to document and report on the fine-tuning process and the resulting model behaviours. These\nframeworks promote understanding and trust among stakeholders by clearly outlining the model\u2019s capabilities, limitations, and ethical considerations.\nIn government applications, where AI systems might be used for decision-making or public services,\nmaintaining transparent documentation through frameworks like AI FactSheets ensures that these systems are accountable and their decisions can be audited and trusted by the public.\nFine-tuning LLMs introduces several ethical challenges, including bias, privacy risks, security vulnerabilities, and accountability concerns. Addressing these requires a multifaceted approach that integrates\nfairness-aware frameworks, privacy-preserving techniques, robust security measures, and transparency\nand accountability mechanisms. By leveraging recent advancements in these areas, researchers and\npractitioners can develop and deploy LLMs that are not only powerful but also ethically sound and\ntrustworthy.\n\n12.4\n\nIntegration with Emerging Technologies\n\nIntegrating LLMs with emerging technologies such as IoT (Internet of Things) and edge computing\npresents numerous opportunities and challenges, reflecting advancements and insights from recent research and industry developments.\n\n12.4.1\n\nOpportunities\n\n\u2022 Enhanced Decision-Making and Automation: LLMs have the capability to analyse and derive\ninsights from vast amounts of unstructured data generated by IoT devices. This data can range\nfrom sensor readings in manufacturing plants to environmental data in smart cities. By processing\nthis data in real-time, LLMs can optimise decision-making processes and automate tasks that\ntraditionally required human intervention. For example:\n\u2013 Industrial Applications: Predictive maintenance can be enhanced by LLMs analysing sensor data to predict equipment failures before they occur, thereby reducing downtime and\nmaintenance costs.\n\u2013 Smart Cities: LLMs can analyse traffic patterns and environmental data from IoT sensors\nto optimise city infrastructure and improve urban planning decisions.\n\u2022 Personalised User Experiences: Integration with edge computing allows LLMs to process\ndata locally on devices rather than relying solely on cloud-based servers. This enables LLMs to\ndeliver highly personalised services based on real-time data and user preferences, enhancing user\nexperiences across various domains:\n\u2013 Healthcare: LLMs can provide personalised healthcare recommendations by analysing data\nfrom wearable devices and integrating it with medical records securely stored on edge devices.\n11 https://aifs360.res.ibm.com/\n\n102\n\n\f\u2022 Improved Natural Language Understanding: IoT data integration enriches LLMs\u2019 ability to\nunderstand context and respond more intelligently to natural language queries. This can significantly improve user interactions with smart environments:\n\u2013 Smart Homes: LLMs integrated with IoT devices can understand and respond to voice\ncommands more accurately, adjusting smart home settings based on real-time sensor data\n(e.g., adjusting lighting and temperature based on occupancy and environmental conditions).\n\n12.4.2\n\nChallenges\n\n\u2022 Data Complexity and Integration: Integrating data from diverse IoT devices poses challenges\nrelated to data quality, interoperability, and scalability. LLMs need to effectively process and\ninterpret this heterogeneous data to derive meaningful insights:\n\u2013 Data Integration: Ensuring seamless integration of data streams from different IoT platforms and devices without compromising data integrity or performance.\n\u2013 Data Preprocessing: Cleaning and preprocessing IoT data to ensure consistency and reliability before feeding it into LLMs for analysis.\n\u2022 Privacy and Security: Edge computing involves processing sensitive data locally on devices,\nraising concerns about data privacy and security:\n\u2013 Data Privacy: Implementing robust encryption techniques and access control mechanisms\nto protect sensitive data processed by LLMs on edge devices.\n\u2013 Secure Communication: Ensuring secure communication channels between IoT devices\nand LLMs to prevent data breaches or unauthorised access.\n\u2022 Real-Time Processing and Reliability: LLMs deployed in edge computing environments must\noperate with low latency and high reliability to support real-time applications:\n\u2013 Latency: Optimising algorithms and processing capabilities of LLMs to handle real-time\ndata streams efficiently without delays.\n\u2013 Reliability: Ensuring the accuracy and consistency of insights generated by LLMs in dynamic\nand unpredictable IoT environments.\n\n12.5\n\nFuture Research Areas\n\n\u2022 Federated Learning and Edge Computing: Exploring federated learning techniques where\nLLMs can be trained collaboratively across edge devices without centralised data aggregation.\nThis approach addresses privacy concerns and reduces communication overhead.\n\u2022 Real-Time Decision Support Systems: Developing LLM-based systems capable of real-time\ndecision-making by integrating with edge computing infrastructure. This includes optimising algorithms for low-latency processing and ensuring reliability under dynamic environmental conditions.\n\u2022 Ethical and Regulatory Implications: Investigating the ethical implications of integrating\nLLMs with IoT and edge computing, particularly regarding data ownership, transparency, and\nfairness. This area requires frameworks for ethical AI deployment and governance.\n\n103\n\n\fGlossary\nLLM Large Language Model \u2013 A type of AI model, typically with billions of parameters, trained on vast\namounts of text data to understand and generate human-like text. They are primarily designed\nfor tasks in natural language processing (NLP).\nNLP Natural Language Processing \u2013 A field of artificial intelligence that focuses on the interaction\nbetween computers and humans through natural language, including tasks like language generation,\ntranslation, and sentiment analysis.\nLoRA Low-Rank Adaptation \u2013 A parameter-efficient fine-tuning technique that adjusts only small lowrank matrices to adapt pre-trained models to specific tasks, thus preserving most of the original\nmodel\u2019s parameters.\nDoRA Weight-Decomposed Low-Rank Adaptation \u2013 A technique that decomposes model weights into\nmagnitude and direction components, facilitating fine-tuning while maintaining inference efficiency.\nQLoRA Quantised Low-Rank Adaptation \u2013 A variation of LoRA, specifically designed for quantised\nmodels, allowing for efficient fine-tuning in resource-constrained environments.\nPPO Proximal Policy Optimisation \u2013 A reinforcement learning algorithm that adjusts policies by balancing the exploration of new actions and exploitation of known rewards, designed for stability and\nefficiency in training.\nDPO Direct Preference Optimisation \u2013 A method that directly aligns language models with human\npreferences through preference optimisation, bypassing reinforcement learning models like PPO.\nMoE Mixture of Experts \u2013 A model architecture that employs multiple specialised subnetworks, called\nexperts, which are selectively activated based on the input to improve model performance and\nefficiency.\nMoA Mixture of Agents \u2013 A multi-agent framework where several agents collaborate during training\nand inference, leveraging the strengths of each agent to improve overall model performance.\nPEFT Parameter-Efficient Fine-Tuning \u2013 A fine-tuning approach for large models that involves adjusting only a subset of model parameters, improving efficiency in scenarios with limited computational\nresources. This includes techniques like LoRA, QLoRA, and adapters.\nAdapters Small, trainable modules introduced into the layers of pre-trained language models, allowing\nefficient task-specific fine-tuning without modifying the core parameters of the original model.\nTechniques such as **AdapterFusion** and **AdapterSoup** fall under this category, facilitating\nthe combination of multiple adapters for complex multitasking.\nSoft Prompt Tuning (SPT) A fine-tuning technique where a set of trainable prompt tokens are added\nto the input sequence to guide a pre-trained model towards task-specific performance without\nmodifying internal model weights.\nPrefix-Tuning A variation of soft prompt tuning where a fixed sequence of trainable vectors is prepended\nto the input layer at every layer of the model, enhancing task-specific adaptation.\nQuantisation The process of reducing the precision of model weights and activations, often from 32-bit\nto lower-bit representations like 8-bit or 4-bit, to reduce memory usage and improve computational\nefficiency.\n\n104\n\n\fQuantised LLMs Large Language Models that have undergone quantisation, a process that reduces\nthe precision of model weights and activations, often from 32-bit to 8-bit or lower, to enhance\nmemory and computational efficiency.\nPruning A model optimisation technique that reduces the complexity of large language models by\nremoving less significant parameters, enabling faster inference and lower memory usage.\nHalf Fine-Tuning (HFT) A fine-tuning method where half of the model\u2019s parameters are kept frozen\nwhile the other half are updated, helping to maintain pre-trained knowledge while adapting the\nmodel to new tasks.\nStructured Masking A technique that masks entire layers, heads, or other structural components of\na model to reduce complexity while fine-tuning for specific tasks.\nUnstructured Masking A technique where certain parameters of the model are masked out randomly\nor based on a pattern during fine-tuning, allowing for the identification of the most important\nmodel weights.\nGLUE General Language Understanding Evaluation \u2013 A benchmark used to evaluate the performance\nof NLP models across a variety of language understanding tasks, such as sentiment analysis and\nnatural language inference.\nSuperGLUE Super General Language Understanding Evaluation \u2013 A more challenging extension of\nGLUE, consisting of harder tasks designed to test the robustness and adaptability of NLP models.\nTruthfulQA A benchmark designed to measure the truthfulness of a language model\u2019s output, focusing\non factual accuracy and resistance to hallucination.\nIFEval Instruction Following Evaluation \u2013 A benchmark that assesses a model\u2019s ability to follow explicit\ninstructions across tasks, usually in the context of fine-tuning large models for adherence to specific\ninstructions.\nBBH Big Bench Hard \u2013 A subset of the Big Bench dataset, which consists of particularly difficult tasks\naimed at evaluating the advanced reasoning abilities of large language models.\nMATH A dataset created to evaluate a model\u2019s ability to solve high-school level mathematical problems,\npresented in formal formats like LaTeX.\nGPQA General-Purpose Question Answering \u2013 A challenging dataset that features knowledge-based\nquestions crafted by experts to assess deep reasoning and factual recall.\nMuSR Multimodal Structured Reasoning \u2013 A dataset that involves complex problems requiring language models to integrate reasoning across modalities, often combining text with other forms of\ndata such as images or graphs.\nMMLU Massive Multitask Language Understanding \u2013 A benchmark that evaluates a language model\u2019s\nability to perform various tasks across diverse domains, such as humanities, STEM, social sciences,\nand others, typically requiring high-level reasoning.\nMMLU-PRO A refined version of the MMLU dataset with a focus on more challenging, multi-choice\nproblems, typically requiring the model to parse long-range context.\nARC AI2 Reasoning Challenge \u2013 A benchmark for evaluating a language model\u2019s reasoning capabilities\nusing a dataset of multiple-choice science questions.\nCOQA Conversational Question Answering \u2013 A benchmark that evaluates how well a language model\ncan understand and engage in back-and-forth conversation, especially in a question-answer format.\nDROP Discrete Reasoning Over Paragraphs \u2013 A benchmark that tests a model\u2019s ability to perform\ndiscrete reasoning over text, especially in scenarios requiring arithmetic, comparison, or logical\nreasoning.\nSQuAD Stanford Question Answering Dataset \u2013 A popular dataset for evaluating a model\u2019s ability to\nunderstand and answer questions based on passages of text.\n\n105\n\n\fTREC Text REtrieval Conference \u2013 A benchmark that evaluates models on various text retrieval tasks,\noften focusing on information retrieval and document search.\nWMT Workshop on Machine Translation \u2013 A dataset and benchmark for evaluating the performance\nof machine translation systems across different language pairs.\nXNLI Cross-lingual Natural Language Inference \u2013 A dataset designed to evaluate a model\u2019s ability to\nunderstand and infer meaning across multiple languages.\nPiQA Physical Interaction Question Answering \u2013 A dataset that measures a model\u2019s understanding of\nphysical interactions and everyday tasks.\nWinogrande A large-scale dataset aimed at evaluating a language model\u2019s ability to handle commonsense reasoning, typically through tasks that involve resolving ambiguous pronouns in sentences.\nRLHF Reinforcement Learning from Human Feedback \u2013 A method where language models are finetuned based on human-provided feedback, often used to guide models towards preferred behaviours\nor outputs.\nRAFT Retrieval-Augmented Fine-Tuning \u2013 A method combining retrieval techniques with fine-tuning\nto enhance the performance of language models by allowing them to access external information\nduring training or inference.\n\n\f",
                "embd": [
                    -0.006622624559066586,
                    0.005473022778176024,
                    0.008886099503170843,
                    -0.024506366986434148,
                    -0.013641315230124402,
                    0.015374066042375421,
                    0.002614130755923261,
                    0.0066958798677563625,
                    -0.03082831335631946,
                    -0.055673708549111633,
                    0.023067087550234743,
                    0.027668635860113283,
                    -0.012128881224288283,
                    0.0030417134928893853,
                    -0.005087457484621872,
                    -0.008109458258734865,
                    0.027913888779678445,
                    -0.003305348433209196,
                    0.01784933584424183,
                    -0.01723971383049376,
                    -0.015597346184845652,
                    -0.017508761242847226,
                    -0.016716406654373184,
                    -0.012348658605905404,
                    0.001121193936753725,
                    0.004546821366340077,
                    0.03787472276412797,
                    -0.035174576725697286,
                    -0.021250558474353795,
                    0.003087163826029135,
                    0.018873955150853137,
                    0.01067337644623046,
                    -0.015283015853854543,
                    0.0008089698810383604,
                    -0.0053922382226541735,
                    -0.007392398506328462,
                    0.0149691761418774,
                    -0.003082991696438669,
                    0.037543223003437404,
                    -0.015913916821443375,
                    0.02136848864344024,
                    0.033188483351834185,
                    0.0072904941798517305,
                    -0.009450281475516235,
                    -0.010342753675649496,
                    0.025534884453453387,
                    -0.0013694176914709453,
                    -0.019301321737325988,
                    -0.018584605637570785,
                    0.012734810078006737,
                    0.010157261598718626,
                    0.03727943681281849,
                    -0.016329423405921705,
                    0.0038552713496163423,
                    -0.014137669838682648,
                    -0.005361540592063117,
                    -0.007353976211520397,
                    0.022944660088464918,
                    0.016466159773415007,
                    -0.014308021650106717,
                    0.005810095631509949,
                    0.013638869735758986,
                    -0.03126567083355134,
                    0.01324415637974758,
                    -0.005172582797244977,
                    -0.006727614248439694,
                    -0.016528923404813496,
                    0.04015763044154384,
                    0.010715204052869963,
                    -0.0179760445711337,
                    0.020624272879024384,
                    0.022499854057328437,
                    -0.005260632838159081,
                    -0.005121634181414527,
                    0.007558105880030296,
                    -0.013952138313526315,
                    -0.006445454295212577,
                    -0.015286142508572789,
                    -0.007898022087988168,
                    -0.010055369017285362,
                    0.016374058040218388,
                    0.0033188888407778634,
                    -0.0076185026093696625,
                    0.03455885277755709,
                    0.018401234512376793,
                    -0.007668157965608299,
                    0.006377372179123169,
                    -0.014654223054151145,
                    0.0010706944599063546,
                    0.0015026393662275696,
                    0.0025270678405898807,
                    0.007790486742410319,
                    0.03253293872321734,
                    0.0211529181515864,
                    0.0022563573148848136,
                    -0.00033799226067358077,
                    -0.008962771347707392,
                    0.006677087523794703,
                    -0.01338995390044758,
                    -0.005691304773492595,
                    -0.014443397894153913,
                    0.0038155514933397388,
                    -0.04589270019219432,
                    -0.009547030880395562,
                    -0.029853210362035185,
                    -0.00793422792728336,
                    0.0066927202696470305,
                    0.0039815813272441256,
                    -0.0019549355983232747,
                    0.002549757935886537,
                    -0.022028407763775775,
                    0.028483136920894585,
                    -0.010864936394959546,
                    -0.02940623178915536,
                    0.0015517059746593188,
                    -0.02034245887903034,
                    0.0179824876901256,
                    -0.0130584953671101,
                    -0.014429969716140332,
                    -0.01609275093638033,
                    0.03910921754860892,
                    0.03446078277388352,
                    0.022273700469733555,
                    -0.012274380798828914,
                    0.03834655719058528,
                    -0.00915364234549999,
                    0.009552584602303195,
                    -0.021947171128524887,
                    -0.021991980586958546,
                    -0.0025344393320345117,
                    0.0033012778151631414,
                    0.008366325391101187,
                    0.00605048042336742,
                    -0.008692088611511726,
                    -0.022877272615685847,
                    0.02086989400461194,
                    -0.00994489916203581,
                    -0.02344789092138723,
                    -0.02227442266618061,
                    -0.02486744603050141,
                    0.023572527087808933,
                    0.023609825582226046,
                    -0.009349622392161198,
                    -0.0169289447001791,
                    -0.01663728272404562,
                    0.025419835765988323,
                    -0.006245429710897207,
                    0.015494623852559414,
                    0.00910367236929343,
                    -0.015289900323879687,
                    -0.000675618160439067,
                    -0.03164093170304716,
                    -0.015222461389268294,
                    0.02335561424663403,
                    0.011075487737431024,
                    -0.007274934469424658,
                    0.0075374050657680694,
                    0.014719861387389668,
                    -0.025435043191127306,
                    -0.029966387258797994,
                    0.007855495957845026,
                    0.013901403866647715,
                    -0.016816750789146975,
                    0.027178631942389442,
                    0.004649187632416165,
                    0.023684732368663947,
                    0.029528892831575046,
                    -0.007025684984822616,
                    -0.006439187320415221,
                    0.0022995223188057795,
                    0.0016041616354512702,
                    0.02133908126221176,
                    -0.0399985086857783,
                    0.019341843023002545,
                    -0.0017037695750611395,
                    0.00897258713182394,
                    0.021548331871599106,
                    -0.020429667948107983,
                    -0.01981167556582319,
                    -0.02150706184943367,
                    0.021262251087664566,
                    0.009983673514726208,
                    0.007248508044524176,
                    0.010656787954340094,
                    -0.020176444188243214,
                    -0.011815195152368113,
                    0.0013660137088089247,
                    -0.006665918905284044,
                    -0.0011671863664419262,
                    -0.002294685930833101,
                    0.005149567862790644,
                    0.02812410139975489,
                    -0.009271429078467602,
                    -0.020728719179284847,
                    -0.6418534873108589,
                    -0.01081320812653292,
                    -0.0007795643166183064,
                    -0.010463142989670087,
                    -0.010230731349764592,
                    0.0018872445694786018,
                    -0.006751239470660989,
                    0.005149032403022258,
                    -0.01848867393170694,
                    0.03784209493511191,
                    -0.010486285508291367,
                    -0.010403889159781606,
                    -0.0045614607751683425,
                    -0.013424569260528027,
                    -0.011251936171257035,
                    -0.019814206128000413,
                    -0.006680194392682177,
                    -0.03261900769719044,
                    0.0014354115133095415,
                    0.005315907647902037,
                    -0.019334684051568355,
                    0.013664030458370528,
                    -0.017157474677506647,
                    0.001756598104142107,
                    -0.013731167619547449,
                    0.011033038676441185,
                    0.014633293021207252,
                    0.0009090137772922546,
                    0.02558348814347121,
                    0.008478890864385162,
                    -0.010178114581622284,
                    0.018826843579777278,
                    0.01297004885327755,
                    0.004843877246447649,
                    0.05625660347867525,
                    -0.003046074788412213,
                    -0.036730537463246525,
                    0.03139354879016347,
                    0.022507203287434083,
                    0.021145564798940857,
                    -0.020485839412632625,
                    -0.020587360317052432,
                    0.003029599393504255,
                    0.0005321718896119172,
                    0.0007421907171401578,
                    0.022065983331351235,
                    0.007756802011898809,
                    0.005364808915316995,
                    -0.006771962007152604,
                    -0.0072457508944015385,
                    -0.0016483585318703843,
                    0.020616374206914975,
                    0.008477609660223569,
                    -0.0077422357573720185,
                    0.009719513424420003,
                    -0.004850726069159462,
                    0.013467422258317393,
                    -0.013409575192958628,
                    0.01872083805429712,
                    -0.012788378387788417,
                    -0.002914285478700515,
                    -0.0012224591609600046,
                    -0.01841522354643602,
                    -0.02120918542820569,
                    -0.01251541770313335,
                    0.009135796844224617,
                    0.0030479195868421127,
                    -0.006475224689209499,
                    0.0003651622839821021,
                    -0.025413725570055713,
                    0.017034369223471332,
                    0.016169352184196583,
                    0.009281002476286979,
                    -0.025212301684783067,
                    0.0023759755781958838,
                    0.00845860026744393,
                    0.030640323060526456,
                    -0.0005801571816274289,
                    0.013513181401392719,
                    0.019130642253665158,
                    0.011222479882734886,
                    -0.007448181907123831,
                    -0.005732438641532439,
                    0.001293837702434345,
                    0.019606952483021342,
                    0.004939629638891751,
                    0.007509557893134415,
                    0.005378587567511675,
                    -0.008614820963156427,
                    0.005360616722138162,
                    0.00563899234116135,
                    0.03115489959557489,
                    -0.02097811354673529,
                    -0.055480343670268696,
                    -0.0035580459991686585,
                    0.011522753965536646,
                    -0.00799038597655802,
                    0.012218424434290978,
                    -0.006087961631068057,
                    -0.020826485110320533,
                    -0.0012308343721670677,
                    -0.013437946550743323,
                    0.013551291373308797,
                    0.01722349487172862,
                    0.04026452319714808,
                    0.007520311839679041,
                    -0.0031796237847478807,
                    -0.0017316758127618962,
                    0.012840617036591362,
                    -0.030775162640308162,
                    -0.007447952941594455,
                    -0.000120806047490439,
                    -0.017584006927278864,
                    -0.004316041174585273,
                    0.015882325298526158,
                    -0.03787881493485321,
                    0.014056412109766386,
                    -0.0007468281816459904,
                    0.006126268676625036,
                    -0.013994513789588802,
                    0.023129167642652465,
                    -0.002036647151889649,
                    0.028284190138993665,
                    0.002515003989807689,
                    0.00040330944886963333,
                    0.009015152859487879,
                    -0.003059426268730383,
                    0.0004243488311671772,
                    0.00042575787332094065,
                    0.017630303317006615,
                    -0.012967966533189873,
                    -0.0242064871375026,
                    0.033936706728480245,
                    -0.02303424705021738,
                    0.03091214096711458,
                    0.022555559837940956,
                    0.026739326695012184,
                    0.004734979939560415,
                    0.012675699828280514,
                    -0.022326451442403365,
                    -0.02466013001318903,
                    -0.007583814804076085,
                    0.017255827649096837,
                    -0.037367426323702116,
                    -0.049519414441723794,
                    -0.03222306813972568,
                    -0.0029414198971691266,
                    0.005675178372336936,
                    0.011625494757705543,
                    -0.0028252274974051284,
                    -0.016400940828159516,
                    -0.020771838756266914,
                    -0.023635521088914237,
                    0.021045599370339108,
                    -0.016946692436209856,
                    -0.009504737763396304,
                    -0.00949880480680698,
                    -0.03132494699835046,
                    -0.009956458814505334,
                    -0.041705979194764896,
                    0.0028638314457991782,
                    0.01512880030252829,
                    -0.018493974070772533,
                    0.009165357322393805,
                    -0.014263193308018653,
                    -0.015507972696233923,
                    -0.0008131214072995321,
                    0.025775778572846013,
                    -0.022436155964108914,
                    -0.03402783853997907,
                    -0.0008218170670530036,
                    -0.04719092772927651,
                    -0.00038164844782740754,
                    0.02408091331014512,
                    -0.004609649501302572,
                    0.017477250845278975,
                    -0.005180388217848021,
                    0.0008206271953885742,
                    0.0007275588618143578,
                    -0.01295161418756638,
                    0.0011663763752107954,
                    0.009762484029829032,
                    -0.005623183666688271,
                    -0.010394664797543127,
                    0.022994095090007223,
                    0.021883322542644055,
                    0.023596253702351275,
                    0.0026347626246935613,
                    -0.025570198544967275,
                    0.013193474792001565,
                    0.00716487404492181,
                    0.00637838414176736,
                    -0.017672557660454034,
                    0.01558052031412437,
                    -0.0025364329304875775,
                    0.0010328930231622957,
                    0.011928827062117084,
                    -0.001667863918290257,
                    -0.002299275481918586,
                    0.028691910231840298,
                    0.013826856623491588,
                    -0.0037961150514807126,
                    0.010052660540489693,
                    -0.02217216459060151,
                    0.00115200018336528,
                    -0.02034151066186163,
                    0.005811916300609128,
                    -0.026800265428045263,
                    0.0006440179561966159,
                    0.006051680401059792,
                    0.010572518637290213,
                    -0.00976686489432608,
                    -0.02753040928616534,
                    -0.02062183558311137,
                    -0.001935484600767245,
                    0.04011188411309718,
                    0.004771483939053447,
                    0.02648186854513864,
                    -0.0213330248351888,
                    0.010186921311165447,
                    -0.008972722644135447,
                    0.012366839390830383,
                    0.02909527947258212,
                    -0.004949180063121139,
                    0.0047810213166303685,
                    1.3977329751474744e-06,
                    -0.0001278497834717435,
                    0.001420505841121361,
                    0.01190451944184282,
                    -0.02759156120504161,
                    -0.017349693892385977,
                    0.0159167400731085,
                    0.004135196936651501,
                    0.012880221433709323,
                    0.016634294741206916,
                    -0.005963937554380697,
                    0.03442151346122168,
                    -0.010705961405496388,
                    0.03267218255664773,
                    -0.01668202855918733,
                    0.018994091326106084,
                    0.007536484055980025,
                    -0.004712280975937641,
                    -0.013244882045209065,
                    0.008357158693123213,
                    -0.0029934020654995073,
                    0.020935108979031256,
                    0.0325451489859821,
                    -0.019210393254522118,
                    -0.012915226933302278,
                    -0.00704764506707752,
                    0.008782285189695356,
                    -0.012829416260043972,
                    0.008864319553003262,
                    0.015490580489464187,
                    -0.011972123953135074,
                    0.012265755588792986,
                    0.010478334442902701,
                    0.023726924336850218,
                    0.007932705434352491,
                    0.0011012777794911625,
                    0.019747482651017563,
                    0.006496048779754082,
                    -0.009013559957357057,
                    0.009021238415477745,
                    -0.020503561837974416,
                    -0.002090538434572726,
                    -0.009772263156772156,
                    -0.01295711061830454,
                    -0.005237019249674253,
                    -0.01832558574513399,
                    -0.017995374987440395,
                    0.002959637518767783,
                    -0.04197016512876127,
                    0.017269674782782712,
                    0.023610403001182126,
                    0.017758652245935258,
                    0.018856313511798155,
                    0.019365454279718444,
                    0.031852057103318294,
                    -0.01964327542582412,
                    -0.03807313592949655,
                    0.029029175414605254,
                    -0.0005466025840124889,
                    -0.011360484293548008,
                    -0.016415202392369946,
                    -0.038182441703201786,
                    0.0028414821720020036,
                    0.0011509647251405119,
                    0.0020568587464597846,
                    0.006479730364554366,
                    0.007443716884291834,
                    -0.004343500756106602,
                    -0.0023322747670431305,
                    -0.022693494281060475,
                    -0.005554127179804205,
                    0.039386194540782904,
                    -0.019217007436681893,
                    -0.003008012704160108,
                    -0.0038518223731902304,
                    0.009502631950109452,
                    0.013784111666917375,
                    -0.04044270472563085,
                    -0.010626758355979095,
                    0.03958670428872625,
                    -0.016592458139850386,
                    -0.02820748698333312,
                    -0.007821778875604809,
                    -0.01141094270620442,
                    -0.011962186447807784,
                    0.019413374971054675,
                    -0.016635943514133868,
                    0.013023412683850715,
                    0.009104472121539987,
                    0.02002527886932077,
                    0.0037254778158863546,
                    0.0067379720713951555,
                    0.01170758542507096,
                    0.024274167052940145,
                    0.017408005076402437,
                    -0.0015892417187264715,
                    -0.022105806416559612,
                    -0.010417456095189372,
                    0.01117460663231674,
                    0.06295562014709329,
                    0.03876367728992762,
                    -0.01904240661578306,
                    0.012583149653254418,
                    -0.0402690742559212,
                    -0.007164719351905611,
                    -0.02058065051564895,
                    -0.023086378953540494,
                    0.008057905200465874,
                    0.02970658684450049,
                    -0.011484189796105418,
                    -0.0089943166773674,
                    0.0126975499626965,
                    0.014001336624229428,
                    0.020580018008721838,
                    -0.00762052270913038,
                    0.01480069528721127,
                    -0.02615184418626407,
                    0.004090254412537083,
                    0.0036129309315491203,
                    0.008260865989095267,
                    -0.001547293497619415,
                    0.008722326484532495,
                    0.028238867137545014,
                    0.024484293828979705,
                    0.006308456260037339,
                    0.03154222302728535,
                    0.015274738262109736,
                    -0.012564583320045798,
                    -0.019085261353944328,
                    -0.0038789592936638233,
                    0.00888840246276738,
                    0.00982431782198038,
                    0.009593167229653352,
                    0.002271538145109627,
                    0.022300636356251874,
                    0.020535515758226944,
                    0.0068987796753266824,
                    0.01196336534594049,
                    0.0013199113134347725,
                    0.014885095278470506,
                    0.028653883289475067,
                    0.0018741718541816679,
                    0.006619912614547255,
                    0.01664599181592184,
                    -0.030614560481122807,
                    -0.008967723652782145,
                    0.022616580832509912,
                    -0.017057980845549384,
                    0.00016870772582539985,
                    0.023383572076880067,
                    -0.0002718690915481423,
                    -0.033342478945719026,
                    -0.010011358448542248,
                    0.028654157306265148,
                    0.0012949750027775693,
                    0.0033919250261942955,
                    -0.009267472393069114,
                    -0.00035484020104492586,
                    -0.0348353453676822,
                    -0.018634819672153656,
                    -0.0333851674148508,
                    0.006332028810855521,
                    -0.004622876951869311,
                    0.0012908884083007034,
                    0.009583986985303728,
                    -0.019571709246485214,
                    0.010212165132417208,
                    -0.020392681604004197,
                    0.010155103340704987,
                    -0.028135837285590038,
                    -0.023158513304911192,
                    -0.044209359424618316,
                    0.003474837169553798,
                    0.013534368742865198,
                    0.017500657623309185,
                    0.01076053595039179,
                    0.0003463328022948715,
                    0.009885223858218493,
                    -0.00755610361889508,
                    -0.0030351326272925424,
                    -0.005196326321531509,
                    0.00675920888576823,
                    -0.029007023852373113,
                    0.006283836064051609,
                    -0.02195265673663225,
                    -0.011307782432609395,
                    -0.00657660185011267,
                    -0.02146096127542162,
                    0.006076257343555511,
                    -0.00794375967826736,
                    0.004523246595481601,
                    -0.010803027238865674,
                    -0.0037243975575506805,
                    0.013313781073435524,
                    0.026060903665279234,
                    0.009532927915283117,
                    0.010738816900144098,
                    0.02122229267646532,
                    -0.03879413706666974,
                    -0.0028237549367923264,
                    -0.025238451853960484,
                    -0.027821910923283177,
                    -0.011822845349757072,
                    -0.007697115197456723,
                    0.0031049590116629363,
                    0.010136324658162171,
                    -0.0049961307132936296,
                    -0.005020689811901436,
                    -0.012504637366983458,
                    0.0019452119169827697,
                    -0.013914921528101018,
                    0.00783991648035451,
                    0.009627742227500288,
                    0.013421898600328807,
                    -0.003455604486996347,
                    0.01682376791545008,
                    0.0066884417393054,
                    0.013660745485321531,
                    -0.024591189914968767,
                    0.004140169508692713,
                    -0.0436163236744034,
                    0.034546978414088686,
                    0.013950093920370074,
                    -0.012355165981142219,
                    0.0335478814687606,
                    0.007566232674561194,
                    -0.007357550350645559,
                    -0.038494242746235155,
                    0.020617412190058144,
                    -0.0017268420794007817,
                    0.027810133994411984,
                    -0.0002942991933045108,
                    -0.022280676989343884,
                    -0.033042138245473744,
                    -0.009585904852607035,
                    -0.02461759710580223,
                    0.0016654656170015728,
                    -0.017552687467597,
                    0.006604220794102211,
                    -0.001110276909366785,
                    0.020060088045012383,
                    0.01564174699782006,
                    -0.014765942202012057,
                    0.02770944237160973,
                    -0.02586823375380414,
                    -0.01727802756407497,
                    0.0023160913006415617,
                    0.008163117810812715,
                    0.029028797134945043,
                    -0.020506890486245575,
                    0.013236725702496555,
                    -0.04032053328689275,
                    -0.02726267728473336,
                    -0.004053943228893459,
                    -0.016604861826169506,
                    0.011657213642835148,
                    -0.009372467752885348,
                    0.027923859781385566,
                    0.017894436928430502,
                    0.046201368518909006,
                    -0.007075955789657015,
                    0.01129268275322513,
                    0.011438582163685146,
                    -0.008951381501132675,
                    -0.0020180784515840145,
                    -0.0031991654452939,
                    -0.007973842383514602,
                    -0.004308799996216577,
                    -0.008707825386114164,
                    0.022496423847134885,
                    -0.0010497493993938613,
                    0.007380792655708496,
                    -0.01810289847089136,
                    0.015541068875119616,
                    0.019298330611861172,
                    -0.015978507491763168,
                    -0.023845568048476103,
                    -0.02436543167734762,
                    -0.014208005966809193,
                    -0.015958085541516544,
                    0.013367389961511355,
                    -0.005237880118304988,
                    0.0012583480704611567,
                    -0.03000826295480567,
                    -0.011710514028198794,
                    0.02812819187258223,
                    0.00488370646453526,
                    0.01944616934102546,
                    0.007972432824881323,
                    0.004482031146425079,
                    -0.022234620775161387,
                    0.005320491206791271,
                    0.015747893990285134,
                    0.011320341063473982,
                    -0.013951107572057813,
                    -0.005433890576829027,
                    -0.047852607084610935,
                    -0.0073387354079001614,
                    0.017336064063597657,
                    -0.012839664483896696,
                    0.009541064060455282,
                    0.014538964561518273,
                    -0.00948364040322303,
                    -0.0012747506086300694,
                    0.008185531600774856,
                    0.005329503049443199,
                    -0.028641711649592835,
                    -0.008807807146983267,
                    -0.02782649138308337,
                    -0.009838706431760871,
                    -0.02657057363921081,
                    0.005682791522522889,
                    0.003573071274996782,
                    0.0043809176098843034,
                    -0.0005063963851423024,
                    0.010642247905662598,
                    0.03183081326025662,
                    -0.02818776212696918,
                    -0.0033320292507181305,
                    0.027345214474708238,
                    0.01808424039377733,
                    0.045938382825029926,
                    -0.001290882843165532,
                    0.015347468816331145,
                    0.00048748610710744854,
                    -0.0008508481027132583,
                    -0.02144753598731193,
                    0.001993987999320458,
                    0.002653576477153653,
                    -0.006538698313003932,
                    0.016611298866380672,
                    0.011936024333627717,
                    -0.017677614374638228,
                    -0.013323080167922847,
                    0.010746429835660811,
                    -0.010802285496160633,
                    -0.028848315195507096,
                    -0.04033328272468678,
                    0.04093901646340793,
                    0.017028140462315617,
                    0.014404574347386415,
                    -0.01835515914267986,
                    -0.02239559467319348,
                    -0.023233697052066515,
                    0.007525279984127979,
                    -0.011583857785482026,
                    0.019539873035773717,
                    0.003945002614148706,
                    -0.006753053630482865,
                    0.004032448809572848,
                    0.03159078680123987,
                    0.03316455229590806,
                    0.010255794634743144,
                    -0.01273166028212326,
                    -0.008344271584989446,
                    0.0204227260052341,
                    -0.005540467016798273,
                    -0.03010494593097163,
                    0.0025027698571300663,
                    -0.0030953557557235564,
                    0.029563088076561297,
                    -0.009605222237063515,
                    0.01581148068811314,
                    0.018618472099856568,
                    -0.012830947385197964,
                    0.0016264295276798544,
                    -0.0023972746118495914,
                    -0.0032944308347648346,
                    0.03162387940766963,
                    -0.0213344690330237,
                    0.0012676291410684062,
                    0.0004569841979327897,
                    -0.014784372451430815,
                    0.004166466137052433,
                    -0.015287458453000608,
                    -0.008608085902303502,
                    -0.0017384174960807445,
                    0.0004621410577069966,
                    0.0034018212266435986,
                    0.018772695255003954,
                    0.0034239289563505197,
                    -0.02861216859263775,
                    0.010000524133197927,
                    -0.001432635698344454,
                    -0.01731499785621316,
                    0.002908456309253824,
                    0.003222053042415213,
                    0.012897631489410305,
                    -0.04349336583478291,
                    -0.007623561277768761,
                    0.0026496861638615654,
                    -0.003462679992484703,
                    0.0047432458158890335,
                    0.0021280747910005387,
                    -0.015472562312296982,
                    -0.01635632223108352,
                    0.029772609581125854,
                    -0.029269501365860883,
                    -0.0015054431854462248,
                    0.020549156525415103,
                    0.0027337367782208745,
                    -0.013580641277055031,
                    0.005467958052766312,
                    -0.024799617208707325,
                    -0.004896359390360906,
                    0.02408253870052945,
                    -0.02004368259995268,
                    -0.031940503661296755,
                    -0.0004793371309167606,
                    -0.004900063252755951,
                    0.010216936505927707,
                    0.009389159570074499,
                    -0.003935945698176048,
                    0.0038393135384073057,
                    0.0024937757805977134,
                    0.010860990868822007,
                    0.0010805898584051244,
                    0.015070931505590489,
                    0.01385305018928496,
                    -0.0011259763110730207,
                    -0.005838395325797812,
                    0.010980165874133698,
                    -0.02644429622420985,
                    0.024318626412573853,
                    -0.008265198796364098,
                    0.0007429577528827714,
                    -0.006036149042848906,
                    -0.03809918892239269,
                    0.0010713005805568806,
                    0.0036509555965241716,
                    0.009022956240424228,
                    -0.026025134286795232,
                    -0.0009106990536740517,
                    -0.0030142260917833024,
                    -0.0059090235879088685,
                    -0.015093279258315908,
                    0.011589190902026618,
                    -0.008840583241034245,
                    0.007792651410478368,
                    0.012128381878241433,
                    0.017192378502934175,
                    0.022617720147584522,
                    0.0131355416952621,
                    -0.018576154519624143,
                    -0.018812945005614132,
                    0.008721893947799667,
                    0.006825714431519967,
                    -0.02468633382382266,
                    -0.017961490775869022,
                    -0.02503077779176605,
                    0.03370345363817233,
                    0.014819298795953331,
                    -0.027513372144788918,
                    0.0013509126157294777,
                    -0.01559946534906154,
                    -0.04141905300411406,
                    -0.009660001863936845,
                    -0.022024623116173307,
                    0.009587725046687064,
                    0.0017259818967508445,
                    8.977730624551349e-05,
                    0.012349340428861811,
                    0.012186873014480671,
                    -0.00018926196863249507,
                    0.012740134836380741,
                    -0.01299204909048942,
                    -0.0036312128993693784,
                    -0.006488461560309109,
                    -0.009338486643913256,
                    0.007081402729070749,
                    -0.014162673900605745,
                    -0.0355191070769069,
                    0.003923874686840432,
                    0.006622791848146289,
                    -0.009101498383241275,
                    0.0041931612301142765,
                    0.03008560936579584,
                    -0.016848370787625945,
                    -0.009296467460204294,
                    -0.009574662398672482,
                    0.02588645694065828,
                    0.00703360145976434,
                    0.005897555987441911,
                    0.023298334388085346,
                    -0.02554752721209158,
                    -0.002892247087374921,
                    0.016970390085951463,
                    -0.003969146982723814,
                    -0.01648255538140312,
                    0.00019354498772675407,
                    0.01843405468270166,
                    0.004587735814188969,
                    0.00422857088756014,
                    0.0034681721539129625,
                    -0.010081323392732053,
                    0.016287694237284527,
                    -0.01899811367044895,
                    0.054478636874163454,
                    0.0048700642446137124,
                    -0.00045491259005997134,
                    0.022642979018355283,
                    0.008460225715615522,
                    0.0023880473773161687,
                    -0.01020464285918326,
                    0.011172786242419686,
                    -0.0022311725994797202,
                    -0.005101060475213963,
                    0.036194356613858385,
                    -0.01916018463029893,
                    -0.010901059771082125,
                    -0.0043773530588532015,
                    0.007353257707503444,
                    -0.001647984634168157,
                    -0.009083144615744,
                    -0.012481226835787241,
                    -0.005650712140062043,
                    -0.03347928779084555,
                    0.02138460460122271,
                    0.016667691350517867,
                    -0.0296192824204587,
                    0.007012482686638857,
                    -0.012259785372446343,
                    0.004812745823151585,
                    -0.00928261358411094,
                    -0.01924144673446832,
                    0.0009208392939060575,
                    -0.007639213318494286,
                    -0.002478953227338343,
                    -0.01515729413262826,
                    -0.02333306849817211,
                    -0.006294166843863414,
                    -0.008761122571126835,
                    0.010242931019779317,
                    -0.008868646686463744,
                    0.01275246986650179,
                    0.21471509949869122,
                    -0.01909815161204293,
                    -0.006533518800909665,
                    0.01352076139801046,
                    0.00784564228477132,
                    0.001962356604818667,
                    0.023506834944407275,
                    0.0029650403204172712,
                    -0.015884450603506725,
                    0.009440238585803729,
                    0.011015756445868519,
                    0.0036846829884832778,
                    -0.020887070286364613,
                    0.004533546975407023,
                    0.01370314753499936,
                    -0.00622022799320825,
                    -0.035420460348072946,
                    -0.03198693708969193,
                    -0.011888427950920714,
                    0.020681541381980136,
                    -0.0021419857105170166,
                    0.004246597579240777,
                    -0.011737701019741493,
                    -0.01842633025150801,
                    0.01937837026949497,
                    -0.004603200766811995,
                    0.003270003822692654,
                    -0.01023195158050211,
                    0.015243312729999773,
                    0.008265426766364525,
                    -0.024265586897578486,
                    0.010073043393763078,
                    0.005553793999218287,
                    0.009010725240960337,
                    -0.007086707103413987,
                    0.003449887649451121,
                    0.009835250824475176,
                    -0.005753179479170095,
                    0.012972233716667291,
                    0.005141764283214272,
                    0.018921144698593767,
                    -2.5316451162692945e-05,
                    0.009391993468714372,
                    -0.03724898241710366,
                    -0.006753512738780194,
                    0.015306694062167135,
                    -0.007665078282614606,
                    -0.0017886733248098194,
                    -0.0041796684509075805,
                    0.004307364235975105,
                    -0.023198455849829092,
                    0.01304303276385638,
                    0.02877133365623271,
                    0.023200916882042316,
                    0.00042183826539671774,
                    0.014201046527982465,
                    -0.019820095073272617,
                    0.029620110248855733,
                    0.002651388515638729,
                    0.02131701175089959,
                    -0.028731839295343314,
                    -0.0015637766515630568,
                    0.009291841577488177,
                    0.00856292109576639,
                    -0.0068092321949130875,
                    0.0027212954481065955,
                    -0.014623972361913337,
                    -0.005069551420850001,
                    0.0043030937294591814,
                    -0.005533425378237192,
                    -0.010173691681733193,
                    -0.010943312900868316,
                    -0.018831110905166606,
                    -0.007192909035877571,
                    -0.044208258158181954,
                    -0.03774824127849559,
                    0.029290074595643593,
                    0.024844571214074324,
                    0.019255196582382332,
                    0.04518440314538904,
                    -0.01168999452381284,
                    -0.014609118794218208,
                    0.0007331386133991532,
                    -0.006797324644675093,
                    -0.003343446826602398,
                    -0.025984335062834023,
                    0.018179736145340325,
                    -0.032700637647704846,
                    -0.02009539215972769,
                    -0.009114631880647215,
                    -0.004320649999553762,
                    -0.006285203534777732,
                    -0.013483403057778691,
                    0.012797372147756586,
                    -0.008179321616656137,
                    -0.014766370289849281,
                    0.021143703799238786,
                    0.0008427063286926987,
                    -0.015442485048829788,
                    -0.022008768439408417,
                    -0.044410041627582275,
                    0.06509905804611228,
                    0.02418570659636495,
                    0.012402771036304345,
                    -0.005215011024050777,
                    0.0013605018076263407,
                    -0.010538474428128757,
                    0.026497419255855387,
                    -0.0012635309346907873,
                    -0.014477234118611296,
                    0.0047261598876543725,
                    -0.049561731074162146,
                    0.012313249787344313,
                    -0.004244137987796863,
                    0.005323897679568121,
                    0.01879483162924321,
                    -0.0007822709439911149,
                    0.011072818858935332,
                    0.01959376286615038,
                    -0.004229335124191273,
                    -0.003277696689760119,
                    -0.021021011336649763,
                    0.01364257155232835,
                    -0.016252317401270526,
                    -0.008756339615869907,
                    -0.028072617323291423,
                    -0.022496907067809768,
                    0.006893646179589185,
                    -0.03163989017481957,
                    0.0005436492384808516,
                    0.01788188095207516,
                    -0.02471570185891458,
                    0.014457330295984065,
                    -0.0015260714823126479,
                    -0.0020610847133726135,
                    -0.017986325312839946,
                    -0.010926826552257364,
                    -0.017164199031207722,
                    -0.006635384707023506,
                    0.012714292133091474,
                    0.006833587358960275,
                    0.021477206855668535,
                    0.012002409494830613,
                    0.00099581593260471,
                    0.015670890098371137,
                    -0.014695849904539844,
                    -0.008767253581997179,
                    0.009231701846568574,
                    -0.002577744365278822,
                    -0.021090027689593477,
                    -0.01429994986102751,
                    0.012127999146467687,
                    0.005433345146921108,
                    -0.01207568616461467,
                    0.006173817369362778,
                    -0.007264411510816831,
                    -0.01704839903611614,
                    -0.015882416970055116,
                    0.014837716351009354,
                    0.010692907592657732,
                    -0.03949086786512966,
                    -0.0020362704429527645,
                    -0.0020557122009996873,
                    -0.0025382905227858813,
                    -0.010667067387429176,
                    -0.014626372076159059,
                    -0.19424371138545,
                    -0.0028342496272706717,
                    0.026087133363112847,
                    -0.04110752631309321,
                    0.015301637932248695,
                    0.01207373751542561,
                    0.025153445482488428,
                    0.010604816374860453,
                    -0.032009239102651084,
                    0.01124354838353231,
                    0.01793952211454531,
                    -0.010850356092205632,
                    -0.008864504542615,
                    -0.024626345728273713,
                    -0.029497792961044593,
                    0.006450983298382023,
                    -0.001461115102375829,
                    -0.0002747601679033755,
                    0.025020032223667672,
                    0.006706163023253346,
                    0.025082868748568545,
                    -0.035839937448251494,
                    -0.0012435708238022333,
                    0.003194234668822336,
                    0.010017815892467866,
                    -0.005635835818083603,
                    0.0007843556912972494,
                    0.03486729509540541,
                    0.006167257482851428,
                    -0.04783739371156721,
                    -0.0281576336173587,
                    -0.011120946702776328,
                    0.02480900553483075,
                    -0.0016540545109031417,
                    -0.004541498552250982,
                    0.014101289500001727,
                    0.012869304115809575,
                    0.0019661015266115058,
                    -0.013873854644478061,
                    0.029981927253532135,
                    0.03279598894193059,
                    0.029021443301566335,
                    -0.0066520599667441475,
                    0.00439772810167654,
                    -0.020145158590031242,
                    0.020779156464587684,
                    0.022182230470524356,
                    -0.029861711642799056,
                    0.006027441458774341,
                    -0.009084184593091038,
                    0.01900232518693278,
                    -0.009354425169364316,
                    0.00914238447919836,
                    0.011688961382573297,
                    -0.015594899634911362,
                    0.0030936541606994524,
                    0.006150486796117665,
                    0.00039668173806524994,
                    -0.009604323206036427,
                    -0.011892181215425257,
                    -0.007950689855393735,
                    -0.003030503185769144,
                    0.012341464708341396,
                    -0.007057424496448759,
                    0.003677719423875116,
                    -0.033580620498909516,
                    -0.018314591209452353,
                    0.012407239235816077,
                    -0.036140424988305594,
                    0.01269788203256123,
                    -0.0043411967115334444,
                    0.0020600161494469826,
                    -0.009041057006403593,
                    0.004991803352417692,
                    0.01077889778703958,
                    0.034213395328156954,
                    -0.009920518909098733,
                    0.023032558472542566,
                    0.021618911237466742,
                    0.005053028885790261,
                    -0.006947498277149152,
                    0.01789237707717086,
                    -0.003967487636706336,
                    0.004964986126309326,
                    -0.015004705864263423,
                    -0.004390397169258389,
                    0.00039766845316757947,
                    0.024668467377066374,
                    -0.013913935128275476,
                    -0.0014634704793236959,
                    0.032344791172278904,
                    -0.009776463489434423,
                    0.01169707386978308,
                    -0.010557056618808205,
                    0.0035393343748904094,
                    0.009708027972676373,
                    0.013032325133300065,
                    -0.01791262788590475,
                    0.002634980070534667,
                    -0.00935658358581212,
                    0.004841237211746365,
                    0.009068716731970648,
                    -0.013933978645632124,
                    0.00047183662719452657,
                    0.04040688997538051,
                    -0.0025677218373526592,
                    -0.01573735458318667,
                    0.024625064797933038,
                    0.03498738129210419,
                    -0.019248266508162262,
                    0.0014072328876649168,
                    -0.005811453004473331,
                    0.009273474893829901,
                    -0.0002366355256404547,
                    -0.008208717880538844,
                    0.015470114967377595,
                    0.010806846827236614,
                    -0.015364622133864282,
                    0.02154013245231642,
                    0.00688033270586984,
                    0.062341413849663255,
                    -0.015088607956119923,
                    -0.01157129533260661,
                    0.02264814349902747,
                    -0.015154485098237297,
                    -0.03841565635793725,
                    -0.11388538596925145,
                    -0.014844679485313771,
                    0.005789405782759917,
                    0.024522998011446733,
                    -0.010426172422945851,
                    0.01452267182163097,
                    -0.0058525821512448355,
                    0.03169232309507342,
                    -0.02403454301012375,
                    0.03674729394113128,
                    -0.02083236032872915,
                    -0.010686327651427279,
                    0.013622167440001234,
                    0.011432996971754992,
                    0.007801559046846041,
                    -0.0021348242283544612,
                    0.004612483957521409,
                    -0.008510844156548724,
                    -0.019695449612737056,
                    0.03512757641775848,
                    0.0033271947734170703,
                    -0.010995517145506737,
                    0.013369942565492636,
                    -0.0063290602986140875,
                    -0.01271325861428593,
                    -0.0013704352095818995,
                    -0.02574200043759135,
                    0.020513300911631206,
                    0.0017204250228680085,
                    -0.006308040615298365,
                    0.018341341919461487,
                    -0.024358101111490192,
                    0.008465572666381933,
                    -0.013149966635517376,
                    0.025683021642348397,
                    -0.00981669344562481,
                    -0.0067507876582956474,
                    -0.02098881031141776,
                    0.006398051529786762,
                    -0.006371563854684207,
                    0.009885817319657283,
                    0.013704217788281745,
                    -0.0009308703806006296,
                    -0.0023653232887448994,
                    -0.011713039582845667,
                    -0.0035648711033210412,
                    -0.013818612395380242,
                    0.02815261988375097,
                    0.011157695057625122,
                    -0.02642892073435975,
                    -0.011645280156383084,
                    -0.04094833800536374,
                    -0.029139155601472802,
                    0.010866444546453836,
                    0.009357979899448338,
                    -0.012161456306555326,
                    0.008039205412830796,
                    -0.0015681588208994038,
                    -0.0013173214972966964,
                    0.0019118335149553734,
                    -0.005547910955732768,
                    0.008370524025133576,
                    -0.024795815477955594,
                    0.02449209627292572,
                    0.02159678287397628,
                    0.005043849821704317,
                    -0.002866009343006256,
                    -0.026687717274173208,
                    0.008402179761336,
                    -0.008143162378579096,
                    -0.024671007125353275,
                    0.015880664723915895,
                    -0.013363693852875355,
                    0.012228253248237259,
                    -0.017995287973113505,
                    0.010946936140895359,
                    -0.02595636179382589,
                    -0.034233238002537206,
                    -0.004273498284987208,
                    0.005455790017604779,
                    -0.016061449957149115,
                    -0.014049539977389868,
                    0.0023340968435820912,
                    -0.020800072713617748,
                    -0.002946052867247664,
                    0.004372024948101814,
                    -0.005765166305575613,
                    -0.010824816369193923,
                    -0.012332116488761495,
                    -0.03981934581565518,
                    0.0005973194559449154,
                    0.018036474512508113,
                    0.02239836013707657,
                    -0.006215070069810223,
                    -0.004205528598692035,
                    -0.003911248303258958,
                    -0.018277014882926546,
                    0.007142012694150515,
                    0.025823764598198854,
                    0.019632699717659748,
                    -0.01834251441960647,
                    0.018461194584019975,
                    -0.07531336935280068,
                    0.026983262736981192,
                    0.007446450917008178,
                    -0.016957064302344043,
                    -0.010379048472847575,
                    -0.022390265053684726,
                    0.004908283714572423,
                    -0.0070202718938142256,
                    -0.010433846591807605,
                    0.007650415017870359,
                    -0.03558723274348489,
                    0.01691246048369103,
                    -0.012301172940898497,
                    -0.022714787975121695,
                    -0.02588130544224622,
                    -0.006047359706086076,
                    0.016962876824166202,
                    -0.03185597699910112,
                    0.014783953883547243,
                    0.017354473255445375,
                    -0.00038285194739113535,
                    0.011596174830203558,
                    0.016241258983245674,
                    0.0159956369340154,
                    0.017912828461932335,
                    -0.028248011415259615,
                    -0.0067027990154712395,
                    0.02683435299470267,
                    -0.0009142359154264822,
                    0.008723421313506953,
                    0.011161392196109603,
                    -0.023025786400648434,
                    -0.018465157439601004,
                    0.02230685740021751,
                    -0.012808926473935698,
                    -0.01509820543957048,
                    0.012043503555134028,
                    0.007665498566941778,
                    0.01781329445637615,
                    0.028070153273428534,
                    -0.02727563860196729,
                    -0.03456994705101059,
                    0.005508851900229777,
                    -0.0009219887390480802,
                    0.004468664360191264,
                    -0.00776576748978526,
                    -0.004088767149669528,
                    -0.022156137326869624,
                    0.015772159058694023,
                    0.009617763431124691,
                    0.04069907616692223,
                    0.020185641771418027,
                    -0.018402037062079768,
                    -0.03343444064615175,
                    -0.006528190758564597,
                    0.021398916779934465,
                    0.012809959656952117,
                    0.003882860765072539,
                    0.007122665548707382,
                    -0.003533741306766943,
                    0.039503228286466636,
                    0.0004124082013284328,
                    0.013561840950367112,
                    -0.01114480110530207,
                    -7.259568309209324e-05,
                    -0.02032750425406346,
                    -0.01348170280411211,
                    -0.00344611716717044,
                    -0.006245191015948496,
                    -0.00916958052234417,
                    -0.013567232772719701,
                    0.007916995805987384,
                    0.010367501449167298,
                    0.021365687466826065,
                    0.013123870778960666,
                    0.012210759754211566,
                    -0.005554110512123264,
                    0.004798719725156565,
                    -0.020254505266757705,
                    0.022957630810303973,
                    0.01869357605710742,
                    -0.005920981878665487,
                    0.007356641944881517,
                    0.018814504118217508,
                    0.02516526609278378,
                    0.001124418606537949,
                    -0.021736155035336612,
                    0.008327159108740939,
                    -0.008498260469320734,
                    0.01086313973687424,
                    0.0030541575690444514,
                    0.009599801396258342,
                    0.004629256969211297,
                    0.012394009566217598,
                    -0.0024178331995443835,
                    0.01172042181644473,
                    0.006831342097663389,
                    0.013203076176636182,
                    0.028411481290637072,
                    0.009357478390519935,
                    0.005872529644582724,
                    -0.011634316655211608,
                    -0.0037820688809921833,
                    -0.022428012649809436,
                    -0.01127402480931116,
                    0.008793694996690995,
                    -0.021243500304528914,
                    -0.021472242518412427,
                    0.026302424505775832,
                    0.03676677797122642,
                    0.001898334872332889,
                    -0.01775252265650753,
                    0.0028626608597420605,
                    0.014057754223222819,
                    -0.007903809185574066,
                    0.008805418333862459,
                    -0.004445380322306633,
                    -0.02138230838463181,
                    -0.024941878969810115,
                    0.017543983530217896,
                    0.02798740746979711,
                    -0.0010558931466607315,
                    0.044813642660812086,
                    -0.0008142646629334375,
                    0.012076349034269051,
                    0.018520003070827903,
                    0.02039360958765403,
                    -0.016984118145728668,
                    0.029765878344215416,
                    0.005461424498680684,
                    -0.001003627569678687,
                    0.005605715415935792,
                    -0.017032785094726437,
                    -0.022133413361488195,
                    -0.008404373998273961,
                    -0.015255025874365426,
                    0.0010665207972860283,
                    0.009144708687488734,
                    0.0068582601384997305,
                    0.0987984624105371,
                    0.0195469045650982,
                    -0.015548464846359298,
                    0.0020043038009800334,
                    -0.011554180721952759,
                    0.01547167523300861,
                    -0.0031484205023785757,
                    0.006105857268017381,
                    -0.010381429584154406,
                    -0.03039867306794543,
                    -0.003919448555432431,
                    -0.0005645870193601433,
                    -0.014142703585392269,
                    -0.030945060753941888,
                    -0.011670742890947227,
                    -0.010923800902699397,
                    0.00558922887196318,
                    0.021071161623406106,
                    -0.0007506547944188911,
                    0.0030964310254423767,
                    0.026077887664507103,
                    -0.0027531842657816455,
                    0.002110653996128062,
                    0.008742738956516609,
                    -0.0025486397659327054,
                    0.008817904586339203,
                    0.033983749682024916,
                    0.019364819070977837,
                    0.0022270743210629613,
                    -0.027940915252786202,
                    0.031035732705438444,
                    0.008947869986014574,
                    -0.03289081067682707,
                    -0.021979799101027802,
                    0.010661634409198786,
                    -0.009455160095759379,
                    -0.007227959793970111,
                    -0.024000778370028196,
                    0.0038774930620342715,
                    -0.0007198929945325342,
                    0.002020071798577519,
                    0.012020963237318707,
                    -0.006862165495918086,
                    -0.05474541631616806,
                    -0.00446250277635553,
                    0.01353015232169212,
                    0.0005361731300295864,
                    -0.006701599109142827,
                    -0.03494081243837383
                ],
                "cluster": [
                    0
                ]
            }
        ],
        "summaries": []
    }
}